{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNcK8NWqcvI08tjJsy29su5","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/gpt2_hindi_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"**Install libraries**","metadata":{}},{"cell_type":"code","source":"!pip install --quiet \"torchtext\" \"datasets\" \"tokenizers\" \"transformers\"\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fte3KS2c68Pr","outputId":"e4e3b557-a825-4dcb-9c4c-70f41239d3ac","execution":{"iopub.status.busy":"2023-10-27T19:14:03.585961Z","iopub.execute_input":"2023-10-27T19:14:03.586357Z","iopub.status.idle":"2023-10-27T19:14:15.699924Z","shell.execute_reply.started":"2023-10-27T19:14:03.586324Z","shell.execute_reply":"2023-10-27T19:14:15.698468Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Lets import all the dependencies**","metadata":{}},{"cell_type":"code","source":"from tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers import Tokenizer\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:15.702172Z","iopub.execute_input":"2023-10-27T19:14:15.702495Z","iopub.status.idle":"2023-10-27T19:14:15.709065Z","shell.execute_reply.started":"2023-10-27T19:14:15.702466Z","shell.execute_reply":"2023-10-27T19:14:15.707934Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Define the dataset paths**","metadata":{}},{"cell_type":"code","source":"# Path on Kaggle\ntokenizer_path = '/kaggle/input/hindiaesthetics/hindi_aesthetics_word_level.json'\ntrain_dataset_path = '/kaggle/input/hindiaesthetics/hindi_train.txt'\nval_dataset_path = '/kaggle/input/hindiaesthetics/hindi_val.txt'\n","metadata":{"id":"VIeI2nM4AKIK","execution":{"iopub.status.busy":"2023-10-27T19:14:15.710264Z","iopub.execute_input":"2023-10-27T19:14:15.710552Z","iopub.status.idle":"2023-10-27T19:14:15.719896Z","shell.execute_reply.started":"2023-10-27T19:14:15.710529Z","shell.execute_reply":"2023-10-27T19:14:15.719107Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"**Define Hyperparameters**","metadata":{}},{"cell_type":"code","source":"# hyperparameters\nbatch_size = 512 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 10000\neval_interval = 1000\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:15.722047Z","iopub.execute_input":"2023-10-27T19:14:15.722342Z","iopub.status.idle":"2023-10-27T19:14:15.733710Z","shell.execute_reply.started":"2023-10-27T19:14:15.722315Z","shell.execute_reply":"2023-10-27T19:14:15.732793Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Load the Vocabulary**","metadata":{"id":"JHD8-uKF2bfP"}},{"cell_type":"code","source":"def build_word_level_tokenizer(data_path, tokenizer_path = None):\n    if tokenizer_path is None:\n        with open(data_path,'r',encoding='UTF-8') as fh:\n            all_data = fh.readlines()\n        # code inspired from huggingface tokenizers\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n                                    min_frequency=2)\n        tokenizer.train_from_iterator(all_data, trainer=trainer)\n        #tokenizer.train(files=[all_data_path], vocab_size=52_000, min_frequency=2, special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"])\n        tokenizer.save('./hindi_aesthetics_word_level.json')\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer","metadata":{"id":"cG6FIv4R2ejr","execution":{"iopub.status.busy":"2023-10-27T19:14:15.734828Z","iopub.execute_input":"2023-10-27T19:14:15.735074Z","iopub.status.idle":"2023-10-27T19:14:15.743951Z","shell.execute_reply.started":"2023-10-27T19:14:15.735046Z","shell.execute_reply":"2023-10-27T19:14:15.743191Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"all_data_path = None # for kaggle\ntokenizer = build_word_level_tokenizer(all_data_path, tokenizer_path)\nvocab_size = tokenizer.get_vocab_size()\n","metadata":{"id":"u5voqahg43lU","execution":{"iopub.status.busy":"2023-10-27T19:14:15.744998Z","iopub.execute_input":"2023-10-27T19:14:15.745332Z","iopub.status.idle":"2023-10-27T19:14:15.782436Z","shell.execute_reply.started":"2023-10-27T19:14:15.745302Z","shell.execute_reply":"2023-10-27T19:14:15.781404Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"**Define dataset class**","metadata":{"id":"jRHHKreN7-z6"}},{"cell_type":"code","source":"class HindiAestheticsDataset(Dataset):\n\n    def __init__(self, ds_path, tokenizer, block_size=64):\n        super().__init__()\n        self.block_size = block_size\n        self.ds_path = ds_path\n        self.tokenizer = tokenizer\n\n        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        with open(ds_path, 'r', encoding='UTF-8') as fh:\n            self.ds = fh.readlines()\n\n    def __len__(self):\n        return len(self.ds)\n\n\n    def __getitem__(self, idx):\n        # get a src, target pair\n        input_text = self.ds[idx]\n\n        # transform the text into tokens\n        input_tokens = self.tokenizer.encode(input_text).ids\n        max_len_of_sentence = self.block_size - 2\n        if len(input_tokens) > max_len_of_sentence:\n            input_tokens = input_tokens[:max_len_of_sentence]\n            \n        # Add sos, eos and padding to each sentence\n        num_padding_tokens_input = max(0, max_len_of_sentence - len(input_tokens))  # we will add <s> and </s>\n        # we will only add only the <s> token to the decoder\n        num_padding_tokens_output = num_padding_tokens_input+1\n\n        # Add <s> and </s> token\n        x = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n            ],\n            dim=0,)\n\n        # Add only the <s>\n        y = torch.cat(\n            [\n                torch.tensor(input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n        #print(\"inside get item and I am returning the dict list!\")\n        #print(f\"x:{len(x)} y: {len(y)}\")\n\n        return {\n            \"x\": x,\n            \"y\": y,\n            \"input_sentences\": input_text,\n        }\n\n    def collate_samples(self, batch):\n        \"\"\"\n        Perform dynamic batching on the sequences.\n        For each batch, we get the length of the longest sentence and pad the remaining sentences according to that.\n        \"\"\"\n\n        #print(\"inside collate function\")\n        # max encoder str length\n        max_len = max(x[\"token_len\"] for x in batch)\n        #print(f\"longest encoder input in this batch: {encoder_input_max}\")\n\n        x_list = []\n        y_list = []\n        input_sentences = []\n\n        for cnt, x in enumerate(batch):\n            # Add sos, eos and padding to each sentence\n            num_padding_tokens_input = max(0, max_len - len(x[\"input_tokens\"]))  # we will add <s> and </s>\n            # we will only add only the <s> token to the decoder\n            num_padding_tokens_output = num_padding_tokens_input+1\n\n            # Add <s> and </s> token\n            batch_x = torch.cat(\n                [\n                    self.sos_token,\n                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n                    self.eos_token,\n                    torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n                ],\n                dim=0,\n            )\n\n            # Add only the <s>\n            batch_y = torch.cat(\n                [\n                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n                    self.eos_token,\n                    torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n                ],\n                dim=0,\n            )\n            x_list.append(batch_x)\n            y_list.append(batch_y)\n            input_sentences.append(x[\"input_sentence\"])\n\n        #print(\"inside get item and I am returning the dict list!\")\n        return {\n            \"x\": torch.vstack(x_list),\n            \"y\": torch.vstack(y_list),\n            \"input_sentences\": input_sentences,\n        }\n\n","metadata":{"id":"OZeAL2Hs9oN2","execution":{"iopub.status.busy":"2023-10-27T19:14:15.784036Z","iopub.execute_input":"2023-10-27T19:14:15.784327Z","iopub.status.idle":"2023-10-27T19:14:15.802022Z","shell.execute_reply.started":"2023-10-27T19:14:15.784303Z","shell.execute_reply":"2023-10-27T19:14:15.801089Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"train_ds = HindiAestheticsDataset(train_dataset_path, tokenizer, block_size = block_size)\nval_ds = HindiAestheticsDataset(val_dataset_path, tokenizer, block_size=block_size)\ntrain_dataloader = DataLoader(dataset = train_ds,\n                              batch_size = batch_size,\n                              num_workers = 1,\n                              collate_fn = None,\n                              shuffle = True)\nval_dataloader = DataLoader(dataset = val_ds,\n                            batch_size = 1,\n                            num_workers = 1,\n                            collate_fn = None,\n                            shuffle = False)","metadata":{"id":"hVqp45lcMJyB","execution":{"iopub.status.busy":"2023-10-27T19:14:15.803406Z","iopub.execute_input":"2023-10-27T19:14:15.804418Z","iopub.status.idle":"2023-10-27T19:14:16.388454Z","shell.execute_reply.started":"2023-10-27T19:14:15.804386Z","shell.execute_reply":"2023-10-27T19:14:16.387599Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**A couple of support functions**","metadata":{}},{"cell_type":"code","source":"def get_batch(data_loader):\n    vals = next(iter(data_loader))\n    x = vals[\"x\"]\n    y = vals[\"y\"]\n    return x.to(device), y.to(device)\n\n\n@torch.no_grad()\ndef estimate_loss(model, data_loader):\n    out = {}\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        X, Y = get_batch(data_loader)\n        logits, loss = model.forward(X, Y)\n        losses[k] = loss.item()\n    out = losses.mean()\n    model.train()\n    return out\n\ndef decode(enc_sec: torch.Tensor, tokenizer: any) -> str:\n    \"\"\"\n    Function to decode a sequence of token indices back to a string\n    \"\"\"\n    # convert the indices to a list\n    enc_sec = enc_sec.tolist()\n    # decode the indices to a string\n    text = tokenizer.decode(enc_sec)\n    return text","metadata":{"id":"eMz3QwJoWbas","execution":{"iopub.status.busy":"2023-10-27T19:14:16.389626Z","iopub.execute_input":"2023-10-27T19:14:16.389913Z","iopub.status.idle":"2023-10-27T19:14:16.397694Z","shell.execute_reply.started":"2023-10-27T19:14:16.389888Z","shell.execute_reply":"2023-10-27T19:14:16.396808Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**Let's define the decoder model**","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        #print(f\"idx shape: {idx.shape}\")\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        #print(f\"token embedding shape:{tok_emb.shape}\")\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n","metadata":{"id":"Bt2IkQdgUMB2","execution":{"iopub.status.busy":"2023-10-27T19:14:16.402452Z","iopub.execute_input":"2023-10-27T19:14:16.402726Z","iopub.status.idle":"2023-10-27T19:14:16.461183Z","shell.execute_reply.started":"2023-10-27T19:14:16.402702Z","shell.execute_reply":"2023-10-27T19:14:16.460280Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Lets train the model**","metadata":{}},{"cell_type":"code","source":"def init_weights(module):\n    if isinstance(module, nn.Linear):\n        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        if module.bias is not None:\n            torch.nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\ndef train_model(gpt_model):\n    # optimizer takes the model's parameters and the learning rate as input,\n    # and updates the parameters during the training process in order to\n    # minimize the loss function.\n    optimizer = torch.optim.AdamW(gpt_model.parameters(), lr=learning_rate)\n    context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n    for cnt in range(max_iters):\n        # every EVAL_INTER evaluate the loss on train and val sets\n        if cnt % eval_iters == 0 or cnt == max_iters - 1:\n            train_loss = estimate_loss(gpt_model, train_dataloader)\n            val_loss = estimate_loss(gpt_model, val_dataloader)\n            print(f\"step {cnt}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n            print(\"generated text:\")\n            print(\"--------------------------------------------\")\n            print(decode(enc_sec=gpt_model.generate(idx=context, max_new_tokens=100, block_size=block_size)[0],\n                tokenizer=tokenizer))\n\n        # sample a batch of data\n        xb, yb = get_batch(train_dataloader)\n        logits, loss = gpt_model(xb, yb)\n        # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n        optimizer.zero_grad(set_to_none=True)\n        # backward() method on the loss variable calculates the gradients\n        # of the loss with respect to the model's parameters.\n        loss.backward()\n        # step() method on the optimizer updates the model's parameters\n        # using the calculated gradients, in order to minimize the loss.\n        optimizer.step()\n    return gpt_model","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:16.462283Z","iopub.execute_input":"2023-10-27T19:14:16.462578Z","iopub.status.idle":"2023-10-27T19:14:16.477113Z","shell.execute_reply.started":"2023-10-27T19:14:16.462544Z","shell.execute_reply":"2023-10-27T19:14:16.476154Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"#model = GPTLanguageModel()\n#m = model.to(device)\n## print the number of parameters in the model\n#print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n## create a PyTorch optimizer\n#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n## generate some output based on the context\n#context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n\n#for cnt in range(max_iters):\n\n#    # every once in a while evaluate the loss on train and val sets\n#    if cnt % eval_interval == 0 or cnt == max_iters - 1:\n#        train_loss = estimate_loss(train_dataloader)\n#        val_loss = estimate_loss(m, val_dataloader)\n#        print(f\"step {cnt}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n#        print(\"generated text:\")\n#        print(\"--------------------------------------------\")\n#        print(decode(enc_sec=m.generate(idx=context, max_new_tokens=100)[0],\n#            tokenizer=tokenizer,))\n\n#    # sample a batch of data\n#    xb, yb = get_batch(train_dataloader)\n\n#    # evaluate the loss\n#    #print(f\"size of xb: {xb.shape}, size of yb:{yb.shape}\")\n#    logits, loss = model(xb, yb)\n#    optimizer.zero_grad(set_to_none=True)\n#    loss.backward()\n#    optimizer.step()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:16.478242Z","iopub.execute_input":"2023-10-27T19:14:16.478562Z","iopub.status.idle":"2023-10-27T19:14:16.492631Z","shell.execute_reply.started":"2023-10-27T19:14:16.478538Z","shell.execute_reply":"2023-10-27T19:14:16.491666Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def generate_sentences(model, num_sentences=10):\n    # generate some output based on the context\n    context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n    for cnt in range(num_sentences):\n        gen = decode(enc_sec=model.generate(idx=context, max_new_tokens=100, block_size=block_size)[0],tokenizer=tokenizer)\n        print(f\"{cnt}: {gen}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:16.493868Z","iopub.execute_input":"2023-10-27T19:14:16.494136Z","iopub.status.idle":"2023-10-27T19:14:16.506710Z","shell.execute_reply.started":"2023-10-27T19:14:16.494113Z","shell.execute_reply":"2023-10-27T19:14:16.505822Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"!git clone \"https://github.com/jyanivaddi/dl_hub.git\"\n!git -C dl_hub pull\n!git pull","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:16.507944Z","iopub.execute_input":"2023-10-27T19:14:16.508313Z","iopub.status.idle":"2023-10-27T19:14:19.754976Z","shell.execute_reply.started":"2023-10-27T19:14:16.508280Z","shell.execute_reply":"2023-10-27T19:14:19.753638Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"fatal: destination path 'dl_hub' already exists and is not an empty directory.\nAlready up to date.\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Now lets try with a GPT model we wrote from a previous session**","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/dl_hub/')\nfrom dl_hub.transformer_models.transformer_models import GPT","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:19.756757Z","iopub.execute_input":"2023-10-27T19:14:19.757194Z","iopub.status.idle":"2023-10-27T19:14:19.763175Z","shell.execute_reply.started":"2023-10-27T19:14:19.757156Z","shell.execute_reply":"2023-10-27T19:14:19.762008Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# train a new model\ngpt_model = GPT(\n    vocab_size=vocab_size,\n    d_model=n_embd,\n    block_size=block_size,\n    num_heads=n_head,\n    num_layers=n_layer,\n    dropout=dropout,\n    device = device\n)\n# load model to GPU if available\ngpt_model = gpt_model.to(device)\ngpt_model.apply(init_weights)\n\n# print the number of parameters in the model\nprint(\n    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in gpt_model.parameters()) / 1e6)\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:19.764271Z","iopub.execute_input":"2023-10-27T19:14:19.764541Z","iopub.status.idle":"2023-10-27T19:14:20.263788Z","shell.execute_reply.started":"2023-10-27T19:14:19.764518Z","shell.execute_reply":"2023-10-27T19:14:20.262639Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Model with 33.71M parameters\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Lets train this model now**","metadata":{}},{"cell_type":"code","source":"gpt_model = train_model(gpt_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:20.265084Z","iopub.execute_input":"2023-10-27T19:14:20.265452Z","iopub.status.idle":"2023-10-27T19:14:20.949920Z","shell.execute_reply.started":"2023-10-27T19:14:20.265419Z","shell.execute_reply":"2023-10-27T19:14:20.948492Z"},"trusted":true},"execution_count":34,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gpt_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_model\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[28], line 18\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(gpt_model)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cnt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iters):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# every EVAL_INTER evaluate the loss on train and val sets\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cnt \u001b[38;5;241m%\u001b[39m eval_iters \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m cnt \u001b[38;5;241m==\u001b[39m max_iters \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 18\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m estimate_loss(gpt_model, val_dataloader)\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: train loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, val loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[26], line 15\u001b[0m, in \u001b[0;36mestimate_loss\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(eval_iters):\n\u001b[1;32m     14\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m get_batch(data_loader)\n\u001b[0;32m---> 15\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     losses[k] \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mmean()\n","File \u001b[0;32m/kaggle/working/dl_hub/transformer_models/transformer_models.py:276\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    274\u001b[0m     logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(logits, (B \u001b[38;5;241m*\u001b[39m T, C))\n\u001b[1;32m    275\u001b[0m     targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(targets, (B \u001b[38;5;241m*\u001b[39m T,))\n\u001b[0;32m--> 276\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 14.76 GiB total capacity; 12.33 GiB already allocated; 61.75 MiB free; 13.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1.83 GiB (GPU 0; 14.76 GiB total capacity; 12.33 GiB already allocated; 61.75 MiB free; 13.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]},{"cell_type":"code","source":"generate_sentences(gpt_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T19:14:20.951154Z","iopub.status.idle":"2023-10-27T19:14:20.951532Z","shell.execute_reply.started":"2023-10-27T19:14:20.951353Z","shell.execute_reply":"2023-10-27T19:14:20.951370Z"},"trusted":true},"execution_count":null,"outputs":[]}]}