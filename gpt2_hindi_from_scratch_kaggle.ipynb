{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79e8aa5c",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.009425,
     "end_time": "2023-10-28T08:57:44.231548",
     "exception": false,
     "start_time": "2023-10-28T08:57:44.222123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/gpt2_hindi_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee18f9",
   "metadata": {
    "papermill": {
     "duration": 0.009102,
     "end_time": "2023-10-28T08:57:44.250483",
     "exception": false,
     "start_time": "2023-10-28T08:57:44.241381",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Install libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d768e7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:57:44.270035Z",
     "iopub.status.busy": "2023-10-28T08:57:44.269347Z",
     "iopub.status.idle": "2023-10-28T08:57:57.323302Z",
     "shell.execute_reply": "2023-10-28T08:57:57.322062Z"
    },
    "id": "Fte3KS2c68Pr",
    "outputId": "e4e3b557-a825-4dcb-9c4c-70f41239d3ac",
    "papermill": {
     "duration": 13.066016,
     "end_time": "2023-10-28T08:57:57.325758",
     "exception": false,
     "start_time": "2023-10-28T08:57:44.259742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --quiet \"torchtext\" \"datasets\" \"tokenizers\" \"transformers\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6330b635",
   "metadata": {
    "papermill": {
     "duration": 0.008347,
     "end_time": "2023-10-28T08:57:57.342905",
     "exception": false,
     "start_time": "2023-10-28T08:57:57.334558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Lets import all the dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2e8c91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:57:57.362376Z",
     "iopub.status.busy": "2023-10-28T08:57:57.362065Z",
     "iopub.status.idle": "2023-10-28T08:58:01.261943Z",
     "shell.execute_reply": "2023-10-28T08:58:01.261002Z"
    },
    "papermill": {
     "duration": 3.912346,
     "end_time": "2023-10-28T08:58:01.264198",
     "exception": false,
     "start_time": "2023-10-28T08:57:57.351852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers import Tokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6c1d59",
   "metadata": {
    "papermill": {
     "duration": 0.008335,
     "end_time": "2023-10-28T08:58:01.281196",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.272861",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Define the dataset paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5a52d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:01.299645Z",
     "iopub.status.busy": "2023-10-28T08:58:01.299227Z",
     "iopub.status.idle": "2023-10-28T08:58:01.303556Z",
     "shell.execute_reply": "2023-10-28T08:58:01.302719Z"
    },
    "id": "VIeI2nM4AKIK",
    "papermill": {
     "duration": 0.015775,
     "end_time": "2023-10-28T08:58:01.305356",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.289581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path on Kaggle\n",
    "tokenizer_path = '/kaggle/input/hindiaesthetics/hindi_aesthetics_word_level.json'\n",
    "train_dataset_path = '/kaggle/input/hindiaesthetics/hindi_train.txt'\n",
    "val_dataset_path = '/kaggle/input/hindiaesthetics/hindi_val.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9230ded3",
   "metadata": {
    "papermill": {
     "duration": 0.008243,
     "end_time": "2023-10-28T08:58:01.322133",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.313890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Define Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e36143b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:01.340812Z",
     "iopub.status.busy": "2023-10-28T08:58:01.340484Z",
     "iopub.status.idle": "2023-10-28T08:58:01.444313Z",
     "shell.execute_reply": "2023-10-28T08:58:01.443218Z"
    },
    "papermill": {
     "duration": 0.115335,
     "end_time": "2023-10-28T08:58:01.446250",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.330915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 256 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 10000\n",
    "eval_interval = 1000\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 1000\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd9a2cf",
   "metadata": {
    "id": "JHD8-uKF2bfP",
    "papermill": {
     "duration": 0.008546,
     "end_time": "2023-10-28T08:58:01.463865",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.455319",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Load the Vocabulary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "737b3785",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:01.482465Z",
     "iopub.status.busy": "2023-10-28T08:58:01.482198Z",
     "iopub.status.idle": "2023-10-28T08:58:01.488620Z",
     "shell.execute_reply": "2023-10-28T08:58:01.487771Z"
    },
    "id": "cG6FIv4R2ejr",
    "papermill": {
     "duration": 0.017901,
     "end_time": "2023-10-28T08:58:01.490571",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.472670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_word_level_tokenizer(data_path, tokenizer_path = None):\n",
    "    if tokenizer_path is None:\n",
    "        with open(data_path,'r',encoding='UTF-8') as fh:\n",
    "            all_data = fh.readlines()\n",
    "        # code inspired from huggingface tokenizers\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n",
    "                                    min_frequency=2)\n",
    "        tokenizer.train_from_iterator(all_data, trainer=trainer)\n",
    "        #tokenizer.train(files=[all_data_path], vocab_size=52_000, min_frequency=2, special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"])\n",
    "        tokenizer.save('./hindi_aesthetics_word_level.json')\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "309c94cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:01.508841Z",
     "iopub.status.busy": "2023-10-28T08:58:01.508569Z",
     "iopub.status.idle": "2023-10-28T08:58:01.571965Z",
     "shell.execute_reply": "2023-10-28T08:58:01.571144Z"
    },
    "id": "u5voqahg43lU",
    "papermill": {
     "duration": 0.074545,
     "end_time": "2023-10-28T08:58:01.573819",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.499274",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_data_path = None # for kaggle\n",
    "tokenizer = build_word_level_tokenizer(all_data_path, tokenizer_path)\n",
    "vocab_size = tokenizer.get_vocab_size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c2627",
   "metadata": {
    "id": "jRHHKreN7-z6",
    "papermill": {
     "duration": 0.008846,
     "end_time": "2023-10-28T08:58:01.592004",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.583158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Define dataset class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cefbea4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:01.610572Z",
     "iopub.status.busy": "2023-10-28T08:58:01.610317Z",
     "iopub.status.idle": "2023-10-28T08:58:01.628849Z",
     "shell.execute_reply": "2023-10-28T08:58:01.628012Z"
    },
    "id": "OZeAL2Hs9oN2",
    "papermill": {
     "duration": 0.03019,
     "end_time": "2023-10-28T08:58:01.630867",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.600677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HindiAestheticsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, ds_path, tokenizer, block_size=64):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.ds_path = ds_path\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "        with open(ds_path, 'r', encoding='UTF-8') as fh:\n",
    "            self.ds = fh.readlines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get a src, target pair\n",
    "        input_text = self.ds[idx]\n",
    "\n",
    "        # transform the text into tokens\n",
    "        input_tokens = self.tokenizer.encode(input_text).ids\n",
    "        max_len_of_sentence = self.block_size - 2\n",
    "        if len(input_tokens) > max_len_of_sentence:\n",
    "            input_tokens = input_tokens[:max_len_of_sentence]\n",
    "            \n",
    "        # Add sos, eos and padding to each sentence\n",
    "        num_padding_tokens_input = max(0, max_len_of_sentence - len(input_tokens))  # we will add <s> and </s>\n",
    "        # we will only add only the <s> token to the decoder\n",
    "        num_padding_tokens_output = num_padding_tokens_input+1\n",
    "\n",
    "        # Add <s> and </s> token\n",
    "        x = torch.cat(\n",
    "            [\n",
    "                self.sos_token,\n",
    "                torch.tensor(input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,)\n",
    "\n",
    "        # Add only the <s>\n",
    "        y = torch.cat(\n",
    "            [\n",
    "                torch.tensor(input_tokens, dtype=torch.int64),\n",
    "                self.eos_token,\n",
    "                torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        #print(\"inside get item and I am returning the dict list!\")\n",
    "        #print(f\"x:{len(x)} y: {len(y)}\")\n",
    "\n",
    "        return {\n",
    "            \"x\": x,\n",
    "            \"y\": y,\n",
    "            \"input_sentences\": input_text,\n",
    "        }\n",
    "\n",
    "    def collate_samples(self, batch):\n",
    "        \"\"\"\n",
    "        Perform dynamic batching on the sequences.\n",
    "        For each batch, we get the length of the longest sentence and pad the remaining sentences according to that.\n",
    "        \"\"\"\n",
    "\n",
    "        #print(\"inside collate function\")\n",
    "        # max encoder str length\n",
    "        max_len = max(x[\"token_len\"] for x in batch)\n",
    "        #print(f\"longest encoder input in this batch: {encoder_input_max}\")\n",
    "\n",
    "        x_list = []\n",
    "        y_list = []\n",
    "        input_sentences = []\n",
    "\n",
    "        for cnt, x in enumerate(batch):\n",
    "            # Add sos, eos and padding to each sentence\n",
    "            num_padding_tokens_input = max(0, max_len - len(x[\"input_tokens\"]))  # we will add <s> and </s>\n",
    "            # we will only add only the <s> token to the decoder\n",
    "            num_padding_tokens_output = num_padding_tokens_input+1\n",
    "\n",
    "            # Add <s> and </s> token\n",
    "            batch_x = torch.cat(\n",
    "                [\n",
    "                    self.sos_token,\n",
    "                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n",
    "                    self.eos_token,\n",
    "                    torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "            # Add only the <s>\n",
    "            batch_y = torch.cat(\n",
    "                [\n",
    "                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n",
    "                    self.eos_token,\n",
    "                    torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "            x_list.append(batch_x)\n",
    "            y_list.append(batch_y)\n",
    "            input_sentences.append(x[\"input_sentence\"])\n",
    "\n",
    "        #print(\"inside get item and I am returning the dict list!\")\n",
    "        return {\n",
    "            \"x\": torch.vstack(x_list),\n",
    "            \"y\": torch.vstack(y_list),\n",
    "            \"input_sentences\": input_sentences,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a6427e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:01.649209Z",
     "iopub.status.busy": "2023-10-28T08:58:01.648963Z",
     "iopub.status.idle": "2023-10-28T08:58:03.262431Z",
     "shell.execute_reply": "2023-10-28T08:58:03.261599Z"
    },
    "id": "hVqp45lcMJyB",
    "papermill": {
     "duration": 1.625344,
     "end_time": "2023-10-28T08:58:03.264687",
     "exception": false,
     "start_time": "2023-10-28T08:58:01.639343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = HindiAestheticsDataset(train_dataset_path, tokenizer, block_size = block_size)\n",
    "val_ds = HindiAestheticsDataset(val_dataset_path, tokenizer, block_size=block_size)\n",
    "train_dataloader = DataLoader(dataset = train_ds,\n",
    "                              batch_size = batch_size,\n",
    "                              num_workers = 1,\n",
    "                              collate_fn = None,\n",
    "                              shuffle = True)\n",
    "val_dataloader = DataLoader(dataset = val_ds,\n",
    "                            batch_size = 1,\n",
    "                            num_workers = 1,\n",
    "                            collate_fn = None,\n",
    "                            shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4d8df1",
   "metadata": {
    "papermill": {
     "duration": 0.008619,
     "end_time": "2023-10-28T08:58:03.282506",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.273887",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**A couple of support functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "728a47af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:03.301350Z",
     "iopub.status.busy": "2023-10-28T08:58:03.301050Z",
     "iopub.status.idle": "2023-10-28T08:58:03.308609Z",
     "shell.execute_reply": "2023-10-28T08:58:03.307734Z"
    },
    "id": "eMz3QwJoWbas",
    "papermill": {
     "duration": 0.019307,
     "end_time": "2023-10-28T08:58:03.310570",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.291263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_batch(data_loader):\n",
    "    vals = next(iter(data_loader))\n",
    "    x = vals[\"x\"]\n",
    "    y = vals[\"y\"]\n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, data_loader):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    for k in range(eval_iters):\n",
    "        X, Y = get_batch(data_loader)\n",
    "        logits, loss = model.forward(X, Y)\n",
    "        losses[k] = loss.item()\n",
    "    out = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def decode(enc_sec: torch.Tensor, tokenizer: any) -> str:\n",
    "    \"\"\"\n",
    "    Function to decode a sequence of token indices back to a string\n",
    "    \"\"\"\n",
    "    # convert the indices to a list\n",
    "    enc_sec = enc_sec.tolist()\n",
    "    # decode the indices to a string\n",
    "    text = tokenizer.decode(enc_sec)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d22f147",
   "metadata": {
    "papermill": {
     "duration": 0.008449,
     "end_time": "2023-10-28T08:58:03.327458",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.319009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Let's define the decoder model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669754a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:03.345814Z",
     "iopub.status.busy": "2023-10-28T08:58:03.345561Z",
     "iopub.status.idle": "2023-10-28T08:58:03.386293Z",
     "shell.execute_reply": "2023-10-28T08:58:03.385559Z"
    },
    "id": "Bt2IkQdgUMB2",
    "papermill": {
     "duration": 0.052252,
     "end_time": "2023-10-28T08:58:03.388179",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.335927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        #print(f\"idx shape: {idx.shape}\")\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        #print(f\"token embedding shape:{tok_emb.shape}\")\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef9aa5d",
   "metadata": {
    "papermill": {
     "duration": 0.008178,
     "end_time": "2023-10-28T08:58:03.404911",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.396733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Lets train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4471d514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:03.423290Z",
     "iopub.status.busy": "2023-10-28T08:58:03.423001Z",
     "iopub.status.idle": "2023-10-28T08:58:03.433067Z",
     "shell.execute_reply": "2023-10-28T08:58:03.432244Z"
    },
    "papermill": {
     "duration": 0.021611,
     "end_time": "2023-10-28T08:58:03.435046",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.413435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_weights(module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        if module.bias is not None:\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "    elif isinstance(module, nn.Embedding):\n",
    "        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "def train_model(gpt_model):\n",
    "    # optimizer takes the model's parameters and the learning rate as input,\n",
    "    # and updates the parameters during the training process in order to\n",
    "    # minimize the loss function.\n",
    "    optimizer = torch.optim.AdamW(gpt_model.parameters(), lr=learning_rate)\n",
    "    context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n",
    "    for cnt in range(max_iters):\n",
    "        # every EVAL_INTER evaluate the loss on train and val sets\n",
    "        if cnt % eval_iters == 0 or cnt == max_iters - 1:\n",
    "            train_loss = estimate_loss(gpt_model, train_dataloader)\n",
    "            val_loss = estimate_loss(gpt_model, val_dataloader)\n",
    "            print(f\"step {cnt}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "            print(\"generated text:\")\n",
    "            print(\"--------------------------------------------\")\n",
    "            print(decode(enc_sec=gpt_model.generate(idx=context, max_new_tokens=100, block_size=block_size)[0],\n",
    "                tokenizer=tokenizer))\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch(train_dataloader)\n",
    "        logits, loss = gpt_model(xb, yb)\n",
    "        # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        # backward() method on the loss variable calculates the gradients\n",
    "        # of the loss with respect to the model's parameters.\n",
    "        loss.backward()\n",
    "        # step() method on the optimizer updates the model's parameters\n",
    "        # using the calculated gradients, in order to minimize the loss.\n",
    "        optimizer.step()\n",
    "    return gpt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30973233",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:03.453595Z",
     "iopub.status.busy": "2023-10-28T08:58:03.453338Z",
     "iopub.status.idle": "2023-10-28T08:58:03.457964Z",
     "shell.execute_reply": "2023-10-28T08:58:03.457090Z"
    },
    "papermill": {
     "duration": 0.015855,
     "end_time": "2023-10-28T08:58:03.459888",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.444033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = GPTLanguageModel()\n",
    "#m = model.to(device)\n",
    "## print the number of parameters in the model\n",
    "#print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "## create a PyTorch optimizer\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## generate some output based on the context\n",
    "#context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n",
    "\n",
    "#for cnt in range(max_iters):\n",
    "\n",
    "#    # every once in a while evaluate the loss on train and val sets\n",
    "#    if cnt % eval_interval == 0 or cnt == max_iters - 1:\n",
    "#        train_loss = estimate_loss(train_dataloader)\n",
    "#        val_loss = estimate_loss(m, val_dataloader)\n",
    "#        print(f\"step {cnt}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n",
    "#        print(\"generated text:\")\n",
    "#        print(\"--------------------------------------------\")\n",
    "#        print(decode(enc_sec=m.generate(idx=context, max_new_tokens=100)[0],\n",
    "#            tokenizer=tokenizer,))\n",
    "\n",
    "#    # sample a batch of data\n",
    "#    xb, yb = get_batch(train_dataloader)\n",
    "\n",
    "#    # evaluate the loss\n",
    "#    #print(f\"size of xb: {xb.shape}, size of yb:{yb.shape}\")\n",
    "#    logits, loss = model(xb, yb)\n",
    "#    optimizer.zero_grad(set_to_none=True)\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91a6c916",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:03.478549Z",
     "iopub.status.busy": "2023-10-28T08:58:03.477819Z",
     "iopub.status.idle": "2023-10-28T08:58:03.483447Z",
     "shell.execute_reply": "2023-10-28T08:58:03.482714Z"
    },
    "papermill": {
     "duration": 0.017135,
     "end_time": "2023-10-28T08:58:03.485319",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.468184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_sentences(model, num_sentences=10):\n",
    "    # generate some output based on the context\n",
    "    context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n",
    "    for cnt in range(num_sentences):\n",
    "        gen = decode(enc_sec=model.generate(idx=context, max_new_tokens=100, block_size=block_size)[0],tokenizer=tokenizer)\n",
    "        print(f\"{cnt}: {gen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a651dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:03.503042Z",
     "iopub.status.busy": "2023-10-28T08:58:03.502745Z",
     "iopub.status.idle": "2023-10-28T08:58:07.246876Z",
     "shell.execute_reply": "2023-10-28T08:58:07.245315Z"
    },
    "papermill": {
     "duration": 3.75592,
     "end_time": "2023-10-28T08:58:07.249514",
     "exception": false,
     "start_time": "2023-10-28T08:58:03.493594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'dl_hub'...\r\n",
      "remote: Enumerating objects: 581, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (333/333), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (143/143), done.\u001b[K\r\n",
      "remote: Total 581 (delta 215), reused 291 (delta 183), pack-reused 248\u001b[K\r\n",
      "Receiving objects: 100% (581/581), 161.26 KiB | 2.04 MiB/s, done.\r\n",
      "Resolving deltas: 100% (359/359), done.\r\n",
      "Already up to date.\r\n",
      "fatal: not a git repository (or any parent up to mount point /kaggle)\r\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\r\n"
     ]
    }
   ],
   "source": [
    "!git clone \"https://github.com/jyanivaddi/dl_hub.git\"\n",
    "!git -C dl_hub pull\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabf2fe8",
   "metadata": {
    "papermill": {
     "duration": 0.01291,
     "end_time": "2023-10-28T08:58:07.274163",
     "exception": false,
     "start_time": "2023-10-28T08:58:07.261253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Now lets try with a GPT model we wrote from a previous session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ced76b46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:07.300287Z",
     "iopub.status.busy": "2023-10-28T08:58:07.299528Z",
     "iopub.status.idle": "2023-10-28T08:58:07.311921Z",
     "shell.execute_reply": "2023-10-28T08:58:07.311220Z"
    },
    "papermill": {
     "duration": 0.028331,
     "end_time": "2023-10-28T08:58:07.313939",
     "exception": false,
     "start_time": "2023-10-28T08:58:07.285608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/kaggle/working/dl_hub/')\n",
    "from dl_hub.transformer_models.transformer_models import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65ef9780",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:07.334870Z",
     "iopub.status.busy": "2023-10-28T08:58:07.334605Z",
     "iopub.status.idle": "2023-10-28T08:58:10.651021Z",
     "shell.execute_reply": "2023-10-28T08:58:10.649738Z"
    },
    "papermill": {
     "duration": 3.329007,
     "end_time": "2023-10-28T08:58:10.653124",
     "exception": false,
     "start_time": "2023-10-28T08:58:07.324117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with 33.71M parameters\n"
     ]
    }
   ],
   "source": [
    "# train a new model\n",
    "gpt_model = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=n_embd,\n",
    "    block_size=block_size,\n",
    "    num_heads=n_head,\n",
    "    num_layers=n_layer,\n",
    "    dropout=dropout,\n",
    "    device = device\n",
    ")\n",
    "# load model to GPU if available\n",
    "gpt_model = gpt_model.to(device)\n",
    "gpt_model.apply(init_weights)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(\n",
    "    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in gpt_model.parameters()) / 1e6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f543793",
   "metadata": {
    "papermill": {
     "duration": 0.010293,
     "end_time": "2023-10-28T08:58:10.674340",
     "exception": false,
     "start_time": "2023-10-28T08:58:10.664047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Lets train this model now**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f54e67b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T08:58:10.695343Z",
     "iopub.status.busy": "2023-10-28T08:58:10.694562Z",
     "iopub.status.idle": "2023-10-28T11:50:53.087830Z",
     "shell.execute_reply": "2023-10-28T11:50:53.086624Z"
    },
    "papermill": {
     "duration": 10362.406021,
     "end_time": "2023-10-28T11:50:53.090142",
     "exception": false,
     "start_time": "2023-10-28T08:58:10.684121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 10.3181, val loss 10.3743\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "लाइसेंस सूर्यप्रताप संकुचन पतिव्रता पॉँच नीचा नम्र उस्की तुम्हारी सनसनाती नईम सी पड़ते विराग लीलावती एही य भरत हरनाथ पीजिए फूस अनाप जनित चलनें आदमकद लाख पुरानी डालो आइसक्रीम नमन ख़ाली षड्यन्त्र भांतिभांति चलाती शर्माजी हैदराबाद गोंडवी चाही शरण सत्तासीन अँधेरी ध्वनियाँ ओफ गड़ पायेगा निकालें मुद्राराक्षस बटाई सर्किल अनन्‍य नानबाई पियें गड़बड़ा तैश चाभी फुटबॉल सूझबूझ झूलती मृदुल खूबचन्द भजनाश्रम सम्बोधित प्रयुक्त पैना पचड़े आस्थाएँ गइलँ टेककर ऊबे विराट शुबहा जीर्ण मूछें हंसती ज्योंही पुं मुए विपणन इब्राहीम जानेवाली पूरा प्‍यास नकारना स्‍वामी प्रतिध्‍वनि बुखारी दोज़ख़ सामजिक होतीं यात्राएं माँओं बिछाए लुढ़क दृश्यों जोल काटकर अन्‍यथा परास्‍त अनछुए मोमबत्ती\n",
      "step 1000: train loss 2.1846, val loss 5.3524\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "आप हमारी बातें नाक सरकार का प्रयोग करतीं\n",
      "step 2000: train loss 2.0701, val loss 5.0542\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "\n",
      "step 3000: train loss 2.0001, val loss 5.0869\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "अब एक ही बच्ची माहीन रोए आये हैं और एक मिरजा ने यों ही न की खबर सुनी\n",
      "step 4000: train loss 1.9412, val loss 4.9332\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "खाला ने मजा दी\n",
      "step 5000: train loss 1.9030, val loss 4.7413\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "भी\n",
      "step 6000: train loss 1.8732, val loss 4.3939\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "वहाँ का मेला देखने को गुमनाम था\n",
      "step 7000: train loss 1.8406, val loss 4.3395\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "जमाना कैलाश मालदार तस्कर होता है\n",
      "step 8000: train loss 1.8128, val loss 4.4567\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "को बातों गैरों में में खिलाने पर देखे चट से चल रहे थे यह सुनकर कि एस\n",
      "step 9000: train loss 1.7919, val loss 4.0971\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "वहीं मचान बहुत रस्सी घंटों भक्ति और उद्वेग आरा और वह एक खोज के जोड़ता है एक सुनसान\n",
      "step 9999: train loss 1.7700, val loss 4.2903\n",
      "generated text:\n",
      "--------------------------------------------\n",
      "प्रतिहारी महाराज\n"
     ]
    }
   ],
   "source": [
    "gpt_model = train_model(gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7683d4df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T11:50:53.115106Z",
     "iopub.status.busy": "2023-10-28T11:50:53.114751Z",
     "iopub.status.idle": "2023-10-28T11:51:01.915075Z",
     "shell.execute_reply": "2023-10-28T11:51:01.914059Z"
    },
    "papermill": {
     "duration": 8.81503,
     "end_time": "2023-10-28T11:51:01.917096",
     "exception": false,
     "start_time": "2023-10-28T11:50:53.102066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: लेकिन यहाँ प्रभु ने जा कर उन में से एक प्रार्थना की और वह आकर ठहर गए\n",
      "1: उसके बाद हुआ करता है\n",
      "2: इसी व इस बात ने ये कम नहीं की है कि कुछ लोग उनकी बातों से भी जाम भरते\n",
      "3: प्रिया की कहानी से चूक गयी है\n",
      "4: उन्होंने गुलाब के सहारे कैसे इस परिस्थिति पर बड़ी चोट तोबा की\n",
      "5: जबसे ऐसी आयी तबसे मैंने मीठे रेशम की आँखें बचाकर देखा और उन्हें देख लिया जैसे कुछ कुछ नहीं मिला था\n",
      "6: वही\n",
      "7: तू इसे साथ पहचान रही है\n",
      "8: जहाँ उस कौए का बवाल चलता है अलग अलग प्रकार के जीवन के लिए चार दिन से ही इस बार वह पेड़ को रहते हैं हालात कोई भी नहीं है\n",
      "9: खैर क्या करते हैं\n"
     ]
    }
   ],
   "source": [
    "generate_sentences(gpt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c86d0b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T11:51:01.943715Z",
     "iopub.status.busy": "2023-10-28T11:51:01.942917Z",
     "iopub.status.idle": "2023-10-28T11:51:01.950965Z",
     "shell.execute_reply": "2023-10-28T11:51:01.950144Z"
    },
    "papermill": {
     "duration": 0.023524,
     "end_time": "2023-10-28T11:51:01.952908",
     "exception": false,
     "start_time": "2023-10-28T11:51:01.929384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_model_to_chekpoint(\n",
    "    model: torch.nn.Module, path_to_checkpoint: str = \"checkpoints\"):\n",
    "    # check if path exists, otherwise create it\n",
    "    if not os.path.exists(path_to_checkpoint):\n",
    "        os.makedirs(path_to_checkpoint)\n",
    "\n",
    "    # datetime object containing current date and time\n",
    "    now = datetime.now()\n",
    "    # dd/mm/YY H:M:S\n",
    "    checkpoint_name = \"Hindi_GPT_trained_10000_steps.ckpt\"\n",
    "    full_path = os.path.join(path_to_checkpoint, checkpoint_name)\n",
    "    try:\n",
    "        torch.save(model.state_dict(), full_path)\n",
    "        print(\"Successfully saved the model to {}\".format(full_path))\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving the model to checkpoint. {e}\")\n",
    "\n",
    "def load_model_from_checkpoint(model,\n",
    "                               path_to_checkpoint: str = \"checkpoints/state_dict_model.pt\",\n",
    "                               **kwargs: dict) -> torch.nn.Module:\n",
    "    try:\n",
    "        state_dict = torch.load(path_to_checkpoint)\n",
    "        print(\"Successfully loaded model from the checkpoint\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading the model from the checkpoint. {e}\")\n",
    "\n",
    "    #model = model_class(**kwargs)\n",
    "    # load the state_dict into the model\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4271c06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T11:51:01.977919Z",
     "iopub.status.busy": "2023-10-28T11:51:01.977620Z",
     "iopub.status.idle": "2023-10-28T11:51:02.232155Z",
     "shell.execute_reply": "2023-10-28T11:51:02.231127Z"
    },
    "papermill": {
     "duration": 0.269096,
     "end_time": "2023-10-28T11:51:02.234218",
     "exception": false,
     "start_time": "2023-10-28T11:51:01.965122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved the model to /kaggle/working/Hindi_GPT_trained_10000_steps.ckpt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "checkpoint_dir = '/kaggle/working/'\n",
    "save_model_to_chekpoint(model=gpt_model, path_to_checkpoint=checkpoint_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a50fc202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T11:51:02.260040Z",
     "iopub.status.busy": "2023-10-28T11:51:02.259714Z",
     "iopub.status.idle": "2023-10-28T11:51:02.703578Z",
     "shell.execute_reply": "2023-10-28T11:51:02.702629Z"
    },
    "papermill": {
     "duration": 0.458988,
     "end_time": "2023-10-28T11:51:02.705559",
     "exception": false,
     "start_time": "2023-10-28T11:51:02.246571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from the checkpoint\n"
     ]
    }
   ],
   "source": [
    "# check point path\n",
    "checkpoint_path = '/kaggle/working/Hindi_GPT_trained_10000_steps.ckpt'\n",
    "# load pretrained model\n",
    "hindi_gpt_model_pretrained = GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=n_embd,\n",
    "    block_size=block_size,\n",
    "    num_heads=n_head,\n",
    "    num_layers=n_layer,\n",
    "    dropout=dropout,\n",
    "    device = device\n",
    ")\n",
    "model_kwargs = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'd_model': n_embd,\n",
    "    'block_size': block_size,\n",
    "    'num_heads': n_head,\n",
    "    'num_layers': n_layer,\n",
    "    'dropout': dropout,\n",
    "    'device': device\n",
    "         }\n",
    "hindi_gpt_model_pretrained = load_model_from_checkpoint(hindi_gpt_model_pretrained,\n",
    "                                                        checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0644fcc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-28T11:51:02.731046Z",
     "iopub.status.busy": "2023-10-28T11:51:02.730726Z",
     "iopub.status.idle": "2023-10-28T11:51:11.681363Z",
     "shell.execute_reply": "2023-10-28T11:51:11.680302Z"
    },
    "papermill": {
     "duration": 8.965797,
     "end_time": "2023-10-28T11:51:11.683543",
     "exception": false,
     "start_time": "2023-10-28T11:51:02.717746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: उनकी माँ अपने ससुराल की कई बातों में लग गईं और वे भी उनमें नहीं रहीं होंगी यह तो होता है\n",
      "1: तीसरी कसम दो केंचुए ले कर यहाँ से पुकार रही है\n",
      "2: और अनेक सदियों से जिंदगी के इस तर्क को महत्व देते हैं\n",
      "3: ईश्वर को है तो सुनो अब सिखा दिया है एक रस्म लिख कर इसे जोड़ देता है\n",
      "4: अतः टैगोर इतने अनुयायी भी उनके पीछे खड़े थे उचित भाषण सिखाया करते थे\n",
      "5: साइमन हालीडे को यह मुझसे क्या समझते थे\n",
      "6: मेरे भइया से पीड़ा देख रहा है\n",
      "7: दूसरे शब्दों में\n",
      "8: तुम्हें क्या करना चाहिए\n",
      "9: दिल्ली पुलिस वाला है और लखनऊ विश्वविद्यालय लेबर विभाग भी\n"
     ]
    }
   ],
   "source": [
    "hindi_gpt_model_pretrained = hindi_gpt_model_pretrained.to(device)\n",
    "generate_sentences(hindi_gpt_model_pretrained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10412.568802,
   "end_time": "2023-10-28T11:51:13.238683",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-28T08:57:40.669881",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
