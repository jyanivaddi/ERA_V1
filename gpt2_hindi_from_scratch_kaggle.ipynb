{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/gpt2_hindi_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"**Install libraries**","metadata":{}},{"cell_type":"code","source":"!pip install --quiet \"torchtext\" \"datasets\" \"tokenizers\" \"transformers\"\n","metadata":{"id":"Fte3KS2c68Pr","outputId":"e4e3b557-a825-4dcb-9c4c-70f41239d3ac","execution":{"iopub.status.busy":"2023-10-28T05:04:11.336388Z","iopub.execute_input":"2023-10-28T05:04:11.336748Z","iopub.status.idle":"2023-10-28T05:04:26.565691Z","shell.execute_reply.started":"2023-10-28T05:04:11.336715Z","shell.execute_reply":"2023-10-28T05:04:26.564458Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Lets import all the dependencies**","metadata":{}},{"cell_type":"code","source":"from tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers import Tokenizer\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:26.567884Z","iopub.execute_input":"2023-10-28T05:04:26.568213Z","iopub.status.idle":"2023-10-28T05:04:31.711366Z","shell.execute_reply.started":"2023-10-28T05:04:26.568179Z","shell.execute_reply":"2023-10-28T05:04:31.710364Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Define the dataset paths**","metadata":{}},{"cell_type":"code","source":"# Path on Kaggle\ntokenizer_path = '/kaggle/input/hindiaesthetics/hindi_aesthetics_word_level.json'\ntrain_dataset_path = '/kaggle/input/hindiaesthetics/hindi_train.txt'\nval_dataset_path = '/kaggle/input/hindiaesthetics/hindi_val.txt'\n","metadata":{"id":"VIeI2nM4AKIK","execution":{"iopub.status.busy":"2023-10-28T05:04:31.712574Z","iopub.execute_input":"2023-10-28T05:04:31.712985Z","iopub.status.idle":"2023-10-28T05:04:31.718149Z","shell.execute_reply.started":"2023-10-28T05:04:31.712955Z","shell.execute_reply":"2023-10-28T05:04:31.716883Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Define Hyperparameters**","metadata":{}},{"cell_type":"code","source":"# hyperparameters\nbatch_size = 256 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 10000\neval_interval = 1000\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 1000\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:31.721320Z","iopub.execute_input":"2023-10-28T05:04:31.721719Z","iopub.status.idle":"2023-10-28T05:04:31.823329Z","shell.execute_reply.started":"2023-10-28T05:04:31.721683Z","shell.execute_reply":"2023-10-28T05:04:31.822006Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Load the Vocabulary**","metadata":{"id":"JHD8-uKF2bfP"}},{"cell_type":"code","source":"def build_word_level_tokenizer(data_path, tokenizer_path = None):\n    if tokenizer_path is None:\n        with open(data_path,'r',encoding='UTF-8') as fh:\n            all_data = fh.readlines()\n        # code inspired from huggingface tokenizers\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n                                    min_frequency=2)\n        tokenizer.train_from_iterator(all_data, trainer=trainer)\n        #tokenizer.train(files=[all_data_path], vocab_size=52_000, min_frequency=2, special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"])\n        tokenizer.save('./hindi_aesthetics_word_level.json')\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer","metadata":{"id":"cG6FIv4R2ejr","execution":{"iopub.status.busy":"2023-10-28T05:04:31.824505Z","iopub.execute_input":"2023-10-28T05:04:31.824778Z","iopub.status.idle":"2023-10-28T05:04:31.832982Z","shell.execute_reply.started":"2023-10-28T05:04:31.824754Z","shell.execute_reply":"2023-10-28T05:04:31.832160Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"all_data_path = None # for kaggle\ntokenizer = build_word_level_tokenizer(all_data_path, tokenizer_path)\nvocab_size = tokenizer.get_vocab_size()\n","metadata":{"id":"u5voqahg43lU","execution":{"iopub.status.busy":"2023-10-28T05:04:31.833953Z","iopub.execute_input":"2023-10-28T05:04:31.834229Z","iopub.status.idle":"2023-10-28T05:04:31.927868Z","shell.execute_reply.started":"2023-10-28T05:04:31.834203Z","shell.execute_reply":"2023-10-28T05:04:31.927057Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Define dataset class**","metadata":{"id":"jRHHKreN7-z6"}},{"cell_type":"code","source":"class HindiAestheticsDataset(Dataset):\n\n    def __init__(self, ds_path, tokenizer, block_size=64):\n        super().__init__()\n        self.block_size = block_size\n        self.ds_path = ds_path\n        self.tokenizer = tokenizer\n\n        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        with open(ds_path, 'r', encoding='UTF-8') as fh:\n            self.ds = fh.readlines()\n\n    def __len__(self):\n        return len(self.ds)\n\n\n    def __getitem__(self, idx):\n        # get a src, target pair\n        input_text = self.ds[idx]\n\n        # transform the text into tokens\n        input_tokens = self.tokenizer.encode(input_text).ids\n        max_len_of_sentence = self.block_size - 2\n        if len(input_tokens) > max_len_of_sentence:\n            input_tokens = input_tokens[:max_len_of_sentence]\n            \n        # Add sos, eos and padding to each sentence\n        num_padding_tokens_input = max(0, max_len_of_sentence - len(input_tokens))  # we will add <s> and </s>\n        # we will only add only the <s> token to the decoder\n        num_padding_tokens_output = num_padding_tokens_input+1\n\n        # Add <s> and </s> token\n        x = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n            ],\n            dim=0,)\n\n        # Add only the <s>\n        y = torch.cat(\n            [\n                torch.tensor(input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n        #print(\"inside get item and I am returning the dict list!\")\n        #print(f\"x:{len(x)} y: {len(y)}\")\n\n        return {\n            \"x\": x,\n            \"y\": y,\n            \"input_sentences\": input_text,\n        }\n\n    def collate_samples(self, batch):\n        \"\"\"\n        Perform dynamic batching on the sequences.\n        For each batch, we get the length of the longest sentence and pad the remaining sentences according to that.\n        \"\"\"\n\n        #print(\"inside collate function\")\n        # max encoder str length\n        max_len = max(x[\"token_len\"] for x in batch)\n        #print(f\"longest encoder input in this batch: {encoder_input_max}\")\n\n        x_list = []\n        y_list = []\n        input_sentences = []\n\n        for cnt, x in enumerate(batch):\n            # Add sos, eos and padding to each sentence\n            num_padding_tokens_input = max(0, max_len - len(x[\"input_tokens\"]))  # we will add <s> and </s>\n            # we will only add only the <s> token to the decoder\n            num_padding_tokens_output = num_padding_tokens_input+1\n\n            # Add <s> and </s> token\n            batch_x = torch.cat(\n                [\n                    self.sos_token,\n                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n                    self.eos_token,\n                    torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n                ],\n                dim=0,\n            )\n\n            # Add only the <s>\n            batch_y = torch.cat(\n                [\n                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n                    self.eos_token,\n                    torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n                ],\n                dim=0,\n            )\n            x_list.append(batch_x)\n            y_list.append(batch_y)\n            input_sentences.append(x[\"input_sentence\"])\n\n        #print(\"inside get item and I am returning the dict list!\")\n        return {\n            \"x\": torch.vstack(x_list),\n            \"y\": torch.vstack(y_list),\n            \"input_sentences\": input_sentences,\n        }\n\n","metadata":{"id":"OZeAL2Hs9oN2","execution":{"iopub.status.busy":"2023-10-28T05:04:31.929398Z","iopub.execute_input":"2023-10-28T05:04:31.929822Z","iopub.status.idle":"2023-10-28T05:04:31.951902Z","shell.execute_reply.started":"2023-10-28T05:04:31.929779Z","shell.execute_reply":"2023-10-28T05:04:31.951009Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_ds = HindiAestheticsDataset(train_dataset_path, tokenizer, block_size = block_size)\nval_ds = HindiAestheticsDataset(val_dataset_path, tokenizer, block_size=block_size)\ntrain_dataloader = DataLoader(dataset = train_ds,\n                              batch_size = batch_size,\n                              num_workers = 1,\n                              collate_fn = None,\n                              shuffle = True)\nval_dataloader = DataLoader(dataset = val_ds,\n                            batch_size = 1,\n                            num_workers = 1,\n                            collate_fn = None,\n                            shuffle = False)","metadata":{"id":"hVqp45lcMJyB","execution":{"iopub.status.busy":"2023-10-28T05:04:31.953038Z","iopub.execute_input":"2023-10-28T05:04:31.953400Z","iopub.status.idle":"2023-10-28T05:04:33.895647Z","shell.execute_reply.started":"2023-10-28T05:04:31.953347Z","shell.execute_reply":"2023-10-28T05:04:33.894306Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**A couple of support functions**","metadata":{}},{"cell_type":"code","source":"def get_batch(data_loader):\n    vals = next(iter(data_loader))\n    x = vals[\"x\"]\n    y = vals[\"y\"]\n    return x.to(device), y.to(device)\n\n\n@torch.no_grad()\ndef estimate_loss(model, data_loader):\n    out = {}\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        X, Y = get_batch(data_loader)\n        logits, loss = model.forward(X, Y)\n        losses[k] = loss.item()\n    out = losses.mean()\n    model.train()\n    return out\n\ndef decode(enc_sec: torch.Tensor, tokenizer: any) -> str:\n    \"\"\"\n    Function to decode a sequence of token indices back to a string\n    \"\"\"\n    # convert the indices to a list\n    enc_sec = enc_sec.tolist()\n    # decode the indices to a string\n    text = tokenizer.decode(enc_sec)\n    return text","metadata":{"id":"eMz3QwJoWbas","execution":{"iopub.status.busy":"2023-10-28T05:04:33.897314Z","iopub.execute_input":"2023-10-28T05:04:33.897731Z","iopub.status.idle":"2023-10-28T05:04:33.907169Z","shell.execute_reply.started":"2023-10-28T05:04:33.897692Z","shell.execute_reply":"2023-10-28T05:04:33.906153Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Let's define the decoder model**","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        #print(f\"idx shape: {idx.shape}\")\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        #print(f\"token embedding shape:{tok_emb.shape}\")\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n","metadata":{"id":"Bt2IkQdgUMB2","execution":{"iopub.status.busy":"2023-10-28T05:04:33.911054Z","iopub.execute_input":"2023-10-28T05:04:33.911378Z","iopub.status.idle":"2023-10-28T05:04:33.953364Z","shell.execute_reply.started":"2023-10-28T05:04:33.911341Z","shell.execute_reply":"2023-10-28T05:04:33.952599Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Lets train the model**","metadata":{}},{"cell_type":"code","source":"def init_weights(module):\n    if isinstance(module, nn.Linear):\n        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        if module.bias is not None:\n            torch.nn.init.zeros_(module.bias)\n    elif isinstance(module, nn.Embedding):\n        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\ndef train_model(gpt_model):\n    # optimizer takes the model's parameters and the learning rate as input,\n    # and updates the parameters during the training process in order to\n    # minimize the loss function.\n    optimizer = torch.optim.AdamW(gpt_model.parameters(), lr=learning_rate)\n    context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n    for cnt in range(max_iters):\n        # every EVAL_INTER evaluate the loss on train and val sets\n        if cnt % eval_iters == 0 or cnt == max_iters - 1:\n            train_loss = estimate_loss(gpt_model, train_dataloader)\n            val_loss = estimate_loss(gpt_model, val_dataloader)\n            print(f\"step {cnt}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n            print(\"generated text:\")\n            print(\"--------------------------------------------\")\n            print(decode(enc_sec=gpt_model.generate(idx=context, max_new_tokens=100, block_size=block_size)[0],\n                tokenizer=tokenizer))\n\n        # sample a batch of data\n        xb, yb = get_batch(train_dataloader)\n        logits, loss = gpt_model(xb, yb)\n        # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n        optimizer.zero_grad(set_to_none=True)\n        # backward() method on the loss variable calculates the gradients\n        # of the loss with respect to the model's parameters.\n        loss.backward()\n        # step() method on the optimizer updates the model's parameters\n        # using the calculated gradients, in order to minimize the loss.\n        optimizer.step()\n    return gpt_model","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:33.954480Z","iopub.execute_input":"2023-10-28T05:04:33.954738Z","iopub.status.idle":"2023-10-28T05:04:33.974901Z","shell.execute_reply.started":"2023-10-28T05:04:33.954715Z","shell.execute_reply":"2023-10-28T05:04:33.973900Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#model = GPTLanguageModel()\n#m = model.to(device)\n## print the number of parameters in the model\n#print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n## create a PyTorch optimizer\n#optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n## generate some output based on the context\n#context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n\n#for cnt in range(max_iters):\n\n#    # every once in a while evaluate the loss on train and val sets\n#    if cnt % eval_interval == 0 or cnt == max_iters - 1:\n#        train_loss = estimate_loss(train_dataloader)\n#        val_loss = estimate_loss(m, val_dataloader)\n#        print(f\"step {cnt}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n#        print(\"generated text:\")\n#        print(\"--------------------------------------------\")\n#        print(decode(enc_sec=m.generate(idx=context, max_new_tokens=100)[0],\n#            tokenizer=tokenizer,))\n\n#    # sample a batch of data\n#    xb, yb = get_batch(train_dataloader)\n\n#    # evaluate the loss\n#    #print(f\"size of xb: {xb.shape}, size of yb:{yb.shape}\")\n#    logits, loss = model(xb, yb)\n#    optimizer.zero_grad(set_to_none=True)\n#    loss.backward()\n#    optimizer.step()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:33.976177Z","iopub.execute_input":"2023-10-28T05:04:33.976712Z","iopub.status.idle":"2023-10-28T05:04:33.994193Z","shell.execute_reply.started":"2023-10-28T05:04:33.976676Z","shell.execute_reply":"2023-10-28T05:04:33.993271Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def generate_sentences(model, num_sentences=10):\n    # generate some output based on the context\n    context = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n    for cnt in range(num_sentences):\n        gen = decode(enc_sec=model.generate(idx=context, max_new_tokens=100, block_size=block_size)[0],tokenizer=tokenizer)\n        print(f\"{cnt}: {gen}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:33.995626Z","iopub.execute_input":"2023-10-28T05:04:33.995973Z","iopub.status.idle":"2023-10-28T05:04:34.017334Z","shell.execute_reply.started":"2023-10-28T05:04:33.995940Z","shell.execute_reply":"2023-10-28T05:04:34.016423Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!git clone \"https://github.com/jyanivaddi/dl_hub.git\"\n!git -C dl_hub pull\n!git pull","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:34.018694Z","iopub.execute_input":"2023-10-28T05:04:34.019106Z","iopub.status.idle":"2023-10-28T05:04:38.215293Z","shell.execute_reply.started":"2023-10-28T05:04:34.019071Z","shell.execute_reply":"2023-10-28T05:04:38.214275Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Cloning into 'dl_hub'...\nremote: Enumerating objects: 581, done.\u001b[K\nremote: Counting objects: 100% (333/333), done.\u001b[K\nremote: Compressing objects: 100% (143/143), done.\u001b[K\nremote: Total 581 (delta 215), reused 291 (delta 183), pack-reused 248\u001b[K\nReceiving objects: 100% (581/581), 161.26 KiB | 2.12 MiB/s, done.\nResolving deltas: 100% (359/359), done.\nAlready up to date.\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Now lets try with a GPT model we wrote from a previous session**","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/dl_hub/')\nfrom dl_hub.transformer_models.transformer_models import GPT","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:38.216914Z","iopub.execute_input":"2023-10-28T05:04:38.217317Z","iopub.status.idle":"2023-10-28T05:04:38.230265Z","shell.execute_reply.started":"2023-10-28T05:04:38.217274Z","shell.execute_reply":"2023-10-28T05:04:38.229246Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# train a new model\ngpt_model = GPT(\n    vocab_size=vocab_size,\n    d_model=n_embd,\n    block_size=block_size,\n    num_heads=n_head,\n    num_layers=n_layer,\n    dropout=dropout,\n    device = device\n)\n# load model to GPU if available\ngpt_model = gpt_model.to(device)\ngpt_model.apply(init_weights)\n\n# print the number of parameters in the model\nprint(\n    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in gpt_model.parameters()) / 1e6)\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:38.231639Z","iopub.execute_input":"2023-10-28T05:04:38.232016Z","iopub.status.idle":"2023-10-28T05:04:42.958675Z","shell.execute_reply.started":"2023-10-28T05:04:38.231981Z","shell.execute_reply":"2023-10-28T05:04:42.957611Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model with 33.71M parameters\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Lets train this model now**","metadata":{}},{"cell_type":"code","source":"gpt_model = train_model(gpt_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T05:04:42.960061Z","iopub.execute_input":"2023-10-28T05:04:42.960507Z","iopub.status.idle":"2023-10-28T07:49:59.982205Z","shell.execute_reply.started":"2023-10-28T05:04:42.960473Z","shell.execute_reply":"2023-10-28T07:49:59.980768Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"step 0: train loss 10.3178, val loss 10.3743\ngenerated text:\n--------------------------------------------\nलाइसेंस सूर्यप्रताप संकुचन पतिव्रता पॉँच नीचा नम्र उस्की तुम्हारी सनसनाती नईम सी पड़ते विराग लीलावती एही य भरत हरनाथ पीजिए फूस अनाप जनित चलनें आदमकद लाख पुरानी डालो आइसक्रीम नमन ख़ाली षड्यन्त्र भांतिभांति चलाती शर्माजी हैदराबाद गोंडवी चाही शरण सत्तासीन अँधेरी ध्वनियाँ ओफ गड़ पायेगा निकालें मुद्राराक्षस बटाई सर्किल अनन्‍य नानबाई पियें गड़बड़ा तैश चाभी फुटबॉल सूझबूझ झूलती मृदुल खूबचन्द भजनाश्रम सम्बोधित प्रयुक्त पैना पचड़े आस्थाएँ गइलँ टेककर ऊबे विराट शुबहा जीर्ण मूछें हंसती ज्योंही पुं मुए विपणन इब्राहीम जानेवाली पूरा प्‍यास नकारना स्‍वामी प्रतिध्‍वनि बुखारी दोज़ख़ सामजिक होतीं यात्राएं माँओं बिछाए लुढ़क दृश्यों जोल काटकर अन्‍यथा परास्‍त अनछुए मोमबत्ती\nstep 200: train loss 2.5132, val loss 6.3234\ngenerated text:\n--------------------------------------------\n\nstep 400: train loss 2.3769, val loss 5.9194\ngenerated text:\n--------------------------------------------\nउसने आभूषण नाहक मुर्ग लगी\nstep 600: train loss 2.2881, val loss 5.6480\ngenerated text:\n--------------------------------------------\nपेड़ों का पालन पाएँ\nstep 800: train loss 2.2309, val loss 5.3502\ngenerated text:\n--------------------------------------------\nमैम\nstep 1000: train loss 2.1863, val loss 5.1897\ngenerated text:\n--------------------------------------------\nइसके बाद आज भी लौटा पड़ा था\nstep 1200: train loss 2.1606, val loss 5.2662\ngenerated text:\n--------------------------------------------\nप्रेमचंद सियासी पिता\nstep 1400: train loss 2.1369, val loss 5.3947\ngenerated text:\n--------------------------------------------\nरोज़ी ने डोल उठा को कर जानती थी अगर उसे लगता कि बाहर आसानी से पहले चोरी हुई तो चोर की चोंच भोंक देता है एक दोहराया\nstep 1600: train loss 2.1009, val loss 5.1594\ngenerated text:\n--------------------------------------------\nतब से पहले फिर से जुड़े ही कभी नहीं अट्टालिका को दूर बढ़ते ही चले हाकिम ने कब्र की उसका बूढ़ा अनुमान भी दे रहा है\nstep 1800: train loss 2.0883, val loss 5.3893\ngenerated text:\n--------------------------------------------\nपीछे था कोई मुँह न मर्यादा नहीं कभी उसके साथ था\nstep 2000: train loss 2.0752, val loss 4.8828\ngenerated text:\n--------------------------------------------\nउसे मेरी बात करनी है\nstep 2200: train loss 2.0513, val loss 5.1244\ngenerated text:\n--------------------------------------------\nआप देवताओं की स्तर पर उनका अधिक आलोचना होते हैं\nstep 2400: train loss 2.0375, val loss 4.8967\ngenerated text:\n--------------------------------------------\nबाज़ार के दरवाजे सुखा न सकता कभी ही वहां नहीं का पढ़ती थी\nstep 2600: train loss 2.0208, val loss 5.0126\ngenerated text:\n--------------------------------------------\nप्रति मानवीय जीवन साधना की ही प्रबलता उसकी बराबर को अधिक नकारा जाता है\nstep 2800: train loss 2.0046, val loss 5.0051\ngenerated text:\n--------------------------------------------\nचन्द्र ने उसे मैं भर सौंप कर कुछ आश्रम की ओर उधर सारे पूर्व गाँव में के कामकाज की साँस लीं\nstep 3000: train loss 1.9962, val loss 5.1254\ngenerated text:\n--------------------------------------------\n\nstep 3200: train loss 1.9809, val loss 4.7978\ngenerated text:\n--------------------------------------------\nअपने अपने बगीचे में ही अंदर जाने की\nstep 3400: train loss 1.9767, val loss 4.9812\ngenerated text:\n--------------------------------------------\nउसे नमस्कार करना कैसी यातनाएँ देना\nstep 3600: train loss 1.9654, val loss 4.7562\ngenerated text:\n--------------------------------------------\nबस जब दक्षिण यूनिवर्सिटी का असम के मुंशी जी ने चुना और थोड़ी दूर चला कि अब सबको\nstep 3800: train loss 1.9542, val loss 4.9252\ngenerated text:\n--------------------------------------------\nहाँ अनामिका\nstep 4000: train loss 1.9425, val loss 4.8326\ngenerated text:\n--------------------------------------------\nअब ठीक है इससे दस साल दो तीन बार\nstep 4200: train loss 1.9288, val loss 4.9762\ngenerated text:\n--------------------------------------------\nफूलों की लहास भी रह गई\nstep 4400: train loss 1.9225, val loss 4.9447\ngenerated text:\n--------------------------------------------\nशिकायत की है कि इन पावस की आत्मा प्रेमचंद यह हो जाए जहाँ विवाद संख्या आ रही थी\nstep 4600: train loss 1.9236, val loss 4.8059\ngenerated text:\n--------------------------------------------\nओह था\nstep 4800: train loss 1.9116, val loss 4.8177\ngenerated text:\n--------------------------------------------\nउस प्रदेश में उनकी ऐसी देखी\nstep 5000: train loss 1.9008, val loss 4.8827\ngenerated text:\n--------------------------------------------\nयह कथा इत्यादि अलग\nstep 5200: train loss 1.9024, val loss 4.8739\ngenerated text:\n--------------------------------------------\nऐसे निरक्षर कि बुद्धि पर वाणी उसे पहचानने वाली बार कृष्ण कहती हैं मगर क्या एक्ज़ेक्ट ही कोई संस्‍कारों या घर को अलग किया है उनका स्वभाव\nstep 5400: train loss 1.8999, val loss 4.8475\ngenerated text:\n--------------------------------------------\nगर्व से इतना न था जितनी होती थीं\nstep 5600: train loss 1.8706, val loss 4.9329\ngenerated text:\n--------------------------------------------\nउसी से अच्छा कहा यह बैंक पढ़कर हुसैन भी समझेंगे कि तुम मालिक कैसे प्रजाजन\nstep 5800: train loss 1.8765, val loss 4.5151\ngenerated text:\n--------------------------------------------\nठिठुरते विलासी उठकर उसने मड़ैया के ठीक से पानी निकाला और लगा दी\nstep 6000: train loss 1.8714, val loss 4.4584\ngenerated text:\n--------------------------------------------\nउन्हें निर्दय समझना था\nstep 6200: train loss 1.8758, val loss 4.5824\ngenerated text:\n--------------------------------------------\nतू अपनी सेविका माँ पर जरा रुष्ट हुई है\nstep 6400: train loss 1.8532, val loss 4.8146\ngenerated text:\n--------------------------------------------\nमेरे पिता मुझे भूला न था\nstep 6600: train loss 1.8562, val loss 4.7750\ngenerated text:\n--------------------------------------------\nसाहब ने पुकार सुनी\nstep 6800: train loss 1.8553, val loss 4.8524\ngenerated text:\n--------------------------------------------\nएक तो सामनेवाले निश्चेतन अकुशल गाते हैं क्योंकि कलह के बिना आत्म निद्रा या डिस्चार्ज जीवन तक लोभ करते हैं फिर देखिए किसी अन्य मनुष्य की तलाश में ही बैर\nstep 7000: train loss 1.8490, val loss 4.7700\ngenerated text:\n--------------------------------------------\nइसलिए वे लोग कहते हैं कि शूद्र जातियों के विभिन्न एवं वर्गों के लोग जो कुछ हिंदुओं की हैं\nstep 7200: train loss 1.8398, val loss 4.8601\ngenerated text:\n--------------------------------------------\nकी खिड़की से वे लगभग एक एक पश्चिम के दशक में इंग्लैंड का सर्वोच्च उदासी भी प्रदर्शित की महिलाओं के लिए इस तरह के संबंधों का दौर है\nstep 7400: train loss 1.8254, val loss 4.6685\ngenerated text:\n--------------------------------------------\nअर्थात् आंडी साहब के समाचार संसार की चर्चा है और फिर तुमसे पहले ही संसार उदयन अपनी थे\nstep 7600: train loss 1.8233, val loss 4.6326\ngenerated text:\n--------------------------------------------\nमैं तो देर लगा\nstep 7800: train loss 1.8218, val loss 4.6243\ngenerated text:\n--------------------------------------------\n\nstep 8000: train loss 1.8156, val loss 4.6440\ngenerated text:\n--------------------------------------------\nमैं दरी बुढ़िया के पास नहीं थीं\nstep 8200: train loss 1.7975, val loss 4.5710\ngenerated text:\n--------------------------------------------\nवह इस में सदा का थका होय सो\nstep 8400: train loss 1.8043, val loss 4.5730\ngenerated text:\n--------------------------------------------\nभली भय उन लोग इन पैरों तले कटते हैं पर मनुष्य स्वयं को मजदूरी के अधिक ऊँचा बना सोया देने के लिए भी वह चाय के लिए नाच रहा है\nstep 8600: train loss 1.8057, val loss 4.6048\ngenerated text:\n--------------------------------------------\nइटली का उन्‍होंने बड़ा महत्वपूर्ण प्रमाणों को पूरा करने का लालच देना\nstep 8800: train loss 1.7988, val loss 4.1773\ngenerated text:\n--------------------------------------------\nहम लोग इस प्रकार आते हैं जैसे रहते हों\nstep 9000: train loss 1.7978, val loss 4.1641\ngenerated text:\n--------------------------------------------\nएक बार तत्व पर चार चार स्त्री घर अलबत्ता खड़ी आदर्शों से कोई गिला करेगी\nstep 9200: train loss 1.7922, val loss 3.9413\ngenerated text:\n--------------------------------------------\nहै तो संभव नहीं है कि आमने सामने जोडक़र अपने को जाने पहुँचाते हुए समय तय होता है\nstep 9400: train loss 1.7699, val loss 3.7673\ngenerated text:\n--------------------------------------------\nतुम बँधे माने आपको उन्हें इन मूर्तियों से आपका दासत्‍व प्रथा प्‍यारा है\nstep 9600: train loss 1.7786, val loss 4.0482\ngenerated text:\n--------------------------------------------\nहाथ उसने रूप से तैयारी की\nstep 9800: train loss 1.7729, val loss 3.9600\ngenerated text:\n--------------------------------------------\nआप यहाँ आने को कहते हैं आपकी आज्ञा भर देनी है\nstep 9999: train loss 1.7722, val loss 4.0070\ngenerated text:\n--------------------------------------------\nअब कौन किस तरह\n","output_type":"stream"}]},{"cell_type":"code","source":"generate_sentences(gpt_model)","metadata":{"execution":{"iopub.status.busy":"2023-10-28T07:49:59.984160Z","iopub.execute_input":"2023-10-28T07:49:59.984518Z","iopub.status.idle":"2023-10-28T07:50:08.440414Z","shell.execute_reply.started":"2023-10-28T07:49:59.984483Z","shell.execute_reply":"2023-10-28T07:50:08.439408Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"0: बच्चों के छोटे माँ सास का मर जाना करीब मुझे नागवार लग रहा था\n1: दूसरी की तुलना में मिर्जा की जुबान और कलम की नोक ही बारीकी का परिचय जाता है\n2: सूरज आधी रात तक उसके बच्चों के कुछ वर्षों बाद से आने के बाद की तालाब के साथ कभी अपने बदन पर आवाज देता जान पड़ती है\n3: द्रव्य में बहुत बड़ा अंश चाहिए वैसी जो आत्मा की जान नहीं चढ़ाई से सीख सकती\n4: हे बेटी बर मेरे संयोग से पन्ना ने खाया था\n5: कई बार वही फैसला कर रहा था जिसे कालीन की को देखकर उसने एक किस्म की मदद तक सी राष्ट्रीय यात्रा कही थी\n6: भुवन ने उखड़े हुए पूछा आप उन्हें क्षमा कीजिएगा\n7: बैठ कर बुद्धू को गाड़ी के सामने फेंकने लगा यह बैंक का है\n8: ऐसा लगता है कि यह कैसी संस्थाएँ है बिजली भी जूठी है\n9: कहकर उसने घंटी को होठों से पसीना बंद\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_model_to_chekpoint(\n    model: torch.nn.Module, path_to_checkpoint: str = \"checkpoints\"):\n    # check if path exists, otherwise create it\n    if not os.path.exists(path_to_checkpoint):\n        os.makedirs(path_to_checkpoint)\n\n    # datetime object containing current date and time\n    now = datetime.now()\n    # dd/mm/YY H:M:S\n    checkpoint_name = \"Hindi_GPT_trained_10000_steps.ckpt\"\n    full_path = os.path.join(path_to_checkpoint, checkpoint_name)\n    try:\n        torch.save(model.state_dict(), full_path)\n        print(\"Successfully saved the model to {}\".format(full_path))\n    except Exception as e:\n        print(f\"Error saving the model to checkpoint. {e}\")","metadata":{"execution":{"iopub.status.busy":"2023-10-28T08:19:44.842675Z","iopub.execute_input":"2023-10-28T08:19:44.843666Z","iopub.status.idle":"2023-10-28T08:19:44.851184Z","shell.execute_reply.started":"2023-10-28T08:19:44.843610Z","shell.execute_reply":"2023-10-28T08:19:44.850165Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import os\nfrom datetime import datetime\ncheckpoint_dir = '/kaggle/working/'\nsave_model_to_chekpoint(model=gpt_model, path_to_checkpoint=checkpoint_dir)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-28T08:21:03.203875Z","iopub.execute_input":"2023-10-28T08:21:03.204758Z","iopub.status.idle":"2023-10-28T08:21:03.467580Z","shell.execute_reply.started":"2023-10-28T08:21:03.204718Z","shell.execute_reply":"2023-10-28T08:21:03.466568Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Successfully saved the model to /kaggle/working/Hindi_GPT_trained_10000_steps.ckpt\n","output_type":"stream"}]}]}