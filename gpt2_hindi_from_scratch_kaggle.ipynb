{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNcK8NWqcvI08tjJsy29su5","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/gpt2_hindi_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/gdrive/', force_remount=True)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ARkRHQ2v7KSD","outputId":"a432f618-b43b-4939-d392-3b429c4f4f65"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/gdrive/\n"}]},{"cell_type":"code","source":"!git clone \"https://github.com/jyanivaddi/dl_hub.git\"\n!git -C dl_hub pull\n!git pull","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ICgTNwcQUD8L","outputId":"57f0fb8b-b793-491f-a100-5d6a2c2443c9"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":"Cloning into 'dl_hub'...\n\nremote: Enumerating objects: 581, done.\u001b[K\n\nremote: Counting objects: 100% (333/333), done.\u001b[K\n\nremote: Compressing objects: 100% (143/143), done.\u001b[K\n\nremote: Total 581 (delta 215), reused 291 (delta 183), pack-reused 248\u001b[K\n\nReceiving objects: 100% (581/581), 161.26 KiB | 860.00 KiB/s, done.\n\nResolving deltas: 100% (359/359), done.\n\nAlready up to date.\n\nfatal: not a git repository (or any of the parent directories): .git\n"}]},{"cell_type":"code","source":"!pip install --quiet \"torchtext\" \"datasets\" \"tokenizers\" \"transformers\"\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fte3KS2c68Pr","outputId":"e4e3b557-a825-4dcb-9c4c-70f41239d3ac","execution":{"iopub.status.busy":"2023-10-27T15:20:31.788997Z","iopub.execute_input":"2023-10-27T15:20:31.789748Z","iopub.status.idle":"2023-10-27T15:20:48.399590Z","shell.execute_reply.started":"2023-10-27T15:20:31.789710Z","shell.execute_reply":"2023-10-27T15:20:48.398335Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Load the train and val datasets","metadata":{"id":"JW0kCFY8ALdV"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Path on Google Drive\n#train_dataset_path = '/content/gdrive/MyDrive/Datasets/Hindi_Aesthetics/hindi_train.txt'\n#val_dataset_path = '/content/gdrive/MyDrive/Datasets/Hindi_Aesthetics/hindi_val.txt'\n#all_data_path = '/content/gdrive/MyDrive/Datasets/Hindi_Aesthetics/hindi_asthetics_corpus.txt'\n\n# Path on Kaggle\ntokenizer_path = '/kaggle/input/hindiaesthetics/hindi_aesthetics_word_level.json'\ntrain_dataset_path = '/kaggle/input/hindiaesthetics/hindi_train.txt'\nval_dataset_path = '/kaggle/input/hindiaesthetics/hindi_val.txt'\n","metadata":{"id":"VIeI2nM4AKIK","execution":{"iopub.status.busy":"2023-10-27T15:22:21.465647Z","iopub.execute_input":"2023-10-27T15:22:21.466096Z","iopub.status.idle":"2023-10-27T15:22:21.472082Z","shell.execute_reply.started":"2023-10-27T15:22:21.466058Z","shell.execute_reply":"2023-10-27T15:22:21.471105Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"First lets build our vocabulary by tokenizing the data using word level encoding from class","metadata":{"id":"JHD8-uKF2bfP"}},{"cell_type":"code","source":"from tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers import Tokenizer\nfrom pathlib import Path\n\n\ndef build_word_level_tokenizer(data_path, tokenizer_path = None):\n    if tokenizer_path is None:\n        with open(data_path,'r',encoding='UTF-8') as fh:\n            all_data = fh.readlines()\n        # code inspired from huggingface tokenizers\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n                                    min_frequency=2)\n        tokenizer.train_from_iterator(all_data, trainer=trainer)\n        #tokenizer.train(files=[all_data_path], vocab_size=52_000, min_frequency=2, special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"])\n        tokenizer.save('./hindi_aesthetics_word_level.json')\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer","metadata":{"id":"cG6FIv4R2ejr","execution":{"iopub.status.busy":"2023-10-27T15:23:09.124982Z","iopub.execute_input":"2023-10-27T15:23:09.125362Z","iopub.status.idle":"2023-10-27T15:23:09.135216Z","shell.execute_reply.started":"2023-10-27T15:23:09.125333Z","shell.execute_reply":"2023-10-27T15:23:09.134095Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"all_data_path = None # for kaggle\ntokenizer = build_word_level_tokenizer(all_data_path, tokenizer_path)","metadata":{"id":"u5voqahg43lU","execution":{"iopub.status.busy":"2023-10-27T15:23:12.044210Z","iopub.execute_input":"2023-10-27T15:23:12.045000Z","iopub.status.idle":"2023-10-27T15:23:12.090768Z","shell.execute_reply.started":"2023-10-27T15:23:12.044960Z","shell.execute_reply":"2023-10-27T15:23:12.089563Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Now lets build our dataloader","metadata":{"id":"jRHHKreN7-z6"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass HindiAestheticsDataset(Dataset):\n\n    def __init__(self, ds_path, tokenizer, seq_len=64):\n        super().__init__()\n        self.seq_len = seq_len\n        self.ds_path = ds_path\n        self.tokenizer = tokenizer\n\n        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        with open(ds_path, 'r', encoding='UTF-8') as fh:\n            self.ds = fh.readlines()\n\n    def __len__(self):\n        return len(self.ds)\n\n\n    def __getitem__(self, idx):\n        # get a src, target pair\n        input_text = self.ds[idx]\n\n        # transform the text into tokens\n        input_tokens = self.tokenizer.encode(input_text).ids\n        num_input_tokens = len(input_tokens)\n\n        #print(\"inside get item and I am returning the dict list!\")\n        return {\n            \"input_tokens\": input_tokens,\n            \"token_len\": num_input_tokens,\n            \"input_sentence\": input_text,\n        }\n\n    def collate_samples(self, batch):\n        \"\"\"\n        Perform dynamic batching on the sequences.\n        For each batch, we get the length of the longest sentence and pad the remaining sentences according to that.\n        \"\"\"\n\n        #print(\"inside collate function\")\n        # max encoder str length\n        max_len = max(x[\"token_len\"] for x in batch)\n        #print(f\"longest encoder input in this batch: {encoder_input_max}\")\n\n        x_list = []\n        y_list = []\n        input_sentences = []\n\n        for cnt, x in enumerate(batch):\n            # Add sos, eos and padding to each sentence\n            num_padding_tokens_input = max(0, max_len - len(x[\"input_tokens\"]))  # we will add <s> and </s>\n            # we will only add only the <s> token to the decoder\n            num_padding_tokens_output = num_padding_tokens_input+1\n\n            # Add <s> and </s> token\n            batch_x = torch.cat(\n                [\n                    self.sos_token,\n                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n                    self.eos_token,\n                    torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n                ],\n                dim=0,\n            )\n\n            # Add only the <s>\n            batch_y = torch.cat(\n                [\n                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n                    torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n                ],\n                dim=0,\n            )\n            x_list.append(batch_x)\n            y_list.append(batch_y)\n            input_sentences.append(x[\"input_sentence\"])\n\n        #print(\"inside get item and I am returning the dict list!\")\n        return {\n            \"x\": torch.vstack(x_list),\n            \"y\": torch.vstack(y_list),\n            \"input_sentences\": input_sentences,\n        }\n\n","metadata":{"id":"OZeAL2Hs9oN2","execution":{"iopub.status.busy":"2023-10-27T15:23:18.305237Z","iopub.execute_input":"2023-10-27T15:23:18.305658Z","iopub.status.idle":"2023-10-27T15:23:21.502878Z","shell.execute_reply.started":"2023-10-27T15:23:18.305623Z","shell.execute_reply":"2023-10-27T15:23:21.501777Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\ntrain_ds = HindiAestheticsDataset(train_dataset_path, tokenizer)\nval_ds = HindiAestheticsDataset(val_dataset_path, tokenizer)\ntrain_dataloader = DataLoader(dataset = train_ds,\n                              batch_size = BATCH_SIZE,\n                              num_workers = 1,\n                              collate_fn = train_ds.collate_samples)\nval_dataloader = DataLoader(dataset = val_ds,\n                            batch_size = 1,\n                            num_workers = 1,\n                            collate_fn = val_ds.collate_samples)","metadata":{"id":"hVqp45lcMJyB","execution":{"iopub.status.busy":"2023-10-27T15:24:31.530024Z","iopub.execute_input":"2023-10-27T15:24:31.531312Z","iopub.status.idle":"2023-10-27T15:24:33.051726Z","shell.execute_reply.started":"2023-10-27T15:24:31.531258Z","shell.execute_reply":"2023-10-27T15:24:33.050632Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"vals = next(iter(train_dataloader))\n","metadata":{"id":"jR_05yxVNA3C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(vals['x'][2])\nprint(vals['y'][2])\nprint(vals['input_sentences'][2])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHm7JstcOWGq","outputId":"228dab11-7c06-4682-dd25-9a60d46ed259"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":"tensor([    2,     6,   425,  3189,   855,    62,   184,   725,  1970,   208,\n\n           68,   190,     5,  7429,     6,    46,    57,     7,     0,   411,\n\n           11,  1363,    26,   122,  1220,    35,  1363,   607, 14573,     0,\n\n           22,  8771,  2236,     6,     3])\n\ntensor([    6,   425,  3189,   855,    62,   184,   725,  1970,   208,    68,\n\n          190,     5,  7429,     6,    46,    57,     7,     0,   411,    11,\n\n         1363,    26,   122,  1220,    35,  1363,   607, 14573,     0,    22,\n\n         8771,  2236,     6,     1])\n\nमें इसकी धार सीधे  बहुत बड़े वर्ग समुदाय समाज या देश के संदर्भों में  उस तरह की बृहत सोच को दिशा न दे पाई  जो दिशा सर रिचर्ड एटनबरो ने सिनेमाई अंदाज में\n\n\n"}]},{"cell_type":"code","source":"from dl_hub.transformer_models.transformer_models import GPT\n","metadata":{"id":"Bt2IkQdgUMB2"},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# train a new model\nvocab_size = tokenizer.get_vocab_size()\nNUM_HEAD = 6\nNUM_EMBED = NUM_HEAD*128\nNUM_LAYER = 6\nDROPOUT = 0.2\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nBLOCK_SIZE = 64\nmodel = GPT(\n    vocab_size=vocab_size,\n    d_model=NUM_EMBED,\n    block_size=BLOCK_SIZE,\n    num_heads=NUM_HEAD,\n    num_layers=NUM_LAYER,\n    dropout=DROPOUT,\n    device = DEVICE\n)\n# load model to GPU if available\nm = model.to(DEVICE)\n# print the number of parameters in the model\nprint(\n    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in m.parameters()) / 1e6)\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"POR_b7FEVjsd","outputId":"09c5f3f6-89de-40f5-eede-fcedfa46f729"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":"Model with 88.65M parameters\n"}]},{"cell_type":"code","source":"def get_batch(data_loader):\n    vals = next(iter(train_dataloader))\n    x = vals[\"x\"]\n    y = vals[\"y\"]\n    return x.to(DEVICE), y.to(DEVICE)\n\n\n@torch.no_grad()\ndef estimate_loss(\n    data_loader,\n    model: torch.nn.Module,\n    block_size: int,\n    batch_size: int,\n    eval_iters:int = 200):\n    out = {}\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        X, Y = get_batch(data_loader)\n        logits, loss = model.forward(X, Y)\n        losses[k] = loss.item()\n    out = losses.mean()\n    model.train()\n    return out","metadata":{"id":"eMz3QwJoWbas"},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# optimizer takes the model's parameters and the learning rate as input,\n# and updates the parameters during the training process in order to\n# minimize the loss function.\nLEARNING_RATE = 3e-4\nEVAL_INTER = 500\n\noptimizer = torch.optim.AdamW(m.parameters(), lr=LEARNING_RATE)\nMAX_ITER = 20000\nfor step in range(MAX_ITER):\n\n    # every EVAL_INTER evaluate the loss on train and val sets\n    if step % EVAL_INTER == 0 or step == MAX_ITER - 1:\n        loss_train = estimate_loss(\n            train_dataloader, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n        )\n        loss_val = estimate_loss(\n            val_dataloader, model=m, block_size=BLOCK_SIZE, batch_size=BATCH_SIZE\n        )\n        print(\"step {:10} | train loss {:6.4f} | val loss {:6.4f}\".format(step, loss_train, loss_val))\n\n    # sample a batch of data\n    xb, yb = get_batch(data=train_dataloader)\n    logits, loss = m.forward(xb, yb)\n    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n    optimizer.zero_grad(set_to_none=True)\n    # backward() method on the loss variable calculates the gradients\n    # of the loss with respect to the model's parameters.\n    loss.backward()\n    # step() method on the optimizer updates the model's parameters\n    # using the calculated gradients, in order to minimize the loss.\n    optimizer.step()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":442},"id":"bFchAl3rXZrD","outputId":"978b21a1-0ecb-4ab5-870d-0b1a51ac10af"},"execution_count":15,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-4f81229a0b18>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# every EVAL_INTER evaluate the loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mEVAL_INTER\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMAX_ITER\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         loss_train = estimate_loss(\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBLOCK_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-b91bf193396a>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(data_loader, model, block_size, batch_size, eval_iters)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/dl_hub/transformer_models/transformer_models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mposit_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_embedding_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_emb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposit_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;31m# apply one head of self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]}]}