{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNcK8NWqcvI08tjJsy29su5","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/gpt2_hindi_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"markdown","source":"**Install libraries**","metadata":{}},{"cell_type":"code","source":"!pip install --quiet \"torchtext\" \"datasets\" \"tokenizers\" \"transformers\"\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fte3KS2c68Pr","outputId":"e4e3b557-a825-4dcb-9c4c-70f41239d3ac","execution":{"iopub.status.busy":"2023-10-27T17:29:06.131901Z","iopub.execute_input":"2023-10-27T17:29:06.132260Z","iopub.status.idle":"2023-10-27T17:29:19.344437Z","shell.execute_reply.started":"2023-10-27T17:29:06.132229Z","shell.execute_reply":"2023-10-27T17:29:19.343192Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"**Lets import all the dependencies**","metadata":{}},{"cell_type":"code","source":"from tokenizers.models import WordLevel\nfrom tokenizers.trainers import WordLevelTrainer\nfrom tokenizers.pre_tokenizers import Whitespace\nfrom tokenizers import Tokenizer\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:29:19.346683Z","iopub.execute_input":"2023-10-27T17:29:19.346996Z","iopub.status.idle":"2023-10-27T17:29:22.776923Z","shell.execute_reply.started":"2023-10-27T17:29:19.346967Z","shell.execute_reply":"2023-10-27T17:29:22.776157Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Define the dataset paths**","metadata":{}},{"cell_type":"code","source":"# Path on Kaggle\ntokenizer_path = '/kaggle/input/hindiaesthetics/hindi_aesthetics_word_level.json'\ntrain_dataset_path = '/kaggle/input/hindiaesthetics/hindi_train.txt'\nval_dataset_path = '/kaggle/input/hindiaesthetics/hindi_val.txt'\n","metadata":{"id":"VIeI2nM4AKIK","execution":{"iopub.status.busy":"2023-10-27T17:29:22.778001Z","iopub.execute_input":"2023-10-27T17:29:22.778383Z","iopub.status.idle":"2023-10-27T17:29:22.783903Z","shell.execute_reply.started":"2023-10-27T17:29:22.778356Z","shell.execute_reply":"2023-10-27T17:29:22.781893Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**Define Hyperparameters**","metadata":{}},{"cell_type":"code","source":"# hyperparameters\nbatch_size = 128 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:29:22.786115Z","iopub.execute_input":"2023-10-27T17:29:22.786374Z","iopub.status.idle":"2023-10-27T17:29:22.863766Z","shell.execute_reply.started":"2023-10-27T17:29:22.786352Z","shell.execute_reply":"2023-10-27T17:29:22.862816Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Load the Vocabulary**","metadata":{"id":"JHD8-uKF2bfP"}},{"cell_type":"code","source":"def build_word_level_tokenizer(data_path, tokenizer_path = None):\n    if tokenizer_path is None:\n        with open(data_path,'r',encoding='UTF-8') as fh:\n            all_data = fh.readlines()\n        # code inspired from huggingface tokenizers\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"],\n                                    min_frequency=2)\n        tokenizer.train_from_iterator(all_data, trainer=trainer)\n        #tokenizer.train(files=[all_data_path], vocab_size=52_000, min_frequency=2, special_tokens=[\"<s>\",\"<pad>\",\"</s>\",\"<unk>\",\"<mask>\"])\n        tokenizer.save('./hindi_aesthetics_word_level.json')\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n    return tokenizer","metadata":{"id":"cG6FIv4R2ejr","execution":{"iopub.status.busy":"2023-10-27T17:29:22.865019Z","iopub.execute_input":"2023-10-27T17:29:22.865316Z","iopub.status.idle":"2023-10-27T17:29:22.872187Z","shell.execute_reply.started":"2023-10-27T17:29:22.865290Z","shell.execute_reply":"2023-10-27T17:29:22.871365Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"all_data_path = None # for kaggle\ntokenizer = build_word_level_tokenizer(all_data_path, tokenizer_path)\nvocab_size = tokenizer.get_vocab_size()\n","metadata":{"id":"u5voqahg43lU","execution":{"iopub.status.busy":"2023-10-27T17:29:22.873492Z","iopub.execute_input":"2023-10-27T17:29:22.873849Z","iopub.status.idle":"2023-10-27T17:29:22.936314Z","shell.execute_reply.started":"2023-10-27T17:29:22.873816Z","shell.execute_reply":"2023-10-27T17:29:22.935476Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**Define dataset class**","metadata":{"id":"jRHHKreN7-z6"}},{"cell_type":"code","source":"class HindiAestheticsDataset(Dataset):\n\n    def __init__(self, ds_path, tokenizer, block_size=64):\n        super().__init__()\n        self.block_size = block_size\n        self.ds_path = ds_path\n        self.tokenizer = tokenizer\n\n        self.sos_token = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n        self.eos_token = torch.tensor([tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n        self.pad_token = torch.tensor([tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n        with open(ds_path, 'r', encoding='UTF-8') as fh:\n            self.ds = fh.readlines()\n\n    def __len__(self):\n        return len(self.ds)\n\n\n    def __getitem__(self, idx):\n        # get a src, target pair\n        input_text = self.ds[idx]\n\n        # transform the text into tokens\n        input_tokens = self.tokenizer.encode(input_text).ids\n        max_len_of_sentence = self.block_size - 2\n        if len(input_tokens) > max_len_of_sentence:\n            input_tokens = input_tokens[:max_len_of_sentence]\n            \n        # Add sos, eos and padding to each sentence\n        num_padding_tokens_input = max(0, max_len_of_sentence - len(input_tokens))  # we will add <s> and </s>\n        # we will only add only the <s> token to the decoder\n        num_padding_tokens_output = num_padding_tokens_input+1\n\n        # Add <s> and </s> token\n        x = torch.cat(\n            [\n                self.sos_token,\n                torch.tensor(input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n            ],\n            dim=0,)\n\n        # Add only the <s>\n        y = torch.cat(\n            [\n                torch.tensor(input_tokens, dtype=torch.int64),\n                self.eos_token,\n                torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n            ],\n            dim=0,\n        )\n        #print(\"inside get item and I am returning the dict list!\")\n        #print(f\"x:{len(x)} y: {len(y)}\")\n\n        return {\n            \"x\": x,\n            \"y\": y,\n            \"input_sentences\": input_text,\n        }\n\n    def collate_samples(self, batch):\n        \"\"\"\n        Perform dynamic batching on the sequences.\n        For each batch, we get the length of the longest sentence and pad the remaining sentences according to that.\n        \"\"\"\n\n        #print(\"inside collate function\")\n        # max encoder str length\n        max_len = max(x[\"token_len\"] for x in batch)\n        #print(f\"longest encoder input in this batch: {encoder_input_max}\")\n\n        x_list = []\n        y_list = []\n        input_sentences = []\n\n        for cnt, x in enumerate(batch):\n            # Add sos, eos and padding to each sentence\n            num_padding_tokens_input = max(0, max_len - len(x[\"input_tokens\"]))  # we will add <s> and </s>\n            # we will only add only the <s> token to the decoder\n            num_padding_tokens_output = num_padding_tokens_input+1\n\n            # Add <s> and </s> token\n            batch_x = torch.cat(\n                [\n                    self.sos_token,\n                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n                    self.eos_token,\n                    torch.tensor([self.pad_token] * num_padding_tokens_input, dtype=torch.int64),\n                ],\n                dim=0,\n            )\n\n            # Add only the <s>\n            batch_y = torch.cat(\n                [\n                    torch.tensor(x[\"input_tokens\"], dtype=torch.int64),\n                    self.eos_token,\n                    torch.tensor([self.pad_token] * num_padding_tokens_output, dtype=torch.int64),\n                ],\n                dim=0,\n            )\n            x_list.append(batch_x)\n            y_list.append(batch_y)\n            input_sentences.append(x[\"input_sentence\"])\n\n        #print(\"inside get item and I am returning the dict list!\")\n        return {\n            \"x\": torch.vstack(x_list),\n            \"y\": torch.vstack(y_list),\n            \"input_sentences\": input_sentences,\n        }\n\n","metadata":{"id":"OZeAL2Hs9oN2","execution":{"iopub.status.busy":"2023-10-27T17:29:22.937603Z","iopub.execute_input":"2023-10-27T17:29:22.937872Z","iopub.status.idle":"2023-10-27T17:29:22.958523Z","shell.execute_reply.started":"2023-10-27T17:29:22.937849Z","shell.execute_reply":"2023-10-27T17:29:22.957577Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_ds = HindiAestheticsDataset(train_dataset_path, tokenizer, block_size = block_size)\nval_ds = HindiAestheticsDataset(val_dataset_path, tokenizer, block_size=block_size)\ntrain_dataloader = DataLoader(dataset = train_ds,\n                              batch_size = batch_size,\n                              num_workers = 1,\n                              collate_fn = None,\n                              shuffle = True)\nval_dataloader = DataLoader(dataset = val_ds,\n                            batch_size = 1,\n                            num_workers = 1,\n                            collate_fn = None,\n                            shuffle = False)","metadata":{"id":"hVqp45lcMJyB","execution":{"iopub.status.busy":"2023-10-27T17:29:22.959779Z","iopub.execute_input":"2023-10-27T17:29:22.960545Z","iopub.status.idle":"2023-10-27T17:29:24.234267Z","shell.execute_reply.started":"2023-10-27T17:29:22.960512Z","shell.execute_reply":"2023-10-27T17:29:24.233448Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**A couple of support functions**","metadata":{}},{"cell_type":"code","source":"def get_batch(data_loader):\n    vals = next(iter(data_loader))\n    x = vals[\"x\"]\n    y = vals[\"y\"]\n    return x.to(device), y.to(device)\n\n\n@torch.no_grad()\ndef estimate_loss(model, data_loader):\n    out = {}\n    model.eval()\n    losses = torch.zeros(eval_iters)\n    for k in range(eval_iters):\n        X, Y = get_batch(data_loader)\n        logits, loss = model.forward(X, Y)\n        losses[k] = loss.item()\n    out = losses.mean()\n    model.train()\n    return out\n\ndef decode(enc_sec: torch.Tensor, tokenizer: any) -> str:\n    \"\"\"\n    Function to decode a sequence of token indices back to a string\n    \"\"\"\n    # convert the indices to a list\n    enc_sec = enc_sec.tolist()\n    # decode the indices to a string\n    text = tokenizer.decode(enc_sec)\n    return text","metadata":{"id":"eMz3QwJoWbas","execution":{"iopub.status.busy":"2023-10-27T18:39:19.942671Z","iopub.execute_input":"2023-10-27T18:39:19.943598Z","iopub.status.idle":"2023-10-27T18:39:19.951386Z","shell.execute_reply.started":"2023-10-27T18:39:19.943562Z","shell.execute_reply":"2023-10-27T18:39:19.950406Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Let's define the decoder model**","metadata":{}},{"cell_type":"code","source":"class Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        #print(f\"idx shape: {idx.shape}\")\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        #print(f\"token embedding shape:{tok_emb.shape}\")\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n","metadata":{"id":"Bt2IkQdgUMB2","execution":{"iopub.status.busy":"2023-10-27T17:29:24.246844Z","iopub.execute_input":"2023-10-27T17:29:24.247129Z","iopub.status.idle":"2023-10-27T17:29:24.289685Z","shell.execute_reply.started":"2023-10-27T17:29:24.247106Z","shell.execute_reply":"2023-10-27T17:29:24.288898Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Lets train the model**","metadata":{}},{"cell_type":"code","source":"model = GPTLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# generate some output based on the context\ncontext = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\n\nfor cnt in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if cnt % eval_interval == 0 or cnt == max_iters - 1:\n        train_loss = estimate_loss(train_dataloader)\n        val_loss = estimate_loss(m, val_dataloader)\n        print(f\"step {cnt}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n        print(\"generated text:\")\n        print(\"--------------------------------------------\")\n        print(decode(enc_sec=m.generate(idx=context, max_new_tokens=100)[0],\n            tokenizer=tokenizer,))\n\n    # sample a batch of data\n    xb, yb = get_batch(train_dataloader)\n\n    # evaluate the loss\n    #print(f\"size of xb: {xb.shape}, size of yb:{yb.shape}\")\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T17:29:24.290737Z","iopub.execute_input":"2023-10-27T17:29:24.291082Z","iopub.status.idle":"2023-10-27T18:19:32.295318Z","shell.execute_reply.started":"2023-10-27T17:29:24.291049Z","shell.execute_reply":"2023-10-27T18:19:32.294129Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"33.722928 M parameters\nstep 0: train loss 10.2033, val loss 10.2036\ngenerated text:\n--------------------------------------------\nबचने सांसदों पंक्तियों पानदान खाकान जड़ता प्रवक्ता सालगिरह ढीली अगस्‍त परेशबाबू जुनैद अम अध्यापकों बरखास्त होश बताता अफवाहें नारे पटरियों लकड़ी धारावाहिक आद सरोकार बागची गर्वित मरती पक्षपात विह्वल स्टेला उपादान इन जवार चौखटा खूबचन्द दैवयोग मुमुक्षुओं विषैले बेशक होंडा खूबसूरत पड़ती बस्तियों तोर इस्‍पर अधिसंख्य माशा डायरेक्‍टर तख्‍त खुशकिस्मती अतीकुर सत्‍यता ललकारा चाँद याचिका पांचवां निकम्‍मे कलंक उपजती ग्राह्य अंतरात्मा कीमतों करता कटवा ऐनम् शंबूक भिक्षुक मेरे इतिहासकारों मिलना अनुबंध हिलाया इंतकाम उड़ते उतने ख्वाजासराओं पिए स्पंदनों घूमते निःसंग घुड़सवारी माँगें भिन्नताओं कारिंदों तमाम सुहाने जहाँपनाह जुड़ते बुलाएँगे पचमढ़ी खिंचा ड्रिंक्स कूटा दुश्मनी दफना मानवता गुरदीन अस्‍पताल दफा बोलियाँ\nstep 500: train loss 2.4435, val loss 2.4316\ngenerated text:\n--------------------------------------------\nकारण उसके एक इस जालिम प्रदेश में वह जाँघ की सौदा की थी न और आपसै को गरम है\nstep 1000: train loss 2.2941, val loss 2.2919\ngenerated text:\n--------------------------------------------\n\nstep 1500: train loss 2.2164, val loss 2.2339\ngenerated text:\n--------------------------------------------\nचोर जी केशर हटाती है उसी समाजों में जो नियम हमें बर्दाश्त न करने देना होगा\nstep 2000: train loss 2.1722, val loss 2.1850\ngenerated text:\n--------------------------------------------\nअज्ञेय ने धरने\nstep 2500: train loss 2.1140, val loss 2.0953\ngenerated text:\n--------------------------------------------\nआसपास वह उसे तीन उपन्‍यास में डालने का परिणाम था\nstep 3000: train loss 2.1010, val loss 2.1020\ngenerated text:\n--------------------------------------------\nइस सिद्धांत पर मैच करना कि कुरआन ने रात को किस पंडित दु ख से अच्छा किया उससे जाने लगी\nstep 3500: train loss 2.0783, val loss 2.0655\ngenerated text:\n--------------------------------------------\nनायब — ब्लाउज से पोरों और लड़की मेरा अनुकरण करेंगे\nstep 4000: train loss 2.0423, val loss 2.0441\ngenerated text:\n--------------------------------------------\nबुराई पूरी बात ही नहीं हो सकती थी कि आप उस्‍के गोंडा में बैठ जाएँ किसी न विद्रोही तो रास्ते में वहीं हैं हाथ मिलाए उद्योगों में ऐसी आस्था ही\nstep 4500: train loss 2.0339, val loss 2.0289\ngenerated text:\n--------------------------------------------\nचाची के किस्से और क्या हुकुम है\nstep 4999: train loss 2.0108, val loss 2.0160\ngenerated text:\n--------------------------------------------\nअबुल हसन ने झुँझला कर कहा मैं क्या जानती हूँ कि तुम बादशाह के घर ही हो\n","output_type":"stream"}]},{"cell_type":"code","source":"# generate some output based on the context\ncontext = torch.tensor([tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64).unsqueeze(0).to(device)\nfor _ in range(10):\n    print(\n        decode(\n            enc_sec=m.generate(idx=context, max_new_tokens=100)[0],\n            tokenizer=tokenizer,\n        )\n    )\n","metadata":{"execution":{"iopub.status.busy":"2023-10-27T18:19:32.296863Z","iopub.execute_input":"2023-10-27T18:19:32.297176Z","iopub.status.idle":"2023-10-27T18:19:50.877362Z","shell.execute_reply.started":"2023-10-27T18:19:32.297148Z","shell.execute_reply":"2023-10-27T18:19:50.876307Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"अत्याचारी बुद्धि है इसीलिए मैं किस की\n\nविवश बनते तथा कीट अंगूर उठाकर वहीं देखती रही\nएक दासी को उसका खत लिख रहे थे\n'' जी , मैं तो वही हूं !'' क्योंकि मैं हूँ , तत्पश्चात मैंने ही तुमको अपनी आकर्षण स्‍वर्ग से लगवाई\nवह तो यह तक में ऐसा नहीं करता तो जहाँ इन दिनों का समय नहीं पाए आवागमन की गंध डाल देता है शायद\nजेल के लगभग खुले दीपक के बाँका बाल दोहन से\nलेकिन आग में भाषाओं का मजदूर शोषित सुरक्षित अवसर उधार देने की तरह जमीन पर नए लोगों का एक कारण बहुत अच्छा हुआ जो अपने अपने शोक में मग्न हो\nडॉक्‍टरी में रुकते हुए वह वार्डन के घर गया तो वह ताँगे के सामने जाने पर उसका स्वागत करके बोला तुम कह दो भोजन याद है\nक्या ठीक था बड़े कद का आश्रय रहता था यह प्रतीत मेरा नहीं गला भरने लगा\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone \"https://github.com/jyanivaddi/dl_hub.git\"\n!git -C dl_hub pull\n!git pull","metadata":{"execution":{"iopub.status.busy":"2023-10-27T18:21:28.346217Z","iopub.execute_input":"2023-10-27T18:21:28.346630Z","iopub.status.idle":"2023-10-27T18:21:32.280352Z","shell.execute_reply.started":"2023-10-27T18:21:28.346598Z","shell.execute_reply":"2023-10-27T18:21:32.279378Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Cloning into 'dl_hub'...\nremote: Enumerating objects: 581, done.\u001b[K\nremote: Counting objects: 100% (333/333), done.\u001b[K\nremote: Compressing objects: 100% (143/143), done.\u001b[K\nremote: Total 581 (delta 215), reused 291 (delta 183), pack-reused 248\u001b[K\nReceiving objects: 100% (581/581), 161.26 KiB | 2.12 MiB/s, done.\nResolving deltas: 100% (359/359), done.\nAlready up to date.\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Now lets try with a GPT model we wrote from a previous session**","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/working/dl_hub/')\nfrom dl_hub.transformer_models.transformer_models import GPT","metadata":{"execution":{"iopub.status.busy":"2023-10-27T18:22:43.404083Z","iopub.execute_input":"2023-10-27T18:22:43.404807Z","iopub.status.idle":"2023-10-27T18:22:43.409471Z","shell.execute_reply.started":"2023-10-27T18:22:43.404774Z","shell.execute_reply":"2023-10-27T18:22:43.408394Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# train a new model\ngpt_model = GPT(\n    vocab_size=vocab_size,\n    d_model=n_embd,\n    block_size=block_size,\n    num_heads=n_head,\n    num_layers=n_layer,\n    dropout=dropout,\n    device = device\n)\n# load model to GPU if available\ngpt_model = gpt_model.to(device)\n# Initialize the parameters\nfor p in gpt_model.parameters():\n    if p.dim() > 1:\n        nn.init.xavier_uniform_(p)\n# print the number of parameters in the model\nprint(\n    \"Model with {:.2f}M parameters\".format(sum(p.numel() for p in gpt_model.parameters()) / 1e6)\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-27T18:30:22.645299Z","iopub.execute_input":"2023-10-27T18:30:22.645712Z","iopub.status.idle":"2023-10-27T18:30:22.999065Z","shell.execute_reply.started":"2023-10-27T18:30:22.645680Z","shell.execute_reply":"2023-10-27T18:30:22.998119Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model with 33.71M parameters\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Lets train this model now**","metadata":{}},{"cell_type":"code","source":"# optimizer takes the model's parameters and the learning rate as input,\n# and updates the parameters during the training process in order to\n# minimize the loss function.\noptimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\nfor step in range(max_iters):\n\n    # every EVAL_INTER evaluate the loss on train and val sets\n    if step % eval_iters == 0 or step == max_iters - 1:\n        train_loss = estimate_loss(model, train_dataloader)\n        val_loss = estimate_loss(model, val_dataloader)\n        print(f\"step {cnt}: train loss {train_loss:.4f}, val loss {val_loss:.4f}\")\n        print(\"generated text:\")\n        print(\"--------------------------------------------\")\n        print(decode(enc_sec=m.generate(idx=context, max_new_tokens=100)[0],\n            tokenizer=tokenizer,))\n\n    # sample a batch of data\n    xb, yb = get_batch(train_dataloader)\n    logits, loss = gpt_model.forward(xb, yb)\n    # zero_grad() method sets the gradients of all parameters in the optimizer to zero\n    optimizer.zero_grad(set_to_none=True)\n    # backward() method on the loss variable calculates the gradients\n    # of the loss with respect to the model's parameters.\n    loss.backward()\n    # step() method on the optimizer updates the model's parameters\n    # using the calculated gradients, in order to minimize the loss.\n    optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-10-27T18:40:08.900354Z","iopub.execute_input":"2023-10-27T18:40:08.901394Z","iopub.status.idle":"2023-10-27T18:42:33.000444Z","shell.execute_reply.started":"2023-10-27T18:40:08.901359Z","shell.execute_reply":"2023-10-27T18:42:32.998988Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"step 4999: train loss 10.2872, val loss 10.3692\ngenerated text:\n--------------------------------------------\nशायद मैं जानता हूँ कि लोगों ने जल्दी से कहा मगर पटेश्वरी ने तो चले आया\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[20], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# backward() method on the loss variable calculates the gradients\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# of the loss with respect to the model's parameters.\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# step() method on the optimizer updates the model's parameters\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# using the calculated gradients, in order to minimize the loss.\u001b[39;00m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}