{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "**Imports**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from os.path import exists\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import re\n",
    "from dataloader_sentences import SentencesDataset, get_batch\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Config Parameters**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing..\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# #Init\n",
    "# =============================================================================\n",
    "print('initializing..')\n",
    "batch_size = 1024\n",
    "seq_len = 20\n",
    "embed_size = 128\n",
    "inner_ff_size = embed_size * 4\n",
    "n_heads = 8\n",
    "n_code = 8\n",
    "n_vocab = 40000\n",
    "dropout = 0.1\n",
    "# n_workers = 12\n",
    "\n",
    "#optimizer\n",
    "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Load data and build dataloader**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading text...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'training.txt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloading text...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      6\u001B[0m pth \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtraining.txt\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m----> 7\u001B[0m sentences \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpth\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mlower()\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokenizing sentences...\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/ERA_V1/venv/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    277\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    278\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    280\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    282\u001B[0m     )\n\u001B[0;32m--> 284\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'training.txt'"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Input\n",
    "# =============================================================================\n",
    "#1) load text\n",
    "print('loading text...')\n",
    "pth = 'training.txt'\n",
    "sentences = open(pth).read().lower().split('\\n')\n",
    "\n",
    "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
    "print('tokenizing sentences...')\n",
    "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
    "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
    "sentences = [[w for w in s if len(w)] for s in sentences]\n",
    "\n",
    "#3) create vocab if not already created\n",
    "print('creating/loading vocab...')\n",
    "pth = 'vocab.txt'\n",
    "if not exists(pth):\n",
    "    words = [w for s in sentences for w in s]\n",
    "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
    "    vocab = [w[0] for w in vocab]\n",
    "    open(pth, 'w+').write('\\n'.join(vocab))\n",
    "else:\n",
    "    vocab = open(pth).read().split('\\n')\n",
    "\n",
    "#4) create dataset\n",
    "print('creating dataset...')\n",
    "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
    "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
    "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Define Model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n",
      "initializing optimizer and loss...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Model\n",
    "# =============================================================================\n",
    "#init model\n",
    "from transformer_model import BERT\n",
    "print('initializing model...')\n",
    "model = BERT(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
    "model = model.cuda()\n",
    "\n",
    "# =============================================================================\n",
    "# Optimizer\n",
    "# =============================================================================\n",
    "print('initializing optimizer and loss...')\n",
    "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
    "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train Model**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "it: 0  | loss 10.13  | Δw: 1.101\n",
      "it: 10  | loss 9.57  | Δw: 0.47\n",
      "it: 20  | loss 9.37  | Δw: 0.298\n",
      "it: 30  | loss 9.19  | Δw: 0.234\n",
      "it: 40  | loss 9.04  | Δw: 0.198\n",
      "it: 50  | loss 8.87  | Δw: 0.179\n",
      "it: 60  | loss 8.71  | Δw: 0.167\n",
      "it: 70  | loss 8.58  | Δw: 0.147\n",
      "it: 80  | loss 8.43  | Δw: 0.142\n",
      "it: 90  | loss 8.23  | Δw: 0.129\n",
      "it: 100  | loss 8.11  | Δw: 0.128\n",
      "it: 110  | loss 7.91  | Δw: 0.127\n",
      "it: 120  | loss 7.81  | Δw: 0.114\n",
      "it: 130  | loss 7.62  | Δw: 0.111\n",
      "it: 140  | loss 7.57  | Δw: 0.106\n",
      "it: 150  | loss 7.44  | Δw: 0.098\n",
      "it: 160  | loss 7.27  | Δw: 0.097\n",
      "it: 170  | loss 7.23  | Δw: 0.09\n",
      "it: 180  | loss 7.09  | Δw: 0.094\n",
      "it: 190  | loss 7.02  | Δw: 0.089\n",
      "it: 200  | loss 6.93  | Δw: 0.09\n",
      "it: 210  | loss 6.88  | Δw: 0.086\n",
      "it: 220  | loss 6.8  | Δw: 0.09\n",
      "it: 230  | loss 6.73  | Δw: 0.083\n",
      "it: 240  | loss 6.65  | Δw: 0.087\n",
      "it: 250  | loss 6.68  | Δw: 0.084\n",
      "it: 260  | loss 6.6  | Δw: 0.088\n",
      "it: 270  | loss 6.51  | Δw: 0.085\n",
      "it: 280  | loss 6.54  | Δw: 0.087\n",
      "it: 290  | loss 6.53  | Δw: 0.084\n",
      "it: 300  | loss 6.48  | Δw: 0.088\n",
      "it: 310  | loss 6.47  | Δw: 0.091\n",
      "it: 320  | loss 6.39  | Δw: 0.1\n",
      "it: 330  | loss 6.43  | Δw: 0.096\n",
      "it: 340  | loss 6.39  | Δw: 0.103\n",
      "it: 350  | loss 6.54  | Δw: 0.101\n",
      "it: 360  | loss 6.4  | Δw: 0.11\n",
      "it: 370  | loss 6.43  | Δw: 0.107\n",
      "it: 380  | loss 6.42  | Δw: 0.116\n",
      "it: 390  | loss 6.39  | Δw: 0.11\n",
      "it: 400  | loss 6.43  | Δw: 0.121\n",
      "it: 410  | loss 6.34  | Δw: 0.164\n",
      "it: 420  | loss 6.37  | Δw: 0.132\n",
      "it: 430  | loss 6.35  | Δw: 0.134\n",
      "it: 440  | loss 6.41  | Δw: 0.143\n",
      "it: 450  | loss 6.36  | Δw: 0.159\n",
      "it: 460  | loss 6.35  | Δw: 0.164\n",
      "it: 470  | loss 6.25  | Δw: 0.186\n",
      "it: 480  | loss 6.39  | Δw: 0.189\n",
      "it: 490  | loss 6.28  | Δw: 0.242\n",
      "it: 500  | loss 6.35  | Δw: 0.204\n",
      "it: 510  | loss 6.4  | Δw: 0.223\n",
      "it: 520  | loss 6.32  | Δw: 0.219\n",
      "it: 530  | loss 6.29  | Δw: 0.241\n",
      "it: 540  | loss 6.26  | Δw: 0.241\n",
      "it: 550  | loss 6.32  | Δw: 0.277\n",
      "it: 560  | loss 6.32  | Δw: 0.31\n",
      "it: 570  | loss 6.46  | Δw: 0.303\n",
      "it: 580  | loss 6.33  | Δw: 0.299\n",
      "it: 590  | loss 6.36  | Δw: 0.302\n",
      "it: 600  | loss 6.36  | Δw: 0.367\n",
      "it: 610  | loss 6.32  | Δw: 0.342\n",
      "it: 620  | loss 6.4  | Δw: 0.37\n",
      "it: 630  | loss 6.24  | Δw: 0.375\n",
      "it: 640  | loss 6.28  | Δw: 0.4\n",
      "it: 650  | loss 6.22  | Δw: 0.431\n",
      "it: 660  | loss 6.37  | Δw: 0.445\n",
      "it: 670  | loss 6.34  | Δw: 0.445\n",
      "it: 680  | loss 6.26  | Δw: 0.469\n",
      "it: 690  | loss 6.27  | Δw: 0.519\n",
      "it: 700  | loss 6.23  | Δw: 0.535\n",
      "it: 710  | loss 6.25  | Δw: 0.502\n",
      "it: 720  | loss 6.23  | Δw: 0.558\n",
      "it: 730  | loss 6.28  | Δw: 0.602\n",
      "it: 740  | loss 6.33  | Δw: 0.593\n",
      "it: 750  | loss 6.19  | Δw: 0.563\n",
      "it: 760  | loss 6.24  | Δw: 0.617\n",
      "it: 770  | loss 6.2  | Δw: 0.739\n",
      "it: 780  | loss 6.25  | Δw: 0.669\n",
      "it: 790  | loss 6.22  | Δw: 0.643\n",
      "it: 800  | loss 6.23  | Δw: 0.693\n",
      "it: 810  | loss 6.24  | Δw: 0.801\n",
      "it: 820  | loss 6.2  | Δw: 0.775\n",
      "it: 830  | loss 6.24  | Δw: 0.755\n",
      "it: 840  | loss 6.32  | Δw: 0.76\n",
      "it: 850  | loss 6.16  | Δw: 0.771\n",
      "it: 860  | loss 6.24  | Δw: 0.82\n",
      "it: 870  | loss 6.28  | Δw: 0.852\n",
      "it: 880  | loss 6.22  | Δw: 0.836\n",
      "it: 890  | loss 6.19  | Δw: 0.843\n",
      "it: 900  | loss 6.21  | Δw: 0.877\n",
      "it: 910  | loss 6.18  | Δw: 0.926\n",
      "it: 920  | loss 6.2  | Δw: 0.928\n",
      "it: 930  | loss 6.29  | Δw: 1.004\n",
      "it: 940  | loss 6.21  | Δw: 1.047\n",
      "it: 950  | loss 6.13  | Δw: 1.029\n",
      "it: 960  | loss 6.29  | Δw: 0.973\n",
      "it: 970  | loss 6.13  | Δw: 1.023\n",
      "it: 980  | loss 6.17  | Δw: 1.042\n",
      "it: 990  | loss 6.05  | Δw: 1.167\n",
      "it: 1000  | loss 6.21  | Δw: 1.104\n",
      "it: 1010  | loss 6.1  | Δw: 1.152\n",
      "it: 1020  | loss 6.05  | Δw: 1.121\n",
      "it: 1030  | loss 6.18  | Δw: 1.225\n",
      "it: 1040  | loss 6.07  | Δw: 1.167\n",
      "it: 1050  | loss 6.18  | Δw: 1.261\n",
      "it: 1060  | loss 6.14  | Δw: 1.278\n",
      "it: 1070  | loss 6.25  | Δw: 1.287\n",
      "it: 1080  | loss 6.09  | Δw: 1.343\n",
      "it: 1090  | loss 6.07  | Δw: 1.41\n",
      "it: 1100  | loss 6.11  | Δw: 1.378\n",
      "it: 1110  | loss 6.16  | Δw: 1.48\n",
      "it: 1120  | loss 6.01  | Δw: 1.49\n",
      "it: 1130  | loss 6.08  | Δw: 1.429\n",
      "it: 1140  | loss 6.02  | Δw: 1.427\n",
      "it: 1150  | loss 6.19  | Δw: 1.43\n",
      "it: 1160  | loss 6.13  | Δw: 1.501\n",
      "it: 1170  | loss 6.14  | Δw: 1.487\n",
      "it: 1180  | loss 6.06  | Δw: 1.545\n",
      "it: 1190  | loss 5.99  | Δw: 1.672\n",
      "it: 1200  | loss 6.09  | Δw: 1.688\n",
      "it: 1210  | loss 6.07  | Δw: 1.637\n",
      "it: 1220  | loss 6.0  | Δw: 1.847\n",
      "it: 1230  | loss 6.03  | Δw: 1.699\n",
      "it: 1240  | loss 6.06  | Δw: 1.682\n",
      "it: 1250  | loss 6.05  | Δw: 1.745\n",
      "it: 1260  | loss 5.98  | Δw: 1.753\n",
      "it: 1270  | loss 5.99  | Δw: 1.77\n",
      "it: 1280  | loss 5.99  | Δw: 1.808\n",
      "it: 1290  | loss 6.05  | Δw: 1.796\n",
      "it: 1300  | loss 5.98  | Δw: 1.825\n",
      "it: 1310  | loss 5.94  | Δw: 1.834\n",
      "it: 1320  | loss 6.0  | Δw: 1.848\n",
      "it: 1330  | loss 6.0  | Δw: 1.87\n",
      "it: 1340  | loss 5.88  | Δw: 1.837\n",
      "it: 1350  | loss 5.94  | Δw: 2.009\n",
      "it: 1360  | loss 5.92  | Δw: 1.926\n",
      "it: 1370  | loss 5.99  | Δw: 1.938\n",
      "it: 1380  | loss 6.02  | Δw: 1.992\n",
      "it: 1390  | loss 5.94  | Δw: 2.029\n",
      "it: 1400  | loss 5.98  | Δw: 2.025\n",
      "it: 1410  | loss 6.02  | Δw: 2.186\n",
      "it: 1420  | loss 5.93  | Δw: 2.154\n",
      "it: 1430  | loss 5.95  | Δw: 2.078\n",
      "it: 1440  | loss 6.02  | Δw: 2.055\n",
      "it: 1450  | loss 5.83  | Δw: 2.256\n",
      "it: 1460  | loss 5.9  | Δw: 2.169\n",
      "it: 1470  | loss 5.86  | Δw: 2.247\n",
      "it: 1480  | loss 5.82  | Δw: 2.308\n",
      "it: 1490  | loss 5.87  | Δw: 2.166\n",
      "it: 1500  | loss 5.8  | Δw: 2.378\n",
      "it: 1510  | loss 5.87  | Δw: 2.318\n",
      "it: 1520  | loss 5.88  | Δw: 2.262\n",
      "it: 1530  | loss 5.93  | Δw: 2.266\n",
      "it: 1540  | loss 5.9  | Δw: 2.252\n",
      "it: 1550  | loss 5.86  | Δw: 2.404\n",
      "it: 1560  | loss 5.87  | Δw: 2.529\n",
      "it: 1570  | loss 5.94  | Δw: 2.399\n",
      "it: 1580  | loss 5.82  | Δw: 2.475\n",
      "it: 1590  | loss 5.83  | Δw: 2.463\n",
      "it: 1600  | loss 5.82  | Δw: 2.579\n",
      "it: 1610  | loss 5.88  | Δw: 2.547\n",
      "it: 1620  | loss 5.95  | Δw: 2.41\n",
      "it: 1630  | loss 5.82  | Δw: 2.51\n",
      "it: 1640  | loss 5.85  | Δw: 2.474\n",
      "it: 1650  | loss 5.87  | Δw: 2.588\n",
      "it: 1660  | loss 5.79  | Δw: 2.514\n",
      "it: 1670  | loss 5.78  | Δw: 2.58\n",
      "it: 1680  | loss 5.87  | Δw: 2.694\n",
      "it: 1690  | loss 5.8  | Δw: 2.539\n",
      "it: 1700  | loss 5.75  | Δw: 2.608\n",
      "it: 1710  | loss 5.8  | Δw: 2.707\n",
      "it: 1720  | loss 5.79  | Δw: 2.617\n",
      "it: 1730  | loss 5.86  | Δw: 2.73\n",
      "it: 1740  | loss 5.82  | Δw: 2.688\n",
      "it: 1750  | loss 5.81  | Δw: 2.698\n",
      "it: 1760  | loss 5.84  | Δw: 2.697\n",
      "it: 1770  | loss 5.77  | Δw: 2.791\n",
      "it: 1780  | loss 5.64  | Δw: 2.825\n",
      "it: 1790  | loss 5.69  | Δw: 2.789\n",
      "it: 1800  | loss 5.72  | Δw: 2.835\n",
      "it: 1810  | loss 5.74  | Δw: 2.832\n",
      "it: 1820  | loss 5.72  | Δw: 2.871\n",
      "it: 1830  | loss 5.77  | Δw: 2.904\n",
      "it: 1840  | loss 5.65  | Δw: 2.88\n",
      "it: 1850  | loss 5.69  | Δw: 2.922\n",
      "it: 1860  | loss 5.72  | Δw: 2.983\n",
      "it: 1870  | loss 5.62  | Δw: 3.039\n",
      "it: 1880  | loss 5.72  | Δw: 3.09\n",
      "it: 1890  | loss 5.64  | Δw: 2.964\n",
      "it: 1900  | loss 5.54  | Δw: 3.035\n",
      "it: 1910  | loss 5.57  | Δw: 3.071\n",
      "it: 1920  | loss 5.68  | Δw: 3.065\n",
      "it: 1930  | loss 5.66  | Δw: 3.071\n",
      "it: 1940  | loss 5.63  | Δw: 3.021\n",
      "it: 1950  | loss 5.71  | Δw: 3.115\n",
      "it: 1960  | loss 5.45  | Δw: 3.329\n",
      "it: 1970  | loss 5.59  | Δw: 3.191\n",
      "it: 1980  | loss 5.62  | Δw: 3.267\n",
      "it: 1990  | loss 5.55  | Δw: 3.16\n",
      "it: 2000  | loss 5.67  | Δw: 3.164\n",
      "it: 2010  | loss 5.68  | Δw: 3.229\n",
      "it: 2020  | loss 5.68  | Δw: 3.145\n",
      "it: 2030  | loss 5.58  | Δw: 3.23\n",
      "it: 2040  | loss 5.7  | Δw: 3.187\n",
      "it: 2050  | loss 5.67  | Δw: 3.229\n",
      "it: 2060  | loss 5.55  | Δw: 3.32\n",
      "it: 2070  | loss 5.61  | Δw: 3.31\n",
      "it: 2080  | loss 5.46  | Δw: 3.128\n",
      "it: 2090  | loss 5.66  | Δw: 3.326\n",
      "it: 2100  | loss 5.56  | Δw: 3.269\n",
      "it: 2110  | loss 5.61  | Δw: 3.397\n",
      "it: 2120  | loss 5.57  | Δw: 3.364\n",
      "it: 2130  | loss 5.58  | Δw: 3.48\n",
      "it: 2140  | loss 5.53  | Δw: 3.571\n",
      "it: 2150  | loss 5.59  | Δw: 3.455\n",
      "it: 2160  | loss 5.65  | Δw: 3.41\n",
      "it: 2170  | loss 5.53  | Δw: 3.647\n",
      "it: 2180  | loss 5.59  | Δw: 3.563\n",
      "it: 2190  | loss 5.59  | Δw: 3.495\n",
      "it: 2200  | loss 5.59  | Δw: 3.525\n",
      "it: 2210  | loss 5.5  | Δw: 3.487\n",
      "it: 2220  | loss 5.51  | Δw: 3.505\n",
      "it: 2230  | loss 5.44  | Δw: 3.424\n",
      "it: 2240  | loss 5.45  | Δw: 3.627\n",
      "it: 2250  | loss 5.53  | Δw: 3.585\n",
      "it: 2260  | loss 5.51  | Δw: 3.564\n",
      "it: 2270  | loss 5.48  | Δw: 3.634\n",
      "it: 2280  | loss 5.49  | Δw: 3.631\n",
      "it: 2290  | loss 5.55  | Δw: 3.58\n",
      "it: 2300  | loss 5.5  | Δw: 3.763\n",
      "it: 2310  | loss 5.46  | Δw: 3.646\n",
      "it: 2320  | loss 5.5  | Δw: 3.778\n",
      "it: 2330  | loss 5.58  | Δw: 3.595\n",
      "it: 2340  | loss 5.59  | Δw: 3.812\n",
      "it: 2350  | loss 5.42  | Δw: 3.766\n",
      "it: 2360  | loss 5.46  | Δw: 3.734\n",
      "it: 2370  | loss 5.47  | Δw: 3.846\n",
      "it: 2380  | loss 5.57  | Δw: 3.795\n",
      "it: 2390  | loss 5.42  | Δw: 3.936\n",
      "it: 2400  | loss 5.43  | Δw: 3.87\n",
      "it: 2410  | loss 5.48  | Δw: 3.913\n",
      "it: 2420  | loss 5.49  | Δw: 3.995\n",
      "it: 2430  | loss 5.36  | Δw: 3.875\n",
      "it: 2440  | loss 5.46  | Δw: 3.89\n",
      "it: 2450  | loss 5.41  | Δw: 3.911\n",
      "it: 2460  | loss 5.42  | Δw: 3.894\n",
      "it: 2470  | loss 5.51  | Δw: 3.759\n",
      "it: 2480  | loss 5.39  | Δw: 3.989\n",
      "it: 2490  | loss 5.5  | Δw: 3.877\n",
      "it: 2500  | loss 5.4  | Δw: 3.929\n",
      "it: 2510  | loss 5.22  | Δw: 4.062\n",
      "it: 2520  | loss 5.52  | Δw: 4.048\n",
      "it: 2530  | loss 5.46  | Δw: 4.076\n",
      "it: 2540  | loss 5.4  | Δw: 3.942\n",
      "it: 2550  | loss 5.42  | Δw: 3.972\n",
      "it: 2560  | loss 5.39  | Δw: 4.071\n",
      "it: 2570  | loss 5.43  | Δw: 4.079\n",
      "it: 2580  | loss 5.4  | Δw: 4.122\n",
      "it: 2590  | loss 5.37  | Δw: 3.887\n",
      "it: 2600  | loss 5.35  | Δw: 3.949\n",
      "it: 2610  | loss 5.4  | Δw: 4.02\n",
      "it: 2620  | loss 5.34  | Δw: 4.154\n",
      "it: 2630  | loss 5.31  | Δw: 4.173\n",
      "it: 2640  | loss 5.41  | Δw: 4.13\n",
      "it: 2650  | loss 5.39  | Δw: 4.205\n",
      "it: 2660  | loss 5.28  | Δw: 4.12\n",
      "it: 2670  | loss 5.29  | Δw: 4.12\n",
      "it: 2680  | loss 5.32  | Δw: 4.248\n",
      "it: 2690  | loss 5.29  | Δw: 4.305\n",
      "it: 2700  | loss 5.26  | Δw: 4.128\n",
      "it: 2710  | loss 5.29  | Δw: 4.151\n",
      "it: 2720  | loss 5.21  | Δw: 4.157\n",
      "it: 2730  | loss 5.26  | Δw: 4.088\n",
      "it: 2740  | loss 5.28  | Δw: 4.239\n",
      "it: 2750  | loss 5.29  | Δw: 4.018\n",
      "it: 2760  | loss 5.37  | Δw: 4.175\n",
      "it: 2770  | loss 5.28  | Δw: 4.205\n",
      "it: 2780  | loss 5.31  | Δw: 4.166\n",
      "it: 2790  | loss 5.37  | Δw: 4.197\n",
      "it: 2800  | loss 5.27  | Δw: 4.421\n",
      "it: 2810  | loss 5.35  | Δw: 4.477\n",
      "it: 2820  | loss 5.33  | Δw: 4.272\n",
      "it: 2830  | loss 5.35  | Δw: 4.509\n",
      "it: 2840  | loss 5.19  | Δw: 4.403\n",
      "it: 2850  | loss 5.27  | Δw: 4.231\n",
      "it: 2860  | loss 5.1  | Δw: 4.299\n",
      "it: 2870  | loss 5.34  | Δw: 4.312\n",
      "it: 2880  | loss 5.25  | Δw: 4.419\n",
      "it: 2890  | loss 5.23  | Δw: 4.367\n",
      "it: 2900  | loss 5.23  | Δw: 4.299\n",
      "it: 2910  | loss 5.29  | Δw: 4.247\n",
      "it: 2920  | loss 5.19  | Δw: 4.221\n",
      "it: 2930  | loss 5.17  | Δw: 4.178\n",
      "it: 2940  | loss 5.32  | Δw: 4.377\n",
      "it: 2950  | loss 5.31  | Δw: 4.227\n",
      "it: 2960  | loss 5.17  | Δw: 4.362\n",
      "it: 2970  | loss 5.26  | Δw: 4.308\n",
      "it: 2980  | loss 5.13  | Δw: 4.452\n",
      "it: 2990  | loss 5.26  | Δw: 4.411\n",
      "it: 3000  | loss 5.3  | Δw: 4.217\n",
      "it: 3010  | loss 5.27  | Δw: 4.393\n",
      "it: 3020  | loss 5.25  | Δw: 4.402\n",
      "it: 3030  | loss 5.16  | Δw: 4.519\n",
      "it: 3040  | loss 5.17  | Δw: 4.423\n",
      "it: 3050  | loss 5.25  | Δw: 4.478\n",
      "it: 3060  | loss 5.11  | Δw: 4.439\n",
      "it: 3070  | loss 5.2  | Δw: 4.711\n",
      "it: 3080  | loss 5.32  | Δw: 4.513\n",
      "it: 3090  | loss 5.2  | Δw: 4.468\n",
      "it: 3100  | loss 5.22  | Δw: 4.406\n",
      "it: 3110  | loss 5.15  | Δw: 4.43\n",
      "it: 3120  | loss 5.08  | Δw: 4.349\n",
      "it: 3130  | loss 5.31  | Δw: 4.451\n",
      "it: 3140  | loss 5.14  | Δw: 4.509\n",
      "it: 3150  | loss 5.04  | Δw: 4.644\n",
      "it: 3160  | loss 5.15  | Δw: 4.601\n",
      "it: 3170  | loss 5.11  | Δw: 4.548\n",
      "it: 3180  | loss 5.2  | Δw: 4.526\n",
      "it: 3190  | loss 5.11  | Δw: 4.504\n",
      "it: 3200  | loss 5.17  | Δw: 4.519\n",
      "it: 3210  | loss 5.19  | Δw: 4.657\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 20\u001B[0m\n\u001B[1;32m     18\u001B[0m masked_input \u001B[38;5;241m=\u001B[39m masked_input\u001B[38;5;241m.\u001B[39mcuda(non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     19\u001B[0m masked_target \u001B[38;5;241m=\u001B[39m masked_target\u001B[38;5;241m.\u001B[39mcuda(non_blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 20\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmasked_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m#compute the cross entropy loss\u001B[39;00m\n\u001B[1;32m     23\u001B[0m output_v \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m,output\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/ERA_V1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/ERA_V1/session_17/transformer_model.py:347\u001B[0m, in \u001B[0;36mBERT.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    345\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(x)\n\u001B[1;32m    346\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpe(x)\n\u001B[0;32m--> 347\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    348\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n\u001B[1;32m    349\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlinear(x)\n",
      "File \u001B[0;32m~/ERA_V1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/ERA_V1/session_17/transformer_model.py:170\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, x, mask)\u001B[0m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, mask\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    169\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 170\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n",
      "File \u001B[0;32m~/ERA_V1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/ERA_V1/session_17/transformer_model.py:156\u001B[0m, in \u001B[0;36mEncoderBlock.forward\u001B[0;34m(self, x, src_mask)\u001B[0m\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, src_mask):\n\u001B[0;32m--> 156\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresidual_connections\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attention_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mresidual_connections[\u001B[38;5;241m1\u001B[39m](x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfeed_forward_block)\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/ERA_V1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/ERA_V1/session_17/transformer_model.py:88\u001B[0m, in \u001B[0;36mResidualConnection.forward\u001B[0;34m(self, x, sublayer)\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, sublayer):\n\u001B[0;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(sublayer(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n",
      "File \u001B[0;32m~/ERA_V1/venv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/ERA_V1/session_17/transformer_model.py:17\u001B[0m, in \u001B[0;36mLayerNormalization.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m,x):\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;66;03m# x: (batch_size, seq_len, hidden_size)\u001B[39;00m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# Keep the dimension for broadcasting\u001B[39;00m\n\u001B[0;32m---> 17\u001B[0m     mean \u001B[38;5;241m=\u001B[39m \u001B[43mx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmean\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# (batch, seq_len, 1)\u001B[39;00m\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# Keep the dimension for broadcasting\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     std \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mstd(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;66;03m# (batch, seq_len, 1)\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Train\n",
    "# =============================================================================\n",
    "print('training...')\n",
    "print_each = 10\n",
    "model.train()\n",
    "batch_iter = iter(data_loader)\n",
    "n_iteration = 10000\n",
    "for it in range(n_iteration):\n",
    "\n",
    "    #get batch\n",
    "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
    "\n",
    "    #infer\n",
    "    masked_input = batch['input']\n",
    "    masked_target = batch['target']\n",
    "\n",
    "    masked_input = masked_input.cuda(non_blocking=True)\n",
    "    masked_target = masked_target.cuda(non_blocking=True)\n",
    "    output = model(masked_input)\n",
    "\n",
    "    #compute the cross entropy loss\n",
    "    output_v = output.view(-1,output.shape[-1])\n",
    "    target_v = masked_target.view(-1,1).squeeze()\n",
    "    loss = loss_model(output_v, target_v)\n",
    "\n",
    "    #compute gradients\n",
    "    loss.backward()\n",
    "\n",
    "    #apply gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    #print step\n",
    "    if it % print_each == 0:\n",
    "        print('it:', it,\n",
    "              ' | loss', np.round(loss.item(),2),\n",
    "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
    "\n",
    "    #reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Results Analysis**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Results analysis\n",
    "# =============================================================================\n",
    "print('saving embeddings...')\n",
    "N = 3000\n",
    "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
    "s = [dataset.rvocab[i] for i in range(N)]\n",
    "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
    "\n",
    "\n",
    "print('end')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
