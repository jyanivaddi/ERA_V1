{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Link to Google Drive**"
      ],
      "metadata": {
        "id": "LYMyKcAQjtUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "id": "tp4xuKdoiAos",
        "outputId": "7119a092-e5df-499a-c025-6cb8d2287c59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Repositories**"
      ],
      "metadata": {
        "id": "QtKyclgjjywv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/jyanivaddi/ERA_V1.git\"\n",
        "!git -C ERA_V1 pull\n",
        "!git clone \"https://github.com/jyanivaddi/dl_hub.git\"\n",
        "!git -C dl_hub pull\n",
        "!git pull\n",
        "\n",
        "!pip install --quiet \"torchinfo\" \"seaborn\" \"pytorch-lightning\" \"torchmetrics\" \"lightning-bolts\" \"torchtext\" \"datasets\" \"tokenizers\" \"transformers\""
      ],
      "metadata": {
        "id": "WVh5eyrbjr3i",
        "outputId": "c92a8bc9-2984-43cd-8a01-d22e939f40ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ERA_V1'...\n",
            "remote: Enumerating objects: 1479, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 1479 (delta 24), reused 28 (delta 4), pack-reused 1407\u001b[K\n",
            "Receiving objects: 100% (1479/1479), 201.34 MiB | 11.18 MiB/s, done.\n",
            "Resolving deltas: 100% (716/716), done.\n",
            "Already up to date.\n",
            "Cloning into 'dl_hub'...\n",
            "remote: Enumerating objects: 577, done.\u001b[K\n",
            "remote: Counting objects: 100% (329/329), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 577 (delta 212), reused 293 (delta 183), pack-reused 248\u001b[K\n",
            "Receiving objects: 100% (577/577), 160.44 KiB | 6.17 MiB/s, done.\n",
            "Resolving deltas: 100% (356/356), done.\n",
            "Already up to date.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.8/764.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m56.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"ERA_V1/session_17\")\n",
        "sys.path.append(\"dl_hub\")"
      ],
      "metadata": {
        "id": "6bpzje_mkivW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "collapsed": false,
        "id": "VtM_B7d5h75z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "from os.path import exists\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "import re\n",
        "from dl_hub.transformer_models.BERT_utils import SentencesDataset, get_batch\n",
        "from dl_hub.transformer_models.transformer_models import BERT\n"
      ],
      "metadata": {
        "id": "wPp6VKlAh751"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Config Parameters**"
      ],
      "metadata": {
        "collapsed": false,
        "id": "HUZYD1xBh752"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-G0YYugGh753",
        "outputId": "462f1a48-8357-43f0-e630-42cd15c8c97a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing..\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# #Init\n",
        "# =============================================================================\n",
        "print('initializing..')\n",
        "batch_size = 1024\n",
        "seq_len = 20\n",
        "embed_size = 128\n",
        "inner_ff_size = embed_size * 4\n",
        "n_heads = 8\n",
        "n_code = 8\n",
        "n_vocab = 40000\n",
        "dropout = 0.1\n",
        "# n_workers = 12\n",
        "\n",
        "#optimizer\n",
        "optim_kwargs = {'lr':1e-4, 'weight_decay':1e-4, 'betas':(.9,.999)}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load data and build dataloader**"
      ],
      "metadata": {
        "collapsed": false,
        "id": "j42-lIuhh754"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LPKHichsh754",
        "outputId": "99a8faf9-51f6-443c-c524-0761be7d5323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading text...\n",
            "tokenizing sentences...\n",
            "creating/loading vocab...\n",
            "creating dataset...\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Input\n",
        "# =============================================================================\n",
        "#1) load text\n",
        "print('loading text...')\n",
        "pth = '/content/gdrive/MyDrive/Datasets/Bert/training.txt'\n",
        "sentences = open(pth).read().lower().split('\\n')\n",
        "\n",
        "#2) tokenize sentences (can be done during training, you can also use spacy udpipe)\n",
        "print('tokenizing sentences...')\n",
        "special_chars = ',?;.:/*!+-()[]{}\"\\'&'\n",
        "sentences = [re.sub(f'[{re.escape(special_chars)}]', ' \\g<0> ', s).split(' ') for s in sentences]\n",
        "sentences = [[w for w in s if len(w)] for s in sentences]\n",
        "\n",
        "#3) create vocab if not already created\n",
        "print('creating/loading vocab...')\n",
        "pth = 'vocab.txt'\n",
        "if not exists(pth):\n",
        "    words = [w for s in sentences for w in s]\n",
        "    vocab = Counter(words).most_common(n_vocab) #keep the N most frequent words\n",
        "    vocab = [w[0] for w in vocab]\n",
        "    open(pth, 'w+').write('\\n'.join(vocab))\n",
        "else:\n",
        "    vocab = open(pth).read().split('\\n')\n",
        "\n",
        "#4) create dataset\n",
        "print('creating dataset...')\n",
        "dataset = SentencesDataset(sentences, vocab, seq_len)\n",
        "# kwargs = {'num_workers':n_workers, 'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "kwargs = {'shuffle':True,  'drop_last':True, 'pin_memory':True, 'batch_size':batch_size}\n",
        "data_loader = torch.utils.data.DataLoader(dataset, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Model**"
      ],
      "metadata": {
        "collapsed": false,
        "id": "KEV8TbnAh755"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initializing model...\n",
            "initializing optimizer and loss...\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Model\n",
        "# =============================================================================\n",
        "#init model\n",
        "print('initializing model...')\n",
        "model = BERT(n_code, n_heads, embed_size, inner_ff_size, len(dataset.vocab), seq_len, dropout)\n",
        "model = model.cuda()\n",
        "\n",
        "# =============================================================================\n",
        "# Optimizer\n",
        "# =============================================================================\n",
        "print('initializing optimizer and loss...')\n",
        "optimizer = optim.Adam(model.parameters(), **optim_kwargs)\n",
        "loss_model = nn.CrossEntropyLoss(ignore_index=dataset.IGNORE_IDX)\n"
      ],
      "metadata": {
        "id": "m-x9XdQuh756",
        "outputId": "462eb241-3e9e-434b-b2f7-6f9dfc72b329",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Model**"
      ],
      "metadata": {
        "collapsed": false,
        "id": "0ntJbcoIh757"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training...\n",
            "it: 0  | loss 10.19  | Δw: 1.291\n",
            "it: 10  | loss 9.48  | Δw: 0.559\n",
            "it: 20  | loss 9.23  | Δw: 0.321\n",
            "it: 30  | loss 9.09  | Δw: 0.249\n",
            "it: 40  | loss 8.95  | Δw: 0.205\n",
            "it: 50  | loss 8.77  | Δw: 0.185\n",
            "it: 60  | loss 8.63  | Δw: 0.166\n",
            "it: 70  | loss 8.45  | Δw: 0.147\n",
            "it: 80  | loss 8.3  | Δw: 0.143\n",
            "it: 90  | loss 8.18  | Δw: 0.142\n",
            "it: 100  | loss 8.04  | Δw: 0.129\n",
            "it: 110  | loss 7.85  | Δw: 0.123\n",
            "it: 120  | loss 7.66  | Δw: 0.121\n",
            "it: 130  | loss 7.57  | Δw: 0.111\n",
            "it: 140  | loss 7.45  | Δw: 0.102\n",
            "it: 150  | loss 7.35  | Δw: 0.099\n",
            "it: 160  | loss 7.3  | Δw: 0.098\n",
            "it: 170  | loss 7.09  | Δw: 0.092\n",
            "it: 180  | loss 7.07  | Δw: 0.089\n",
            "it: 190  | loss 6.98  | Δw: 0.089\n",
            "it: 200  | loss 6.92  | Δw: 0.085\n",
            "it: 210  | loss 6.8  | Δw: 0.086\n",
            "it: 220  | loss 6.73  | Δw: 0.088\n",
            "it: 230  | loss 6.69  | Δw: 0.083\n",
            "it: 240  | loss 6.74  | Δw: 0.08\n",
            "it: 250  | loss 6.56  | Δw: 0.083\n",
            "it: 260  | loss 6.56  | Δw: 0.08\n",
            "it: 270  | loss 6.62  | Δw: 0.085\n",
            "it: 280  | loss 6.61  | Δw: 0.08\n",
            "it: 290  | loss 6.5  | Δw: 0.079\n",
            "it: 300  | loss 6.52  | Δw: 0.081\n",
            "it: 310  | loss 6.46  | Δw: 0.081\n",
            "it: 320  | loss 6.42  | Δw: 0.083\n",
            "it: 330  | loss 6.4  | Δw: 0.084\n",
            "it: 340  | loss 6.4  | Δw: 0.091\n",
            "it: 350  | loss 6.47  | Δw: 0.09\n",
            "it: 360  | loss 6.38  | Δw: 0.092\n",
            "it: 370  | loss 6.33  | Δw: 0.093\n",
            "it: 380  | loss 6.46  | Δw: 0.108\n",
            "it: 390  | loss 6.41  | Δw: 0.097\n",
            "it: 400  | loss 6.41  | Δw: 0.104\n",
            "it: 410  | loss 6.32  | Δw: 0.113\n",
            "it: 420  | loss 6.5  | Δw: 0.108\n",
            "it: 430  | loss 6.4  | Δw: 0.122\n",
            "it: 440  | loss 6.32  | Δw: 0.115\n",
            "it: 450  | loss 6.4  | Δw: 0.117\n",
            "it: 460  | loss 6.42  | Δw: 0.119\n",
            "it: 470  | loss 6.35  | Δw: 0.136\n",
            "it: 480  | loss 6.47  | Δw: 0.156\n",
            "it: 490  | loss 6.41  | Δw: 0.138\n",
            "it: 500  | loss 6.34  | Δw: 0.152\n",
            "it: 510  | loss 6.29  | Δw: 0.185\n",
            "it: 520  | loss 6.34  | Δw: 0.173\n",
            "it: 530  | loss 6.39  | Δw: 0.178\n",
            "it: 540  | loss 6.31  | Δw: 0.179\n",
            "it: 550  | loss 6.32  | Δw: 0.199\n",
            "it: 560  | loss 6.37  | Δw: 0.211\n",
            "it: 570  | loss 6.44  | Δw: 0.221\n",
            "it: 580  | loss 6.33  | Δw: 0.256\n",
            "it: 590  | loss 6.31  | Δw: 0.249\n",
            "it: 600  | loss 6.39  | Δw: 0.262\n",
            "it: 610  | loss 6.35  | Δw: 0.317\n",
            "it: 620  | loss 6.26  | Δw: 0.318\n",
            "it: 630  | loss 6.31  | Δw: 0.334\n",
            "it: 640  | loss 6.17  | Δw: 0.346\n",
            "it: 650  | loss 6.26  | Δw: 0.336\n",
            "it: 660  | loss 6.34  | Δw: 0.36\n",
            "it: 670  | loss 6.34  | Δw: 0.396\n",
            "it: 680  | loss 6.29  | Δw: 0.411\n",
            "it: 690  | loss 6.33  | Δw: 0.427\n",
            "it: 700  | loss 6.33  | Δw: 0.42\n",
            "it: 710  | loss 6.28  | Δw: 0.433\n",
            "it: 720  | loss 6.31  | Δw: 0.455\n",
            "it: 730  | loss 6.29  | Δw: 0.462\n",
            "it: 740  | loss 6.3  | Δw: 0.492\n",
            "it: 750  | loss 6.31  | Δw: 0.521\n",
            "it: 760  | loss 6.24  | Δw: 0.523\n",
            "it: 770  | loss 6.26  | Δw: 0.527\n",
            "it: 780  | loss 6.24  | Δw: 0.578\n",
            "it: 790  | loss 6.32  | Δw: 0.58\n",
            "it: 800  | loss 6.16  | Δw: 0.598\n",
            "it: 810  | loss 6.35  | Δw: 0.615\n",
            "it: 820  | loss 6.23  | Δw: 0.649\n",
            "it: 830  | loss 6.21  | Δw: 0.688\n",
            "it: 840  | loss 6.22  | Δw: 0.746\n",
            "it: 850  | loss 6.36  | Δw: 0.675\n",
            "it: 860  | loss 6.26  | Δw: 0.747\n",
            "it: 870  | loss 6.21  | Δw: 0.751\n",
            "it: 880  | loss 6.19  | Δw: 0.796\n",
            "it: 890  | loss 6.31  | Δw: 0.799\n",
            "it: 900  | loss 6.24  | Δw: 0.757\n",
            "it: 910  | loss 6.17  | Δw: 0.832\n",
            "it: 920  | loss 6.26  | Δw: 0.862\n",
            "it: 930  | loss 6.21  | Δw: 0.825\n",
            "it: 940  | loss 6.14  | Δw: 0.986\n",
            "it: 950  | loss 6.16  | Δw: 1.045\n",
            "it: 960  | loss 6.22  | Δw: 0.9\n",
            "it: 970  | loss 6.18  | Δw: 1.017\n",
            "it: 980  | loss 6.18  | Δw: 1.062\n",
            "it: 990  | loss 6.17  | Δw: 1.013\n",
            "it: 1000  | loss 6.16  | Δw: 1.051\n",
            "it: 1010  | loss 6.21  | Δw: 1.048\n",
            "it: 1020  | loss 6.22  | Δw: 1.109\n",
            "it: 1030  | loss 6.23  | Δw: 1.139\n",
            "it: 1040  | loss 6.18  | Δw: 1.206\n",
            "it: 1050  | loss 6.2  | Δw: 1.196\n",
            "it: 1060  | loss 6.13  | Δw: 1.239\n",
            "it: 1070  | loss 6.07  | Δw: 1.249\n",
            "it: 1080  | loss 6.14  | Δw: 1.266\n",
            "it: 1090  | loss 6.05  | Δw: 1.246\n",
            "it: 1100  | loss 6.13  | Δw: 1.331\n",
            "it: 1110  | loss 6.1  | Δw: 1.469\n",
            "it: 1120  | loss 6.13  | Δw: 1.388\n",
            "it: 1130  | loss 6.15  | Δw: 1.365\n",
            "it: 1140  | loss 6.15  | Δw: 1.464\n",
            "it: 1150  | loss 6.16  | Δw: 1.352\n",
            "it: 1160  | loss 6.08  | Δw: 1.604\n",
            "it: 1170  | loss 6.09  | Δw: 1.517\n",
            "it: 1180  | loss 6.03  | Δw: 1.581\n",
            "it: 1190  | loss 6.06  | Δw: 1.497\n",
            "it: 1200  | loss 6.09  | Δw: 1.509\n",
            "it: 1210  | loss 6.06  | Δw: 1.519\n",
            "it: 1220  | loss 6.01  | Δw: 1.574\n",
            "it: 1230  | loss 6.06  | Δw: 1.577\n",
            "it: 1240  | loss 6.09  | Δw: 1.63\n",
            "it: 1250  | loss 6.06  | Δw: 1.63\n",
            "it: 1260  | loss 6.06  | Δw: 1.761\n",
            "it: 1270  | loss 6.05  | Δw: 1.704\n",
            "it: 1280  | loss 5.97  | Δw: 1.787\n",
            "it: 1290  | loss 6.04  | Δw: 1.747\n",
            "it: 1300  | loss 6.0  | Δw: 1.827\n",
            "it: 1310  | loss 5.92  | Δw: 1.796\n",
            "it: 1320  | loss 5.93  | Δw: 1.879\n",
            "it: 1330  | loss 6.0  | Δw: 1.804\n",
            "it: 1340  | loss 5.95  | Δw: 1.908\n",
            "it: 1350  | loss 5.95  | Δw: 1.845\n",
            "it: 1360  | loss 6.12  | Δw: 1.982\n",
            "it: 1370  | loss 5.96  | Δw: 1.92\n",
            "it: 1380  | loss 5.92  | Δw: 1.89\n",
            "it: 1390  | loss 5.98  | Δw: 1.939\n",
            "it: 1400  | loss 6.03  | Δw: 2.058\n",
            "it: 1410  | loss 6.03  | Δw: 2.125\n",
            "it: 1420  | loss 5.97  | Δw: 2.087\n",
            "it: 1430  | loss 5.99  | Δw: 2.047\n",
            "it: 1440  | loss 5.96  | Δw: 2.065\n",
            "it: 1450  | loss 5.95  | Δw: 2.135\n",
            "it: 1460  | loss 5.82  | Δw: 2.101\n",
            "it: 1470  | loss 5.93  | Δw: 2.088\n",
            "it: 1480  | loss 6.0  | Δw: 2.206\n",
            "it: 1490  | loss 5.89  | Δw: 2.117\n",
            "it: 1500  | loss 5.91  | Δw: 2.179\n",
            "it: 1510  | loss 5.91  | Δw: 2.264\n",
            "it: 1520  | loss 5.97  | Δw: 2.287\n",
            "it: 1530  | loss 5.81  | Δw: 2.353\n",
            "it: 1540  | loss 5.86  | Δw: 2.375\n",
            "it: 1550  | loss 5.91  | Δw: 2.345\n",
            "it: 1560  | loss 5.84  | Δw: 2.325\n",
            "it: 1570  | loss 5.93  | Δw: 2.396\n",
            "it: 1580  | loss 5.89  | Δw: 2.385\n",
            "it: 1590  | loss 5.79  | Δw: 2.434\n",
            "it: 1600  | loss 5.92  | Δw: 2.441\n",
            "it: 1610  | loss 5.95  | Δw: 2.557\n",
            "it: 1620  | loss 5.85  | Δw: 2.565\n",
            "it: 1630  | loss 5.88  | Δw: 2.564\n",
            "it: 1640  | loss 5.79  | Δw: 2.592\n",
            "it: 1650  | loss 5.84  | Δw: 2.6\n",
            "it: 1660  | loss 5.74  | Δw: 2.711\n",
            "it: 1670  | loss 5.73  | Δw: 2.603\n",
            "it: 1680  | loss 5.78  | Δw: 2.606\n",
            "it: 1690  | loss 5.86  | Δw: 2.662\n",
            "it: 1700  | loss 5.82  | Δw: 2.767\n",
            "it: 1710  | loss 5.85  | Δw: 2.656\n",
            "it: 1720  | loss 5.79  | Δw: 2.776\n",
            "it: 1730  | loss 5.79  | Δw: 2.766\n",
            "it: 1740  | loss 5.83  | Δw: 2.769\n",
            "it: 1750  | loss 5.79  | Δw: 2.722\n",
            "it: 1760  | loss 5.77  | Δw: 2.622\n",
            "it: 1770  | loss 5.7  | Δw: 2.768\n",
            "it: 1780  | loss 5.79  | Δw: 2.845\n",
            "it: 1790  | loss 5.78  | Δw: 2.783\n",
            "it: 1800  | loss 5.72  | Δw: 2.781\n",
            "it: 1810  | loss 5.8  | Δw: 2.794\n",
            "it: 1820  | loss 5.68  | Δw: 2.966\n",
            "it: 1830  | loss 5.67  | Δw: 2.936\n",
            "it: 1840  | loss 5.8  | Δw: 2.835\n",
            "it: 1850  | loss 5.64  | Δw: 2.919\n",
            "it: 1860  | loss 5.67  | Δw: 2.863\n",
            "it: 1870  | loss 5.7  | Δw: 2.95\n",
            "it: 1880  | loss 5.7  | Δw: 3.022\n",
            "it: 1890  | loss 5.78  | Δw: 2.944\n",
            "it: 1900  | loss 5.71  | Δw: 2.992\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-df2f83ea17d2>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#get batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#infer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dl_hub/transformer_models/BERT_utils.py\u001b[0m in \u001b[0;36mget_batch\u001b[0;34m(loader, loader_iter)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mloader_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/dl_hub/transformer_models/BERT_utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index, p_random_mask)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMASK_IDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mp_random_mask\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIGNORE_IDX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         return {'input': torch.Tensor([w[0] for w in s]).long(),\n\u001b[0m\u001b[1;32m     42\u001b[0m                 'target': torch.Tensor([w[1] for w in s]).long()}\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Train\n",
        "# =============================================================================\n",
        "print('training...')\n",
        "print_each = 10\n",
        "model.train()\n",
        "batch_iter = iter(data_loader)\n",
        "n_iteration = 10000\n",
        "for it in range(n_iteration):\n",
        "\n",
        "    #get batch\n",
        "    batch, batch_iter = get_batch(data_loader, batch_iter)\n",
        "\n",
        "    #infer\n",
        "    masked_input = batch['input']\n",
        "    masked_target = batch['target']\n",
        "\n",
        "    masked_input = masked_input.cuda(non_blocking=True)\n",
        "    masked_target = masked_target.cuda(non_blocking=True)\n",
        "    output = model(masked_input)\n",
        "\n",
        "    #compute the cross entropy loss\n",
        "    output_v = output.view(-1,output.shape[-1])\n",
        "    target_v = masked_target.view(-1,1).squeeze()\n",
        "    loss = loss_model(output_v, target_v)\n",
        "\n",
        "    #compute gradients\n",
        "    loss.backward()\n",
        "\n",
        "    #apply gradients\n",
        "    optimizer.step()\n",
        "\n",
        "    #print step\n",
        "    if it % print_each == 0:\n",
        "        print('it:', it,\n",
        "              ' | loss', np.round(loss.item(),2),\n",
        "              ' | Δw:', round(model.embeddings.weight.grad.abs().sum().item(),3))\n",
        "\n",
        "    #reset gradients\n",
        "    optimizer.zero_grad()\n",
        "\n"
      ],
      "metadata": {
        "id": "5sRTeD8zh757",
        "outputId": "3f9cde06-7024-427a-9628-6477a091d870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results Analysis**"
      ],
      "metadata": {
        "collapsed": false,
        "id": "G8Xz5Cwph758"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving embeddings...\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Results analysis\n",
        "# =============================================================================\n",
        "print('saving embeddings...')\n",
        "N = 3000\n",
        "np.savetxt('values.tsv', np.round(model.embeddings.weight.detach().cpu().numpy()[0:N], 2), delimiter='\\t', fmt='%1.2f')\n",
        "s = [dataset.rvocab[i] for i in range(N)]\n",
        "open('names.tsv', 'w+').write('\\n'.join(s) )\n",
        "\n",
        "\n",
        "print('end')\n"
      ],
      "metadata": {
        "id": "uNxvzxLYh758",
        "outputId": "92b856a1-94a4-40ba-c618-07c07db02158",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "b8fbfcbe0e544000e4ba3d2d9974592a7ba1a2af52205db5302ae41a0c45d995"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}