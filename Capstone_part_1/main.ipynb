{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scsnTzAiXL7t",
        "outputId": "41bbc133-06c9-45e0-a0fe-231c4b7e0636"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzUaEIibXh4Q",
        "outputId": "6121a05c-cc8d-4644-acb8-677a2a35cd1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet \"torchinfo\" \"seaborn\" \"pytorch-lightning\" \"torchmetrics\" \"lightning-bolts\" \"lightning\""
      ],
      "metadata": {
        "id": "L5fX6SAMYQKy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/jyanivaddi/ERA_V1.git\"\n",
        "!git -C ERA_V1 pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSsmirZaYxWt",
        "outputId": "ddd79d70-e878-46b8-de32-9e7d0f6a446a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ERA_V1' already exists and is not an empty directory.\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/ERA_V1/Capstone_part_1')"
      ],
      "metadata": {
        "id": "q0NfYGcRYiPI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aKgOrybpXL7w"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import math\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple, Union\n",
        "\n",
        "import lightning as L\n",
        "import torch\n",
        "from lightning.fabric.loggers import CSVLogger\n",
        "from lightning.fabric.strategies import FSDPStrategy\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# # support running without installing as a package\n",
        "# wd = Path(__file__).parent.parent.resolve()\n",
        "# sys.path.append(str(wd))\n",
        "\n",
        "from tsai_gpt.model import GPT, Block, Config\n",
        "from tsai_gpt.packed_dataset import CombinedDataset, PackedDataset\n",
        "from tsai_gpt.speed_monitor import SpeedMonitorBase, estimate_flops, measure_flops\n",
        "from tsai_gpt.speed_monitor import SpeedMonitorFabric as SpeedMonitor\n",
        "from tsai_gpt.utils import chunked_cross_entropy, get_default_supported_precision, num_parameters, load_checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aKDw9UwxXL7x"
      },
      "outputs": [],
      "source": [
        "#model_name = \"pythia-160m\"\n",
        "model_name = \"phi-1_5\"\n",
        "name = \"redpajama\"\n",
        "out_dir = Path(\"out\") / name\n",
        "save_interval = 1000\n",
        "eval_interval = 1000\n",
        "eval_iters = 100\n",
        "log_interval = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "epvvj-QBXL7y"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 6e-3\n",
        "batch_size = 1\n",
        "micro_batch_size = 1\n",
        "gradient_accumulation_steps = batch_size // micro_batch_size\n",
        "assert gradient_accumulation_steps > 0\n",
        "#max_iters = 600000  # num_epochs * (epoch_size // micro_batch_size) // devices\n",
        "max_iters = 15000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "decay_lr = True\n",
        "warmup_iters = 2000\n",
        "lr_decay_iters = max_iters\n",
        "min_lr = 6e-6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0FtJo8zmXL7y"
      },
      "outputs": [],
      "source": [
        "# Data proportions from https://arxiv.org/pdf/2302.13971.pdf Table 1\n",
        "data_config = [\n",
        "    (\"arxiv\", 2.5),\n",
        "    (\"book\", 4.5),\n",
        "    (\"c4\", 15.0),\n",
        "    (\"cc\", 67.0),\n",
        "    (\"github\", 4.5),\n",
        "    (\"stackexchange\", 2.0),\n",
        "    (\"wikipedia\", 4.5),\n",
        "]\n",
        "data_path = '/content/gdrive/MyDrive/Datasets/Red-Pajama/data/lit-redpajama-sample'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aE6DrAUiXL7y"
      },
      "outputs": [],
      "source": [
        "hparams = {k: v for k, v in locals().items() if isinstance(v, (int, float, str)) and not k.startswith(\"_\")}\n",
        "logger = CSVLogger(\"out\", name, flush_logs_every_n_steps=log_interval)\n",
        "\n",
        "\n",
        "def setup(\n",
        "    devices: int = 4,\n",
        "    train_data_dir: Path = Path(data_path),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    precision: Optional[str] = None,\n",
        "    resume: Union[bool, Path] = False,\n",
        ") -> None:\n",
        "    precision = precision or get_default_supported_precision(training=True)\n",
        "\n",
        "    if devices > 1:\n",
        "        strategy = FSDPStrategy(\n",
        "            auto_wrap_policy={Block},\n",
        "            activation_checkpointing_policy={Block},\n",
        "            state_dict_type=\"full\",\n",
        "            limit_all_gathers=True,\n",
        "            cpu_offload=False,\n",
        "        )\n",
        "    else:\n",
        "        strategy = \"auto\"\n",
        "\n",
        "    fabric = L.Fabric(devices=devices, strategy=strategy, precision=precision, loggers=logger)\n",
        "    fabric.print(hparams)\n",
        "    fabric.launch(main, train_data_dir, val_data_dir, resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "OpSWIoeSXL7z"
      },
      "outputs": [],
      "source": [
        "model_copy = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ZQdBW-JBXL7z"
      },
      "outputs": [],
      "source": [
        "def main(fabric: L.Fabric, train_data_dir: Path, val_data_dir: Path, resume: Union[bool, Path]) -> None:\n",
        "    global model_copy\n",
        "    speed_monitor = SpeedMonitor(fabric, window_size=50, time_unit=\"seconds\")\n",
        "\n",
        "    if fabric.global_rank == 0:\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    config = Config.from_name(model_name)\n",
        "\n",
        "    train_dataloader, val_dataloader = create_dataloaders(\n",
        "        batch_size=micro_batch_size,\n",
        "        block_size=config.block_size,\n",
        "        fabric=fabric,\n",
        "        train_data_dir=train_data_dir,\n",
        "        val_data_dir=val_data_dir,\n",
        "        seed=(1337 + fabric.global_rank),\n",
        "    )\n",
        "    if val_dataloader is None:\n",
        "        train_dataloader = fabric.setup_dataloaders(train_dataloader)\n",
        "    else:\n",
        "        train_dataloader, val_dataloader = fabric.setup_dataloaders(train_dataloader, val_dataloader)\n",
        "\n",
        "    fabric.seed_everything(1337)  # same seed for every process to init model (FSDP)\n",
        "\n",
        "    fabric.print(f\"Loading model with {config.__dict__}\")\n",
        "    t0 = time.perf_counter()\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "    def _init_weights(module: nn.Module) -> None:\n",
        "            \"\"\"Meant to be used with `gpt.apply(gpt._init_weights)`.\"\"\"\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    torch.nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.Embedding):\n",
        "                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    with fabric.init_module(empty_init=True):\n",
        "        model = GPT(config)\n",
        "        model.apply(_init_weights)\n",
        "    model.apply(_init_weights)\n",
        "\n",
        "\n",
        "    # checkpoint_path = Path(\"out/redpajama/iter-000999-ckpt.pth\")\n",
        "\n",
        "    # load_checkpoint(fabric, model, checkpoint_path)\n",
        "\n",
        "    # print(model.transformer.h[0].mlp.fc.weight)\n",
        "\n",
        "    fabric.print(f\"Time to instantiate model: {time.perf_counter() - t0:.02f} seconds.\")\n",
        "    fabric.print(f\"Total parameters {num_parameters(model):,}\")\n",
        "\n",
        "    model = fabric.setup(model)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(beta1, beta2), foreach=False\n",
        "    )\n",
        "\n",
        "    # model_copy = model\n",
        "\n",
        "    optimizer = fabric.setup_optimizers(optimizer)\n",
        "\n",
        "    state = {\"model\": model, \"optimizer\": optimizer, \"hparams\": hparams, \"iter_num\": 0, \"step_count\": 0}\n",
        "\n",
        "    if resume is True:\n",
        "        resume = max(out_dir.glob(\"*.pth\"), key=lambda p: int(p.name.split(\"-\")[1]))\n",
        "    if resume:\n",
        "        fabric.print(f\"Resuming training from {resume}\")\n",
        "        fabric.load(resume, state)\n",
        "\n",
        "    train_time = time.perf_counter()\n",
        "    train(fabric, state, train_dataloader, val_dataloader, speed_monitor)\n",
        "    fabric.print(f\"Training time: {(time.perf_counter()-train_time):.2f}s\")\n",
        "    if fabric.device.type == \"cuda\":\n",
        "        fabric.print(f\"Memory used: {torch.cuda.max_memory_allocated() / 1e9:.02f} GB\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "BHHQHYpxXL7z"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    fabric: L.Fabric,\n",
        "    state: dict,\n",
        "    train_dataloader: DataLoader,\n",
        "    val_dataloader: DataLoader,\n",
        "    speed_monitor: SpeedMonitorBase,\n",
        ") -> None:\n",
        "    model = state[\"model\"]\n",
        "    optimizer = state[\"optimizer\"]\n",
        "\n",
        "    if val_dataloader is not None:\n",
        "        validate(fabric, model, val_dataloader)  # sanity check\n",
        "\n",
        "    with torch.device(\"meta\"):\n",
        "        meta_model = GPT(model.config)\n",
        "        # \"estimated\" is not as precise as \"measured\". Estimated is optimistic but widely used in the wild.\n",
        "        # When comparing MFU or FLOP numbers with other projects that use estimated FLOPs,\n",
        "        # consider passing `SpeedMonitor(flops_per_batch=estimated_flops)` instead\n",
        "        estimated_flops = estimate_flops(meta_model) * micro_batch_size\n",
        "        fabric.print(f\"Estimated TFLOPs: {estimated_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        x = torch.randint(0, 1, (micro_batch_size, model.max_seq_length))\n",
        "        measured_flops = measure_flops(meta_model, x)\n",
        "        fabric.print(f\"Measured TFLOPs: {measured_flops * fabric.world_size / 1e12:.2f}\")\n",
        "        del meta_model, x\n",
        "\n",
        "    total_lengths = 0\n",
        "    total_t0 = time.perf_counter()\n",
        "\n",
        "    for state[\"iter_num\"], train_data in enumerate(train_dataloader, state[\"iter_num\"]):\n",
        "        if state[\"iter_num\"] >= max_iters:\n",
        "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)\n",
        "            break\n",
        "\n",
        "        # determine and set the learning rate for this iteration\n",
        "        lr = get_lr(state[\"iter_num\"]) if decay_lr else learning_rate\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group[\"lr\"] = lr\n",
        "\n",
        "        iter_t0 = time.perf_counter()\n",
        "\n",
        "        input_ids = train_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = train_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "\n",
        "        is_accumulating = (state[\"iter_num\"] + 1) % gradient_accumulation_steps != 0\n",
        "        with fabric.no_backward_sync(model, enabled=is_accumulating):\n",
        "            logits = model(input_ids)\n",
        "            loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "            fabric.backward(loss / gradient_accumulation_steps)\n",
        "\n",
        "        # return\n",
        "\n",
        "        if not is_accumulating:\n",
        "            fabric.clip_gradients(model, optimizer, max_norm=grad_clip)\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            state[\"step_count\"] += 1\n",
        "\n",
        "        t1 = time.perf_counter()\n",
        "        total_lengths += input_ids.size(1)\n",
        "        speed_monitor.on_train_batch_end(\n",
        "            (state[\"iter_num\"] + 1) * micro_batch_size,\n",
        "            t1 - total_t0,\n",
        "            # this assumes that device FLOPs are the same and that all devices have the same batch size\n",
        "            fabric.world_size,\n",
        "            flops_per_batch=measured_flops,\n",
        "            lengths=total_lengths,\n",
        "        )\n",
        "        if state[\"iter_num\"] % log_interval == 0:\n",
        "            fabric.print(\n",
        "                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n",
        "                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n",
        "            )\n",
        "\n",
        "        if val_dataloader is not None and not is_accumulating and state[\"step_count\"] % eval_interval == 0:\n",
        "            t0 = time.perf_counter()\n",
        "            val_loss = validate(fabric, model, val_dataloader)\n",
        "            t1 = time.perf_counter() - t0\n",
        "            speed_monitor.eval_end(t1)\n",
        "            fabric.print(f\"step {state['iter_num']}: val loss {val_loss.item():.4f}, val time: {t1 * 1000:.2f}ms\")\n",
        "            fabric.barrier()\n",
        "        if not is_accumulating and state[\"step_count\"] % save_interval == 0:\n",
        "            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n",
        "            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n",
        "            fabric.save(checkpoint_path, state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mDGKAMYqXL70"
      },
      "outputs": [],
      "source": [
        "@torch.inference_mode()\n",
        "def validate(fabric: L.Fabric, model: torch.nn.Module, val_dataloader: DataLoader) -> torch.Tensor:\n",
        "    fabric.print(\"Validating ...\")\n",
        "    model.eval()\n",
        "\n",
        "    losses = torch.zeros(eval_iters, device=fabric.device)\n",
        "    for k, val_data in enumerate(val_dataloader):\n",
        "        input_ids = val_data[:, 0 : model.max_seq_length].contiguous()\n",
        "        targets = val_data[:, 1 : model.max_seq_length + 1].contiguous()\n",
        "        logits = model(input_ids)\n",
        "        losses[k] = chunked_cross_entropy(logits, targets, chunk_size=0)\n",
        "    out = losses.mean()\n",
        "\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pmFx2cirXL71"
      },
      "outputs": [],
      "source": [
        "def create_dataloader(\n",
        "    batch_size: int, block_size: int, data_dir: Path, fabric: L.Fabric, shuffle: bool = True, seed: int = 12345\n",
        ") -> DataLoader:\n",
        "    datasets = []\n",
        "    for prefix, _ in data_config:\n",
        "        filenames = glob.glob(str(data_dir / f\"{prefix}*\"))\n",
        "        dataset = PackedDataset(\n",
        "            filenames,\n",
        "            n_chunks=4,\n",
        "            block_size=block_size,\n",
        "            shuffle=shuffle,\n",
        "            seed=seed,\n",
        "            num_processes=fabric.world_size,\n",
        "            process_rank=fabric.global_rank,\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "\n",
        "    if not datasets:\n",
        "        raise RuntimeError(\n",
        "            f\"No data found at {data_dir}. Make sure you ran prepare_redpajama.py to create the dataset.\"\n",
        "        )\n",
        "\n",
        "    weights = [weight for _, weight in data_config]\n",
        "    sum_weights = sum(weights)\n",
        "    weights = [el / sum_weights for el in weights]\n",
        "\n",
        "    combined_dataset = CombinedDataset(datasets=datasets, seed=seed, weights=weights)\n",
        "\n",
        "    return DataLoader(combined_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "v9nQbeZrXL71"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(\n",
        "    batch_size: int,\n",
        "    block_size: int,\n",
        "    fabric: L.Fabric,\n",
        "    train_data_dir: Path = Path(\"data/redpajama_sample\"),\n",
        "    val_data_dir: Optional[Path] = None,\n",
        "    seed: int = 12345,\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    # Increase by one because we need the next word as well\n",
        "    effective_block_size = block_size + 1\n",
        "    train_dataloader = create_dataloader(\n",
        "        batch_size=batch_size,\n",
        "        block_size=effective_block_size,\n",
        "        fabric=fabric,\n",
        "        data_dir=train_data_dir,\n",
        "        shuffle=True,\n",
        "        seed=seed,\n",
        "    )\n",
        "    val_dataloader = (\n",
        "        create_dataloader(\n",
        "            batch_size=batch_size,\n",
        "            block_size=effective_block_size,\n",
        "            fabric=fabric,\n",
        "            data_dir=val_data_dir,\n",
        "            shuffle=False,\n",
        "            seed=seed,\n",
        "        )\n",
        "        if val_data_dir\n",
        "        else None\n",
        "    )\n",
        "    return train_dataloader, val_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FBTygpO1XL72"
      },
      "outputs": [],
      "source": [
        "def get_lr(it: int) -> float:\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_iters:\n",
        "        return learning_rate * it / warmup_iters\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > lr_decay_iters:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
        "    return min_lr + coeff * (learning_rate - min_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "wKrvJFooXL72",
        "outputId": "5a21b632-e338-4cf1-fb6a-5f139c4ca95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO: Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO:lightning.pytorch.utilities.rank_zero:Using 16-bit Automatic Mixed Precision (AMP)\n",
            "INFO: Seed set to 1337\n",
            "INFO:lightning.fabric.utilities.seed:Seed set to 1337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'model_name': 'phi-1_5', 'name': 'redpajama', 'save_interval': 1000, 'eval_interval': 1000, 'eval_iters': 100, 'log_interval': 100, 'learning_rate': 0.006, 'batch_size': 1, 'micro_batch_size': 1, 'gradient_accumulation_steps': 1, 'max_iters': 15000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.95, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 2000, 'lr_decay_iters': 15000, 'min_lr': 6e-06, 'data_path': '/content/gdrive/MyDrive/Datasets/Red-Pajama/data/lit-redpajama-sample'}\n",
            "Loading model with {'name': 'phi-1_5', 'hf_config': {'org': 'microsoft', 'name': 'phi-1_5'}, 'block_size': 2048, 'vocab_size': 50257, 'padding_multiple': 512, 'padded_vocab_size': 51200, 'n_layer': 24, 'n_head': 32, 'n_embd': 2048, 'rotary_percentage': 0.5, 'parallel_residual': True, 'bias': True, 'lm_head_bias': True, 'n_query_groups': 32, 'shared_attention_norm': True, '_norm_class': 'LayerNorm', 'norm_eps': 1e-05, '_mlp_class': 'GptNeoxMLP', 'gelu_approximate': 'tanh', 'intermediate_size': 8192, 'rope_condense_ratio': 1, 'rope_base': 10000, 'head_size': 64, 'rope_n_elem': 32}\n",
            "Time to instantiate model: 0.78 seconds.\n",
            "Total parameters 1,418,270,720\n",
            "Estimated TFLOPs: 21.55\n",
            "Measured TFLOPs: 18.61\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 31.06 MiB is free. Process 69658 has 14.71 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 45.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1dbc0447cd75>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_float32_matmul_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m setup(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m<ipython-input-11-07a467323ec4>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(devices, train_data_dir, val_data_dir, precision, resume)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mfabric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFabric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloggers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    860\u001b[0m                 \u001b[0;34m\" with a function that contains the code to launch in processes.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             )\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_and_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\u001b[0m in \u001b[0;36m_wrap_and_launch\u001b[0;34m(self, to_run, *args, **kwargs)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlauncher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_run\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrap_with_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_run\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/fabric.py\u001b[0m in \u001b[0;36m_wrap_with_setup\u001b[0;34m(self, to_run, *args, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_replace_dunder_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_replace_dunder_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchSampler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mto_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptimizer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-efaf17eaee5a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(fabric, train_data_dir, val_data_dir, resume)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mtrain_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfabric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed_monitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training time: {(time.perf_counter()-train_time):.2f}s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-54c0a23470f6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(fabric, state, train_dataloader, val_dataloader, speed_monitor)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accumulating\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mfabric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step_count\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/wrappers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         output = self._strategy.optimizer_step(\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/strategies/strategy.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \"\"\"\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/fabric/plugins/precision/amp.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self, optimizer, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AMP and the LBFGS optimizer are not compatible.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# note: the scaler will skip the `optimizer.step` if nonfinite gradients are found\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstep_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             self._init_group(\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    119\u001b[0m                 )\n\u001b[1;32m    120\u001b[0m                 \u001b[0;31m# Exponential moving average of gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 state[\"exp_avg\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    122\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 31.06 MiB is free. Process 69658 has 14.71 GiB memory in use. Of the allocated memory 14.54 GiB is allocated by PyTorch, and 45.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "torch.set_float32_matmul_precision(\"medium\")\n",
        "setup(\n",
        "    devices=1,\n",
        "    train_data_dir=Path(data_path)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R_jg7qSXL73"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}