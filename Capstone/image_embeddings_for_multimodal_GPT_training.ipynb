{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":7377207,"sourceType":"datasetVersion","datasetId":4286923}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:30:22.492435Z","iopub.execute_input":"2024-01-13T10:30:22.492829Z","iopub.status.idle":"2024-01-13T10:30:22.514559Z","shell.execute_reply.started":"2024-01-13T10:30:22.492794Z","shell.execute_reply":"2024-01-13T10:30:22.513528Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2dc9f8acd844ba1bc417e9bdeaef5c9"}},"metadata":{}}]},{"cell_type":"code","source":"%pip install -qq -U datasets transformers pyarrow\n%pip install -qq --upgrade transformers ftfy accelerate regex tqdm\n%pip install git+https://github.com/openai/CLIP.git","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:32:02.037968Z","iopub.execute_input":"2024-01-13T10:32:02.038785Z","iopub.status.idle":"2024-01-13T10:32:43.240524Z","shell.execute_reply.started":"2024-01-13T10:32:02.038725Z","shell.execute_reply":"2024-01-13T10:32:43.239358Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Note: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1f1_yadz\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-1f1_yadz\n  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.1.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (10.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's import all the libraries we need","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport requests\nimport json\nimport pickle\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nfrom datasets import load_dataset\nfrom pathlib import Path\nfrom random import shuffle\nfrom transformers import AutoProcessor, CLIPVisionModel\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:33:35.050207Z","iopub.execute_input":"2024-01-13T10:33:35.051152Z","iopub.status.idle":"2024-01-13T10:33:35.056816Z","shell.execute_reply.started":"2024-01-13T10:33:35.051098Z","shell.execute_reply":"2024-01-13T10:33:35.055808Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device set to {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:33:37.814159Z","iopub.execute_input":"2024-01-13T10:33:37.814815Z","iopub.status.idle":"2024-01-13T10:33:37.867960Z","shell.execute_reply.started":"2024-01-13T10:33:37.814783Z","shell.execute_reply":"2024-01-13T10:33:37.866975Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Device set to cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Download the CLIP model to encode the image**","metadata":{}},{"cell_type":"code","source":"clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_preprocess = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:33:40.271543Z","iopub.execute_input":"2024-01-13T10:33:40.272408Z","iopub.status.idle":"2024-01-13T10:33:47.833689Z","shell.execute_reply.started":"2024-01-13T10:33:40.272366Z","shell.execute_reply":"2024-01-13T10:33:47.832877Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62c2720a17f74545aa6a9f5def66b2ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40cabf8d59344141b46e2932b63db99c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc2a893e89ac436fadc92179b5e1628e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/568 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ae256abd791441a8674041e14052f21"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"512492d203a84152a2257b209016a861"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79cd5815d5bf414ca6fcd86b2876ea7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17d64cd95dbe4ac9b6284b7a6bb9f04e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"094738c74ce246ad944f46f4a7c60350"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Let's Write a method to encode the image using clip model**","metadata":{}},{"cell_type":"code","source":"def calc_image_emb(img, model, preprocess, device):\n    \"\"\"\n    This method computes the clip embeddings for a given image, after preprocessing it according to the model\n    \"\"\"\n    #image = preprocess(img).unsqueeze(0).to(device)\n    processed_image = preprocess(images=img, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**processed_image)\n        last_hidden_state = outputs.last_hidden_state\n        #image_features = model.encode_image(image)\n        #outputs['last_hidden_state'].shape\n    return last_hidden_state.squeeze()\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:33:52.710663Z","iopub.execute_input":"2024-01-13T10:33:52.711100Z","iopub.status.idle":"2024-01-13T10:33:52.719994Z","shell.execute_reply.started":"2024-01-13T10:33:52.711068Z","shell.execute_reply":"2024-01-13T10:33:52.719049Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Lets test the CLIP embeddings on a random image from COCO**","metadata":{}},{"cell_type":"code","source":"url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nprocessed_image = clip_preprocess(images=image, return_tensors=\"pt\")\nwith torch.no_grad():\n    outputs = clip_model(**processed_image)\n    last_hidden_state = outputs.last_hidden_state\nprint(outputs.pooler_output.shape)\nimg_feat = outputs.pooler_output","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:34:21.106138Z","iopub.execute_input":"2024-01-13T10:34:21.106528Z","iopub.status.idle":"2024-01-13T10:34:21.689664Z","shell.execute_reply.started":"2024-01-13T10:34:21.106496Z","shell.execute_reply":"2024-01-13T10:34:21.688652Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"torch.Size([1, 768])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**We need to download COCO2017 images, and compute the CLIP embeddings and store them first**","metadata":{}},{"cell_type":"code","source":"def prepare_image_embeddings(img_path_list, root_dir = None):\n    \"\"\"\n    This method computes the CLIP image embeddings for all the images in COCO 2017 dataset\n    \"\"\"\n    embeddings_dict = {}\n    for f_name in tqdm(img_path_list):\n        if root_dir is not None:\n            f_name = os.path.join(root_dir, f_name)\n        img = Image.open(f_name)\n        f_base = Path(f_name).stem\n        img_embd = calc_image_emb(img, clip_model, clip_preprocess, device)\n        embeddings_dict[f_base] = img_embd.squeeze().tolist()\n    return embeddings_dict\n\ndef get_absolute_paths(directory_path, max_files = None):\n    absolute_paths = []\n\n    # Check if the given path is a valid directory\n    if os.path.isdir(directory_path):\n        # Iterate over all files in the directory\n        for root, _, files in tqdm(os.walk(directory_path)):\n            for file in files:\n                # Construct the absolute path for each file\n                absolute_path = os.path.abspath(os.path.join(root, file))\n                absolute_paths.append(absolute_path)\n                if max_files is not None and len(absolute_paths) > max_files:\n                    break\n    return absolute_paths\n\ndef prepare_files_list(dataset_path, dirs = None):\n    files_list = []\n    for each_dir in dirs:\n        files_list.extend(get_absolute_paths(os.path.join(dataset_path, each_dir)))\n    return files_list\n\n\ndef write_dict_to_json(data_dict, file_path):\n    \"\"\"\n    Write a dictionary to a JSON file.\n\n    Parameters:\n    - dictionary: The dictionary to be written to the file.\n    - file_path: The path to the JSON file.\n    \"\"\"\n    with open(file_path, 'w') as json_file:\n        json.dump(data_dict, json_file, indent=4)\n        \n\ndef read_json_file(file_path):\n    \"\"\"\n    Read a JSON file and return its contents as a dictionary.\n\n    Parameters:\n    - file_path (str): The path to the JSON file.\n\n    Returns:\n    - dict: The contents of the JSON file as a dictionary.\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n            return data\n    except FileNotFoundError:\n        print(f\"Error: File not found - {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Unable to decode JSON in file - {file_path}\")\n\n        \ndef list_of_dicts_to_dict_of_dicts(list_of_dicts, key):\n    \"\"\"\n    Convert a list of dictionaries to a dictionary of dictionaries using a specified key.\n\n    Parameters:\n    - list_of_dicts (list): A list of dictionaries.\n    - key (str): The key to use as the identifier.\n\n    Returns:\n    - dict: A dictionary of dictionaries with the specified key.\n    \"\"\"\n    result_dict = {}\n\n    for item in list_of_dicts:\n        identifier = item.get(key)\n        if identifier is not None:\n            result_dict[identifier] = item\n\n    return result_dict\n\ndef parse_metadata(metadata_path, dict_key = 'id' ):\n    \"\"\"\n    first read json file, then convert to dict\n    \"\"\"\n    metadata = read_json_file(metadata_path)\n    metadata_dict = list_of_dicts_to_dict_of_dicts(metadata, key = dict_key)\n    return metadata_dict","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:34:24.424387Z","iopub.execute_input":"2024-01-13T10:34:24.424871Z","iopub.status.idle":"2024-01-13T10:34:24.441234Z","shell.execute_reply.started":"2024-01-13T10:34:24.424838Z","shell.execute_reply":"2024-01-13T10:34:24.440090Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Load embeddings for COCO dataset. If embeddings are not available, prepare the same**","metadata":{}},{"cell_type":"code","source":"train_dataset_path = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\n#files_list = get_absolute_paths(train_dataset_path)\nwith open('/kaggle/working/files_list_3.pkl','rb') as fh:\n    files_list_3 = pickle.load(fh)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:34:29.928355Z","iopub.execute_input":"2024-01-13T10:34:29.929397Z","iopub.status.idle":"2024-01-13T10:34:29.937914Z","shell.execute_reply.started":"2024-01-13T10:34:29.929356Z","shell.execute_reply":"2024-01-13T10:34:29.936964Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#shuffle(files_list)\n#files_list_1 = files_list[:50000]\n#files_list_2 = files_list[50001:100000]\n#files_list_3 = files_list[100001:]\n# Write files list to pkl files\n#with open('files_list_1.pkl','wb') as fh:\n#    pickle.dump(files_list_1, fh)\n#with open('files_list_2.pkl','wb') as fh:\n#    pickle.dump(files_list_2, fh)\n#with open('files_list_3.pkl','wb') as fh:\n#    pickle.dump(files_list_3, fh)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm '/kaggle/working/coco_embeddings.json'","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:35:28.876397Z","iopub.execute_input":"2024-01-13T10:35:28.877374Z","iopub.status.idle":"2024-01-13T10:35:29.868639Z","shell.execute_reply.started":"2024-01-13T10:35:28.877335Z","shell.execute_reply":"2024-01-13T10:35:29.867346Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"pkl_file_path = '/kaggle/working/coco_embeddings_3.pkl'\njson_file_path = '/kaggle/working/coco_embeddings_3.json'\n\nembeddings_dict = prepare_image_embeddings(files_list_3[:15000])\n\n# Write to pkl file\nwith open(pkl_file_path,'wb') as fh:\n    pickle.dump(embeddings_dict, fh)\n\n# Write to JSON\nwrite_dict_to_json(embeddings_dict, json_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-01-13T10:35:48.630824Z","iopub.execute_input":"2024-01-13T10:35:48.631859Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/15000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b01fa82069b0449fbddca485f0fcc682"}},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nc = embeddings_dict['000000501175']\nc2 = np.asarray(c)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compute CLIP embeddings for CC3M dataset (https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K)**","metadata":{}},{"cell_type":"markdown","source":"**First load the metadata file for captions**","metadata":{}},{"cell_type":"code","source":"json_file_path = '/kaggle/input/cc3m-captions/metadata.json'\nmetadata_key = 'id'\ncc3m_metadata = parse_metadata(json_file_path, dict_key = metadata_key)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gather the list of images in our dataset**","metadata":{}},{"cell_type":"code","source":"cc3m_path = '/kaggle/input/cc3m-pretrain'\nfiles_list = os.listdir(cc3m_path)\nshuffle(files_list)\nmax_images = 175000\nfiles_list = files_list[:max_images]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate clip embeddings for all the images**","metadata":{}},{"cell_type":"code","source":"embedding_dict = prepare_image_embeddings(files_list, root_dir = cc3m_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_captions_metadata_with_embeddings(cc3m_metadata, embedding_dict):\n    cc3m_metadata_updated = {}\n    for key,embedding in tqdm(embedding_dict.items()):\n        cc3m_metadata_updated[key] = {\n            'clip_embeddings': embedding,\n            'id': key,\n            'caption': cc3m_metadata[key]['caption'],\n            'blip_caption': cc3m_metadata[key]['blip_caption']\n        }\n    return cc3m_metadata_updated\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_keys = list(metadata.keys())\nprint(all_keys[0])\nmetadata[all_keys[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = Image.open('/kaggle/input/cc3m-pretrain/GCC_train_002582585.jpg')\nim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_metadata = update_captions_metadata_with_embeddings(cc3m_metadata, embedding_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**write updated metadata to json**","metadata":{}},{"cell_type":"code","source":"# Write to JSON\ncc3m_metadata_path = 'cc3m_captions_and_clip_embeddings.json'\nwrite_dict_to_json(updated_metadata, cc3m_metadata_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Write updated metadata to pickle**","metadata":{}},{"cell_type":"code","source":"# Write to pkl file\npkl_file_path = 'cc3m_captions_and_clip_embeddings.pkl'\nwith open(pkl_file_path,'wb') as fh:\n    pickle.dump(updated_metadata, fh)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}