{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":7377207,"sourceType":"datasetVersion","datasetId":4286923}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install -qq -U datasets transformers pyarrow\n%pip install -qq --upgrade transformers ftfy accelerate regex tqdm\n%pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's import all the libraries we need","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport requests\nimport json\nimport pickle\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom PIL import Image\nfrom datasets import load_dataset\nfrom pathlib import Path\nfrom random import shuffle\nfrom transformers import AutoProcessor, CLIPVisionModel\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device set to {device}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Download the CLIP model to encode the image**","metadata":{}},{"cell_type":"code","source":"clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_preprocess = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Let's Write a method to encode the image using clip model**","metadata":{}},{"cell_type":"code","source":"def calc_image_emb(img, model, preprocess, device):\n    \"\"\"\n    This method computes the clip embeddings for a given image, after preprocessing it according to the model\n    \"\"\"\n    #image = preprocess(img).unsqueeze(0).to(device)\n    processed_image = preprocess(images=img, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**processed_image)\n        last_hidden_state = outputs.last_hidden_state\n        #image_features = model.encode_image(image)\n        #outputs['last_hidden_state'].shape\n    return last_hidden_state.squeeze()\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lets test the CLIP embeddings on a random image from COCO**","metadata":{}},{"cell_type":"code","source":"\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nimg_features = calc_image_emb(image, clip_model, clip_preprocess, device)\nprint(img_features.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dataset = load_dataset(\"json\", data_files=\"llava_instruct_150k.json\")\n#dataset = load_dataset(\"liuhaotian/LLaVA-Instruct-150K\", data_files='llava_instruct_150k.json', split='train')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**We need to download COCO2017 images, and compute the CLIP embeddings and store them first**","metadata":{}},{"cell_type":"code","source":"def prepare_image_embeddings(img_path_list, root_dir = None):\n    \"\"\"\n    This method computes the CLIP image embeddings for all the images in COCO 2017 dataset\n    \"\"\"\n    embeddings_dict = {}\n    for f_name in tqdm(img_path_list):\n        if root_dir is not None:\n            f_name = os.path.join(root_dir, f_name)\n        img = Image.open(f_name)\n        f_base = Path(f_name).stem\n        img_embd = calc_image_emb(img, clip_model, clip_preprocess, device)\n        embeddings_dict[f_base] = img_embd.squeeze().tolist()\n    return embeddings_dict\n\ndef get_absolute_paths(directory_path, max_files = None):\n    absolute_paths = []\n\n    # Check if the given path is a valid directory\n    if os.path.isdir(directory_path):\n        # Iterate over all files in the directory\n        for root, _, files in tqdm(os.walk(directory_path)):\n            for file in files:\n                # Construct the absolute path for each file\n                absolute_path = os.path.abspath(os.path.join(root, file))\n                absolute_paths.append(absolute_path)\n                if max_files is not None and len(absolute_paths) > max_files:\n                    break\n    return absolute_paths\n\ndef prepare_files_list(dataset_path, dirs = None):\n    files_list = []\n    for each_dir in dirs:\n        files_list.extend(get_absolute_paths(os.path.join(dataset_path, each_dir)))\n    return files_list\n\n\ndef write_dict_to_json(data_dict, file_path):\n    \"\"\"\n    Write a dictionary to a JSON file.\n\n    Parameters:\n    - dictionary: The dictionary to be written to the file.\n    - file_path: The path to the JSON file.\n    \"\"\"\n    with open(file_path, 'w') as json_file:\n        json.dump(data_dict, json_file, indent=4)\n        \n\ndef read_json_file(file_path):\n    \"\"\"\n    Read a JSON file and return its contents as a dictionary.\n\n    Parameters:\n    - file_path (str): The path to the JSON file.\n\n    Returns:\n    - dict: The contents of the JSON file as a dictionary.\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            data = json.load(file)\n            return data\n    except FileNotFoundError:\n        print(f\"Error: File not found - {file_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Unable to decode JSON in file - {file_path}\")\n\n        \ndef list_of_dicts_to_dict_of_dicts(list_of_dicts, key):\n    \"\"\"\n    Convert a list of dictionaries to a dictionary of dictionaries using a specified key.\n\n    Parameters:\n    - list_of_dicts (list): A list of dictionaries.\n    - key (str): The key to use as the identifier.\n\n    Returns:\n    - dict: A dictionary of dictionaries with the specified key.\n    \"\"\"\n    result_dict = {}\n\n    for item in list_of_dicts:\n        identifier = item.get(key)\n        if identifier is not None:\n            result_dict[identifier] = item\n\n    return result_dict\n\ndef parse_metadata(metadata_path, dict_key = 'id' ):\n    \"\"\"\n    first read json file, then convert to dict\n    \"\"\"\n    metadata = read_json_file(metadata_path)\n    metadata_dict = list_of_dicts_to_dict_of_dicts(metadata, key = dict_key)\n    return metadata_dict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load embeddings for COCO dataset. If embeddings are not available, prepare the same**","metadata":{}},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_path = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\nfiles_list = get_absolute_paths(train_dataset_path)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shuffle(files_list)\nfiles_list_1 = files_list[:50000]\nfiles_list_2 = files_list[50001:100000]\nfiles_list_3 = files_list[100001:]\n# Write files list to pkl files\nwith open('files_list_1.pkl','wb') as fh:\n    pickle.dump(files_list_1, fh)\nwith open('files_list_2.pkl','wb') as fh:\n    pickle.dump(files_list_2, fh)\nwith open('files_list_3.pkl','wb') as fh:\n    pickle.dump(files_list_3, fh)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pkl_file_path = '/kaggle/working/coco_embeddings_1.pkl'\njson_file_path = '/kaggle/working/coco_embeddings_1.json'\n\nembeddings_dict = prepare_image_embeddings(files_list_1)\n\n# Write to pkl file\nwith open(pkl_file_path,'wb') as fh:\n    pickle.dump(embeddings_dict, fh)\n\n# Write to JSON\nwrite_dict_to_json(embeddings_dict, json_file_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nc = embeddings_dict['000000501175']\nc2 = np.asarray(c)\nc2.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Compute CLIP embeddings for CC3M dataset (https://huggingface.co/datasets/liuhaotian/LLaVA-CC3M-Pretrain-595K)**","metadata":{}},{"cell_type":"markdown","source":"**First load the metadata file for captions**","metadata":{}},{"cell_type":"code","source":"json_file_path = '/kaggle/input/cc3m-captions/metadata.json'\nmetadata_key = 'id'\ncc3m_metadata = parse_metadata(json_file_path, dict_key = metadata_key)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Gather the list of images in our dataset**","metadata":{}},{"cell_type":"code","source":"cc3m_path = '/kaggle/input/cc3m-pretrain'\nfiles_list = os.listdir(cc3m_path)\nshuffle(files_list)\nmax_images = 175000\nfiles_list = files_list[:max_images]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Calculate clip embeddings for all the images**","metadata":{}},{"cell_type":"code","source":"embedding_dict = prepare_image_embeddings(files_list, root_dir = cc3m_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dict","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def update_captions_metadata_with_embeddings(cc3m_metadata, embedding_dict):\n    cc3m_metadata_updated = {}\n    for key,embedding in tqdm(embedding_dict.items()):\n        cc3m_metadata_updated[key] = {\n            'clip_embeddings': embedding,\n            'id': key,\n            'caption': cc3m_metadata[key]['caption'],\n            'blip_caption': cc3m_metadata[key]['blip_caption']\n        }\n    return cc3m_metadata_updated\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_keys = list(metadata.keys())\nprint(all_keys[0])\nmetadata[all_keys[0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im = Image.open('/kaggle/input/cc3m-pretrain/GCC_train_002582585.jpg')\nim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"updated_metadata = update_captions_metadata_with_embeddings(cc3m_metadata, embedding_dict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**write updated metadata to json**","metadata":{}},{"cell_type":"code","source":"# Write to JSON\ncc3m_metadata_path = 'cc3m_captions_and_clip_embeddings.json'\nwrite_dict_to_json(updated_metadata, cc3m_metadata_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Write updated metadata to pickle**","metadata":{}},{"cell_type":"code","source":"# Write to pkl file\npkl_file_path = 'cc3m_captions_and_clip_embeddings.pkl'\nwith open(pkl_file_path,'wb') as fh:\n    pickle.dump(updated_metadata, fh)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}