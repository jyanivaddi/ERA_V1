{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx3eMje+DMJPVe8gjlQL4l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/Capstone/MultiModal_phi_with_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Google Drive**"
      ],
      "metadata": {
        "id": "S7jZ3kj6uMkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUsU0-tcswRq",
        "outputId": "c91eb7e8-0286-4f67-fcfd-91dedb70ac77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "y-d6m1QkvbAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -quiet numpy transformers datasets tqdm matplotlib wandb torchmetrics torchinfo pillow pytorch-lightning peft bitsandbytes einops"
      ],
      "metadata": {
        "id": "TdnGSzY7vBkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "sCviR4Ervedz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/jyanivaddi/ERA_V1.git\"\n",
        "!git -C ERA_V1 pull\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUv9VAimveEN",
        "outputId": "955a5af8-fa62-44a2-e763-3e252c81aa92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ERA_V1'...\n",
            "remote: Enumerating objects: 2202, done.\u001b[K\n",
            "remote: Counting objects: 100% (795/795), done.\u001b[K\n",
            "remote: Compressing objects: 100% (433/433), done.\u001b[K\n",
            "remote: Total 2202 (delta 405), reused 709 (delta 348), pack-reused 1407\u001b[K\n",
            "Receiving objects: 100% (2202/2202), 276.88 MiB | 27.66 MiB/s, done.\n",
            "Resolving deltas: 100% (1097/1097), done.\n",
            "Updating files: 100% (263/263), done.\n",
            "Already up to date.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "import wandb\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import pickle\n",
        "import json\n",
        "import torchinfo\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch.multiprocessing as mp\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from typing import Union, List\n",
        "from torch.cuda.amp import autocast\n",
        "from matplotlib import pyplot as plt\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from transformers import AutoProcessor, CLIPVisionModel\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from peft import LoraConfig\n",
        "\n",
        "from llava_instruct_dataset import LlavaFinetuneDataset, LlavaCollator, split_data_to_train_and_val\n",
        "from model_finetune import LitMultiModalPhiFineTune, SimpleLinearBlock, model_summary"
      ],
      "metadata": {
        "id": "ifuHOpN1v0Uw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}