{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6e955986d5894862996d6dd58fb693e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f15bb2a07f144d66bfa9437db7837d64",
              "IPY_MODEL_0780b62390b84af685f2e8ab90ae7d93",
              "IPY_MODEL_fbfd0c59e93048a4b5e8f8c64a3decde"
            ],
            "layout": "IPY_MODEL_f35e232b92ba4b30bec13a143412f222"
          }
        },
        "f15bb2a07f144d66bfa9437db7837d64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4709190c2eb347458c77271c22d7bde0",
            "placeholder": "​",
            "style": "IPY_MODEL_a729e2a1ea514dc59cebca0a70d95725",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0780b62390b84af685f2e8ab90ae7d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a91863944fb462fa67c80457938de0a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_499305ced0384596a6ac0a1335b14a88",
            "value": 2
          }
        },
        "fbfd0c59e93048a4b5e8f8c64a3decde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_480a32ea74c44d84b7cdb5810bfcb533",
            "placeholder": "​",
            "style": "IPY_MODEL_154240fefc4f42fd90cd8910af92437d",
            "value": " 2/2 [00:18&lt;00:00,  8.24s/it]"
          }
        },
        "f35e232b92ba4b30bec13a143412f222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4709190c2eb347458c77271c22d7bde0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a729e2a1ea514dc59cebca0a70d95725": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a91863944fb462fa67c80457938de0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "499305ced0384596a6ac0a1335b14a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "480a32ea74c44d84b7cdb5810bfcb533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "154240fefc4f42fd90cd8910af92437d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/Capstone/MultiModal_phi_with_audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mount Google Drive**"
      ],
      "metadata": {
        "id": "S7jZ3kj6uMkh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUsU0-tcswRq",
        "outputId": "ae2ed342-b2c4-4938-d62a-68169b0ecb13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install dependencies**"
      ],
      "metadata": {
        "id": "y-d6m1QkvbAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --quiet transformers datasets tqdm matplotlib wandb torchmetrics torchinfo pytorch-lightning peft bitsandbytes einops pillow gradio\n",
        "!pip install 'git+https://github.com/m-bain/whisperx.git' --quiet"
      ],
      "metadata": {
        "id": "TdnGSzY7vBkj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba8259b-6621-4774-dc31-d01e03bed8dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Imports**"
      ],
      "metadata": {
        "id": "sCviR4Ervedz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"https://github.com/jyanivaddi/ERA_V1.git\"\n",
        "!git -C ERA_V1 pull\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUv9VAimveEN",
        "outputId": "63019372-2345-4b71-936e-4a542c3c4411"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ERA_V1' already exists and is not an empty directory.\n",
            "Already up to date.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "import wandb\n",
        "import io\n",
        "import requests\n",
        "import os\n",
        "import sys\n",
        "import gc\n",
        "import torch\n",
        "import pickle\n",
        "import whisperx\n",
        "import json\n",
        "import torchinfo\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pytorch_lightning as pl\n",
        "import torch.multiprocessing as mp\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "from typing import Union, List\n",
        "from torch.cuda.amp import autocast\n",
        "from matplotlib import pyplot as plt\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
        "from transformers import AutoProcessor, CLIPVisionModel\n",
        "from pytorch_lightning.callbacks import Callback\n",
        "from peft import LoraConfig\n"
      ],
      "metadata": {
        "id": "ifuHOpN1v0Uw",
        "outputId": "ae0ceb97-b912-436c-892d-aa10e78df7ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
            "  torchaudio.set_audio_backend(\"soundfile\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"ERA_V1/Capstone\")\n",
        "from llava_instruct_dataset import LlavaFinetuneDataset, LlavaCollator, split_data_to_train_and_val, get_image_embeddings\n",
        "from model_finetune import LitMultiModalPhiFineTune, SimpleLinearBlock, model_summary"
      ],
      "metadata": {
        "id": "9tYjWVxGzGaG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**set parameters**"
      ],
      "metadata": {
        "id": "PUPo6tERzSqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#json_path = './data/llava_instruct_150k.json'\n",
        "#batch_size = 20\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "projection_layer_in_channels = 768\n",
        "projection_layer_out_channels = 2560\n",
        "#seq_len = 72\n",
        "#num_image_tokens = 49\n",
        "#max_ques_length = seq_len - (1+num_image_tokens+1) # 1 for image start, 1 for comment\n",
        "stage1_projection_checkpoints = 'phi2_projection_checkpoints/ckpt_60001.pt'\n",
        "projection_layer_finetuning_checkpoint_path = '/content/gdrive/MyDrive/ERA_Capstone/phi2_finetune_checkpoints_run2_low_lr/projection_layer_finetuning/projection_layer_ckpt_finetuning_global_step_4001.pt'\n",
        "#projection_layer_finetuning_checkpoint_path = '/content/gdrive/MyDrive/ERA_Capstone/phi2_projection_checkpoints/ckpt_60001.pt'\n",
        "finetuned_phi_checkpoint_path = '/content/gdrive/MyDrive/ERA_Capstone/phi2_finetune_checkpoints_run2_low_lr/phi_model_finetuning/adapter_layer_ckpt_finetuning_global_step_4001'\n",
        "\n",
        "# Define configurations for QLORA finetuning\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, # Load the model in 4 bits\n",
        "    bnb_4bit_quant_type=\"nf4\", # 4 bit quant type\n",
        "    bnb_4bit_use_double_quant=True, # double quant saves more bits\n",
        "    bnb_4bit_compute_dtype=torch.float16, # use bfloat16\n",
        "    )\n",
        "\n",
        "# Define the models and tokenizers\n",
        "clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_preprocessor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "phi_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMar93z5zhmq",
        "outputId": "373d6fa9-9b46-4c67-fe81-0be0b3db6261"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define multimodal model\n",
        "multimodal_phi_model = LitMultiModalPhiFineTune(projection_layer_in_channels,\n",
        "                                                projection_layer_out_channels,\n",
        "                                                quantization_config)\n",
        "\n",
        "\n",
        "multimodal_phi_model.projection_layer.load_state_dict(torch.load(projection_layer_finetuning_checkpoint_path))\n",
        "multimodal_phi_model.llm_model.from_pretrained(multimodal_phi_model.llm_model, finetuned_phi_checkpoint_path)\n"
      ],
      "metadata": {
        "id": "py3JO2hW1Xgh",
        "outputId": "8a98d87d-b997-4672-919a-d1b57219ff3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6e955986d5894862996d6dd58fb693e9",
            "f15bb2a07f144d66bfa9437db7837d64",
            "0780b62390b84af685f2e8ab90ae7d93",
            "fbfd0c59e93048a4b5e8f8c64a3decde",
            "f35e232b92ba4b30bec13a143412f222",
            "4709190c2eb347458c77271c22d7bde0",
            "a729e2a1ea514dc59cebca0a70d95725",
            "5a91863944fb462fa67c80457938de0a",
            "499305ced0384596a6ac0a1335b14a88",
            "480a32ea74c44d84b7cdb5810bfcb533",
            "154240fefc4f42fd90cd8910af92437d"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e955986d5894862996d6dd58fb693e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 11,796,480 || all params: 2,791,480,320 || trainable%: 0.4225886858482312\n",
            "Number of Training Parameters\n",
            "********************************\n",
            "Projection Layer:1970176\n",
            "Phi Model:11796480\n",
            "********************************\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): PeftModelForCausalLM(\n",
              "      (base_model): LoraModel(\n",
              "        (model): PhiForCausalLM(\n",
              "          (model): PhiModel(\n",
              "            (embed_tokens): Embedding(51200, 2560)\n",
              "            (embed_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (layers): ModuleList(\n",
              "              (0-31): 32 x PhiDecoderLayer(\n",
              "                (self_attn): PhiAttention(\n",
              "                  (q_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=2560, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=2560, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=2560, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=2560, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (v_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=2560, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=2560, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (dense): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=2560, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=2560, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (rotary_emb): PhiRotaryEmbedding()\n",
              "                )\n",
              "                (mlp): PhiMLP(\n",
              "                  (activation_fn): NewGELUActivation()\n",
              "                  (fc1): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=2560, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=10240, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (fc2): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=10240, out_features=8, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=8, out_features=2560, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                )\n",
              "                (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "                (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "              )\n",
              "            )\n",
              "            (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "          (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define all the helper methods**"
      ],
      "metadata": {
        "id": "I0zzwyia-LmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_inputs(multimodal_phi_model, proj_output=None, question_embeddings=None, batch_size=1):\n",
        "\n",
        "    # define comment and im start tokens\n",
        "    comment_token = torch.tensor(multimodal_phi_model.COMMENT_TOKEN_ID).repeat(batch_size, 1).to(device)\n",
        "    comment = multimodal_phi_model.llm_model.model.model.embed_tokens(comment_token).to(device) #\n",
        "\n",
        "    im_start_token = torch.tensor(multimodal_phi_model.IMAGE_TOKEN_ID).repeat(batch_size, 1).to(device)\n",
        "    im_start = multimodal_phi_model.llm_model.model.model.embed_tokens(im_start_token).to(device) #\n",
        "\n",
        "    if proj_output is None and question_embeddings is None:\n",
        "        raise Exception(\"you need to provide an image, text, or audio input\")\n",
        "    if question_embeddings is None:\n",
        "        # prepare input embeddings\n",
        "        print(im_start.shape)\n",
        "        print(proj_output.shape)\n",
        "        print(comment.shape)\n",
        "        inputs_embeds = torch.cat([im_start, # <IM> [B x 1 x 2560]\n",
        "                                   proj_output, # [B x 49 x 2560]\n",
        "                                   comment, # [B x 1 x 2560]\n",
        "                                   ], dim=1) # total dim: (B, 64, 2560)\n",
        "    else:\n",
        "        # prepare input embeddings\n",
        "        inputs_embeds = torch.cat([im_start, # <IM> [B x 1 x 2560]\n",
        "                                   proj_output, # [B x 49 x 2560]\n",
        "                                   comment, # [B x 1 x 2560]\n",
        "                                   question_embeddings,\n",
        "                                   ], dim=1) # total dim: (B, 64, 2560)\n",
        "    return inputs_embeds\n",
        "\n",
        "\n",
        "def generate_phi_responses(multimodal_phi_model, batch, batch_size=1):\n",
        "    question_embeddings = None\n",
        "    proj_output = None\n",
        "    if 'ques_tokenized' in batch:\n",
        "        question_tokens = batch['ques_tokenized']\n",
        "        question_embeddings = multimodal_phi_model.llm_model.model.model.embed_tokens(question_tokens).to(device)\n",
        "\n",
        "    if 'image_embeddings' in batch:\n",
        "        image_embeddings = batch['image_embeddings']\n",
        "        proj_output = multimodal_phi_model.projection_layer(image_embeddings).to(device)\n",
        "\n",
        "    inputs_embeds = prepare_inputs(multimodal_phi_model, proj_output, question_embeddings)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with autocast(True):\n",
        "            pred_logits = multimodal_phi_model.llm_model.generate(inputs_embeds = inputs_embeds, max_new_tokens=20)\n",
        "            generated_text = multimodal_phi_model.tokenizer.batch_decode(pred_logits, skip_special_tokens=True, clean_up_tokenization_spaces=True, verbose=False)[0]\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "vtGcSCdb9S_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_embeddings(image, model, preprocessor, device=None):\n",
        "    \"\"\"\n",
        "    This method computes the clip embeddings for a given image, after preprocessing it according to the model\n",
        "    \"\"\"\n",
        "    processed_image = preprocessor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**processed_image)\n",
        "    return outputs.last_hidden_state.squeeze()[1:,:].unsqueeze(0)\n",
        "\n",
        "def tokenize_sentence(sentence, tokenizer):\n",
        "    tokenizer_output = tokenizer(sentence, return_tensors=\"pt\", return_attention_mask=False)\n",
        "    tokenized_sentence = tokenizer_output['input_ids']\n",
        "    return tokenized_sentence\n",
        "\n",
        "def generate_embeddings_from_inputs(image, text, clip_model, clip_preprocessor, tokenizer):\n",
        "    image_embeddings = get_image_embeddings(image, clip_model, clip_preprocessor)\n",
        "    tokenized_sentence = tokenize_sentence(text, tokenizer)\n",
        "    return {'image_embeddings': image_embeddings, 'ques_tokenized': tokenized_sentence}\n"
      ],
      "metadata": {
        "id": "pw5Cvor67d_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's try out the code!**"
      ],
      "metadata": {
        "id": "SKM1itet-OiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_url = r'/content/gdrive/MyDrive/temp.jpg'\n",
        "image = Image.open(image_url)\n",
        "\n",
        "#image_url = r'http://images.cocodataset.org/train2017/000000010005.jpg'\n",
        "#image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "question = \"what is the color of the cat in this image?\"\n",
        "batch = generate_embeddings_from_inputs(image, question, clip_model, clip_preprocessor, phi_tokenizer)\n",
        "#question = tokenize_sentence(\"who is the greatest cricketer of all time?\", phi_tokenizer)\n",
        "#batch = {'ques_tokenized': question}\n",
        "#print(batch['image_embeddings'].shape)\n",
        "#print(batch['ques_tokenized'].shape)\n",
        "#batch = {'image_embeddings': batch['image_embeddings']}"
      ],
      "metadata": {
        "id": "6lkT4jmi-Knb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_phi_responses(multimodal_phi_model, batch, batch_size=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "x0GgUcdxEIOt",
        "outputId": "2d53ba35-3895-489a-f53c-8383661290a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' white. white. white. white. white. white. white. white. white. white.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "\n",
        "device = \"cuda\"\n",
        "audio_file = \"/content/gdrive/MyDrive/test_audio.mp4\"\n",
        "batch_size = 16 # reduce if low on GPU mem\n",
        "compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "# 1. Transcribe with original whisper (batched)\n",
        "model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type)\n",
        "\n",
        "# save model to local path (optional)\n",
        "# model_dir = \"/path/\"\n",
        "# model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, download_root=model_dir)\n",
        "\n",
        "audio = whisperx.load_audio(audio_file)\n",
        "print(audio)\n",
        "result = model.transcribe(audio, batch_size=batch_size)\n",
        "print(result[\"segments\"]) # before alignment\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-AxBwZhFrp5",
        "outputId": "185ad7dd-f3f4-46f4-dc5c-61c6f5d949f3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "[-0.00012207 -0.00015259 -0.00015259 ...  0.          0.\n",
            "  0.        ]\n",
            "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
            "Detected language: en (0.85) in first 30s of audio...\n",
            "[{'text': \" Hello. How are you? I'm okay. I will be. I said she could stay with us till she feels better. Of course she can. No, this won't be for long. Well, you can stay as long as you want, my love. I really missed you. Great to see you, love.\", 'start': 0.145, 'end': 20.776}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_inference_on_model(audio_file):\n",
        "    print(audio_file)\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "\n",
        "    print(audio)\n",
        "    if True:\n",
        "        # whisper\n",
        "        audio_model = whisperx.load_model(\"large-v2\", device, compute_type='float16')\n",
        "        compute_type = \"float16\" # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
        "\n",
        "        # 1. Transcribe with original whisper (batched)\n",
        "        #audio = whisperx.load_audio(audio_file)\n",
        "        #result = audio_model.transcribe(audio[1], batch_size=1)\n",
        "        #print(result[\"segments\"]['text'])\n",
        "        audio_result = audio_model.transcribe(audio)\n",
        "        audio_text = ''\n",
        "        for seg in audio_result['segments']:\n",
        "            audio_text += seg['text']\n",
        "        audio_text = audio_text.strip()\n",
        "        return audio_text # before alignment\n",
        "    return None"
      ],
      "metadata": {
        "id": "0AzG8FHxQuDn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integrate Gradio**"
      ],
      "metadata": {
        "id": "AaphfIxBL2CT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import whisperx\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "demo = gr.Interface(run_inference_on_model,\n",
        "                    inputs = [gr.Audio(sources=[\"microphone\"], type='filepath')],\n",
        "                    outputs = [gr.Textbox(label='AI response', scale=2)],\n",
        "                    title = \"none\",\n",
        "                    description = \"mpme\"\n",
        "                   )\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "wDIlmQsBL4iQ",
        "outputId": "48c64466-f918-411e-ce80-e1b99ccd0fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://d3fdacc8d5f7917327.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d3fdacc8d5f7917327.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/gradio/35d476c4447f1744452ce8004ed5fee2129101b9/audio.wav\n",
            "[0.         0.         0.         ... 0.00125122 0.00192261 0.00213623]\n",
            "No language specified, language will be first be detected for each audio file (increases inference time).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu121. Bad things might happen unless you revert torch to 1.x.\n",
            "Warning: audio is shorter than 30s, language detection may be inaccurate.\n",
            "Detected language: en (0.89) in first 30s of audio...\n"
          ]
        }
      ]
    }
  ]
}