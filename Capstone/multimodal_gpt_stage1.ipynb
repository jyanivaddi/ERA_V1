{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":7399829,"sourceType":"datasetVersion","datasetId":4302683}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install -qq -U datasets transformers pyarrow torchinfo\n%pip install -qq --upgrade transformers ftfy accelerate regex tqdm\n%pip install git+https://github.com/openai/CLIP.git\n%pip install GPUtil\n%pip install --q -U pytorch-lightning lightning-bolts\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:11:03.063668Z","iopub.execute_input":"2024-01-20T20:11:03.064064Z","iopub.status.idle":"2024-01-20T20:12:28.365438Z","shell.execute_reply.started":"2024-01-20T20:11:03.064033Z","shell.execute_reply":"2024-01-20T20:12:28.364324Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.3 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.10 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 14.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-1aq75y76\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-1aq75y76\n  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.1.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=e6dfed43dd2a91eda9d2ca6031f98163cbdb58271b36c0020a6291a41e079871\n  Stored in directory: /tmp/pip-ephem-wheel-cache-gtlgrr9r/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=580430a764d912515687b6fb607460ffdea3c9c9b09647b474cf113d9033977a\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\nNote: you may need to restart the kernel to use updated packages.\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**All the imports**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport pickle\nimport json\nimport torchinfo\nimport torch.nn as nn\nimport numpy as np\nimport pytorch_lightning as pl\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\nfrom typing import Union, List\nimport torch.multiprocessing as mp \nfrom torch.cuda.amp import autocast\nfrom pytorch_lightning import LightningModule, Trainer\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\nfrom pytorch_lightning.loggers.tensorboard import TensorBoardLogger\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.callbacks import Callback\nimport torchmetrics\n#mp.set_start_method('spawn')","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:12:35.140383Z","iopub.execute_input":"2024-01-20T20:12:35.140774Z","iopub.status.idle":"2024-01-20T20:12:42.785839Z","shell.execute_reply.started":"2024-01-20T20:12:35.140743Z","shell.execute_reply":"2024-01-20T20:12:42.785063Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Define Hyperparameters**","metadata":{}},{"cell_type":"code","source":"raw_images_path = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\ntrain_dataset_path = '/kaggle/input/coco2017-clip-image-embeddings/coco_embeddings_clip_vision_1x768'\ncaptions_path = '/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\ncaptions_key = 'annotations'\nbatch_size = 1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nval_split_size = 0.1\nprojection_layer_in_channels = 24\nprojection_layer_out_channels = 2560\nprojection_hidden_size = 64\nmax_training_steps = 100\nseq_len = 32\nlog_dir = '/kaggle/working/'\nexp_name = 'phi2_proj_layer'\ncheck_point_save_dir = '/kaggle/working/phi2_projection_checkpoints/' \nsave_freq = 10000","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:12:55.071146Z","iopub.execute_input":"2024-01-20T20:12:55.072020Z","iopub.status.idle":"2024-01-20T20:12:55.095688Z","shell.execute_reply.started":"2024-01-20T20:12:55.071985Z","shell.execute_reply":"2024-01-20T20:12:55.094952Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"First, the projection layer...","metadata":{}},{"cell_type":"code","source":"class IdentityMap(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n    @property\n    def config(self):\n        return {\"mm_projector_type\": 'identity'}\n\n\nclass SimpleResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.pre_norm = nn.LayerNorm(in_channels)\n\n        self.proj = nn.Sequential(\n            nn.Linear(in_channels, out_channels),\n            nn.GELU(),\n            nn.Linear(out_channels, out_channels)\n        )\n    def forward(self, x):\n        x = self.pre_norm(x)\n        return x + self.proj(x)\n\n\nclass SimpleLinearBlock(nn.Module):\n    def __init__(self, in_size, out_size, hidden_size = 50, add_residual_connection=True):\n        super().__init__()\n        self.pre_norm = nn.LayerNorm(in_size)\n        self.proj = nn.Sequential(nn.Linear(in_size, hidden_size),\n                                  nn.GELU(),\n                                  nn.Linear(hidden_size, out_size))\n        self.add_residual_connection = add_residual_connection\n        \n    def forward(self,x):\n        return self.proj(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:18:41.598438Z","iopub.execute_input":"2024-01-20T20:18:41.599226Z","iopub.status.idle":"2024-01-20T20:18:41.608576Z","shell.execute_reply.started":"2024-01-20T20:18:41.599196Z","shell.execute_reply":"2024-01-20T20:18:41.607702Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def model_summary(model, input_size):\n    torchinfo.summary(model, \n                      input_size = input_size, \n                      batch_dim=0, \n                      col_names=(\"kernel_size\",\n                                 \"input_size\",\n                                 \"output_size\",\n                                 \"num_params\",\n                                 \"mult_adds\"),\n                       verbose=1,) ","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:18:45.413680Z","iopub.execute_input":"2024-01-20T20:18:45.414450Z","iopub.status.idle":"2024-01-20T20:18:45.419347Z","shell.execute_reply.started":"2024-01-20T20:18:45.414416Z","shell.execute_reply":"2024-01-20T20:18:45.418425Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"projection_layer_in_channels = 24\nprojection_layer_out_channels = 2560\ntest_model = SimpleLinearBlock(projection_layer_in_channels,projection_layer_out_channels, hidden_size=64)\nmodel_summary(test_model, (1,32,24))","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:24:23.948595Z","iopub.execute_input":"2024-01-20T19:24:23.948971Z","iopub.status.idle":"2024-01-20T19:24:24.271102Z","shell.execute_reply.started":"2024-01-20T19:24:23.948935Z","shell.execute_reply":"2024-01-20T19:24:24.270128Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"=====================================================================================================================================================================\nLayer (type:depth-idx)                   Kernel Shape              Input Shape               Output Shape              Param #                   Mult-Adds\n=====================================================================================================================================================================\nSimpleLinearBlock                        --                        [1, 1, 32, 24]            [1, 1, 32, 2560]          48                        --\n├─Sequential: 1-1                        --                        [1, 1, 32, 24]            [1, 1, 32, 2560]          --                        --\n│    └─Linear: 2-1                       --                        [1, 1, 32, 24]            [1, 1, 32, 64]            1,600                     1,600\n│    └─GELU: 2-2                         --                        [1, 1, 32, 64]            [1, 1, 32, 64]            --                        --\n│    └─Linear: 2-3                       --                        [1, 1, 32, 64]            [1, 1, 32, 2560]          166,400                   166,400\n=====================================================================================================================================================================\nTotal params: 168,048\nTrainable params: 168,048\nNon-trainable params: 0\nTotal mult-adds (M): 0.17\n=====================================================================================================================================================================\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.67\nParams size (MB): 0.67\nEstimated Total Size (MB): 1.35\n=====================================================================================================================================================================\n","output_type":"stream"}]},{"cell_type":"code","source":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:18:49.418687Z","iopub.execute_input":"2024-01-20T20:18:49.419435Z","iopub.status.idle":"2024-01-20T20:18:49.423923Z","shell.execute_reply.started":"2024-01-20T20:18:49.419400Z","shell.execute_reply":"2024-01-20T20:18:49.422988Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"**Pytorch Model definition**\n","metadata":{}},{"cell_type":"code","source":"class MultiModalGPT(nn.Module):\n    \"\"\"\n    Pytorch Lightning module for Transformer\n\n    \"\"\"\n    def __init__(self,\n                 llm_model,\n                 tokenizer,\n                 projection_layer_in_channels,\n                 projection_layer_out_channels,\n                 hidden_size = 32,\n                 ):\n        super(MultiModalGPT, self).__init__()\n        self.tokenizer = tokenizer\n        self.projection_layer = SimpleLinearBlock(projection_layer_in_channels,projection_layer_out_channels, hidden_size=hidden_size)\n        self.llm_model = llm_model\n        \n        # freeze the llm\n        for param in self.llm_model.parameters():\n            param.requires_grad = False\n    \n    \n    def forward(self, x, max_length=1):\n        #print(f\"beginning of projection: {x.shape}\")\n        x = self.projection_layer(x)\n        #print(f\"end of projection: {x.shape}\")\n        with torch.no_grad():  \n            x = self.llm_model(inputs_embeds = x, return_dict=False)\n        #print(f\"end of llm: logits: {x[0].shape}\")\n        return x\n    \n    def generate(self, x):\n        proj_outs = self.projection_layer(x)\n        with torch.no_grad():\n            output_tokens = self.llm_model.generate(**proj_outs, max_length=200)\n            generated_text = self.tokenizer.batch_decode(output_tokens)[0]\n        return generated_text","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:18:51.879347Z","iopub.execute_input":"2024-01-20T20:18:51.879723Z","iopub.status.idle":"2024-01-20T20:18:51.890061Z","shell.execute_reply.started":"2024-01-20T20:18:51.879695Z","shell.execute_reply":"2024-01-20T20:18:51.889196Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**Loss Function**","metadata":{}},{"cell_type":"code","source":"def chunked_cross_entropy(\n    logits: Union[torch.Tensor, List[torch.Tensor]], targets: torch.Tensor, chunk_size: int = 128\n) -> torch.Tensor:\n    # with large max_sequence_lengths, the beginning of `backward` allocates a large memory chunk which can dominate\n    # the memory usage in fine-tuning settings with low number of parameters.\n    # as a workaround hack, the cross entropy computation is chunked to force it to deallocate on the go, reducing\n    # the memory spike's magnitude\n\n    # lm_head was chunked (we are fine-tuning)\n    if isinstance(logits, list):\n        # don't want to chunk cross entropy\n        if chunk_size == 0:\n            logits = torch.cat(logits, dim=1)\n            logits = logits.reshape(-1, logits.size(-1))\n            targets = targets.reshape(-1)\n            return torch.nn.functional.cross_entropy(logits, targets, ignore_index=-1)\n\n        # chunk cross entropy\n        logit_chunks = [logit_chunk.reshape(-1, logit_chunk.size(-1)) for logit_chunk in logits]\n        target_chunks = [target_chunk.reshape(-1) for target_chunk in targets.split(logits[0].size(1), dim=1)]\n        loss_chunks = [\n            torch.nn.functional.cross_entropy(logit_chunk, target_chunk, ignore_index=-1, reduction=\"none\")\n            for logit_chunk, target_chunk in zip(logit_chunks, target_chunks)\n        ]\n        return torch.cat(loss_chunks).mean()\n\n    # no chunking at all\n    logits = logits.reshape(-1, logits.size(-1))\n    targets = targets.reshape(-1)\n    if chunk_size == 0:\n        return torch.nn.functional.cross_entropy(logits, targets, ignore_index=-1)\n\n    # lm_head wasn't chunked, chunk cross entropy\n    logit_chunks = logits.split(chunk_size)\n    target_chunks = targets.split(chunk_size)\n    loss_chunks = [\n        torch.nn.functional.cross_entropy(logit_chunk, target_chunk, ignore_index=-1, reduction=\"none\")\n        for logit_chunk, target_chunk in zip(logit_chunks, target_chunks)\n    ]\n    return torch.cat(loss_chunks).mean()","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:18:55.848593Z","iopub.execute_input":"2024-01-20T20:18:55.848964Z","iopub.status.idle":"2024-01-20T20:18:55.860476Z","shell.execute_reply.started":"2024-01-20T20:18:55.848935Z","shell.execute_reply":"2024-01-20T20:18:55.859510Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"**Pytorch Lightning Model definition**","metadata":{}},{"cell_type":"code","source":"class LitMultiModalGPT(LightningModule):\n    \"\"\"\n    Pytorch Lightning module for Transformer\n\n    \"\"\"\n    def __init__(self,\n                 projection_layer_in_channels,\n                 projection_layer_out_channels,\n                 hidden_size = 32,\n                 num_validation_examples=10,\n                 num_training_steps=100000):\n        super().__init__()\n        self.num_validation_examples = num_validation_examples\n        self.num_training_steps = num_training_steps\n        self.scheduler = None\n        self.scheduler_dict = {}\n        self.optimizer = None\n        self.this_step_train_loss = None\n        self.predicted_list = []\n        self.expected_list = []\n        self.save_hyperparameters(ignore=['loss_criterion', 'epoch'])\n        self.projection_layer = SimpleLinearBlock(projection_layer_in_channels,projection_layer_out_channels, hidden_size=hidden_size)\n        self.llm_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", torch_dtype=torch.bfloat16)\n        self.tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n\n     \n        # freeze the llm\n        for param in self.llm_model.parameters():\n            param.requires_grad = False\n    \n    \n    def generate(self, x):\n        proj_outs = self.projection_layer(x)\n        with torch.no_grad():\n            output_tokens = self.llm_model.generate(**proj_outs, max_length=200)\n            generated_text = self.tokenizer.batch_decode(output_tokens)[0]\n        return generated_text\n\n    \n    def set_optimizer(self, optimizer):\n        self.optimizer = optimizer\n\n    \n    def set_scheduler_dict(self, scheduler, freq='step'):\n        self.scheduler = scheduler\n        self.scheduler_dict = {\n            \"scheduler\": self.scheduler,\n            \"interval\": freq,\n        }\n\n    def configure_optimizers(self):\n        if self.scheduler_dict:\n            return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler_dict}\n        return {\"optimizer\": self.optimizer}\n         \n   \n    def forward(self, x):\n        #print(f\"beginning of projection: {x.shape}\")\n        x = self.projection_layer(x)\n        #print(f\"end of projection: {x.shape}\")\n        x = self.llm_model(inputs_embeds = x, return_dict=False)\n        #print(f\"end of llm: logits: {x[0].shape}\")\n        return x\n\n    \n    def evaluate(self,x, stage):\n        if stage:\n            predicted = self.generate(x)\n            self.predicted_list.append(predicted)\n            self.expected_list.append(batch['caption'])\n            # print the source, target, and the model output\n            print(\"*****************************************\")\n            print(f\"{f'Input: ' :>12}{x['caption']}\")\n            print(f\"{f'predicted: ' :>12}{predicted}\")\n            print(\"*****************************************\\n\")\n        \n\n    def training_step(self, batch):\n        targets = batch['tokenized_caption']  # (B, seq_len)\n        self.llm_model.to(device)\n        with torch.autocast(device_type=\"cuda\"):\n            logits, outputs = self(batch['image_embeddings'])\n            #print(f\"logits: {logits.shape}, targets: {targets.shape}\")\n            loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n        torch.cuda.empty_cache()\n        gc.collect()\n        self.llm_model.to(\"cpu\")\n        self.log(\"train_loss\", loss.item(), prog_bar=True)\n        self.this_step_train_loss = loss.item()\n        return loss\n\n    \n    def validation_step(self, batch, batch_idx):\n        self.evaluate(batch, \"val\")\n        if batch_idx % 10000 == 0:\n            raw_img_path = batch['raw_image_path']\n            image = Image.open(raw_img_path)\n            image.show()\n            print(\"*****************************************\")\n            print(f\"{f'Input: ' :>12}{x['caption']}\")\n            print(f\"{f'predicted: ' :>12}{predicted}\")\n            print(\"*****************************************\\n\")\n\n\n\n    def test_step(self, batch, batch_idx):\n        self.evaluate(batch, \"test\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:19:00.542927Z","iopub.execute_input":"2024-01-20T20:19:00.543276Z","iopub.status.idle":"2024-01-20T20:19:00.563371Z","shell.execute_reply.started":"2024-01-20T20:19:00.543250Z","shell.execute_reply":"2024-01-20T20:19:00.562437Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"**Define Trainer**","metadata":{}},{"cell_type":"code","source":"class PeriodicCheckpoint(ModelCheckpoint):\n    def __init__(self, checkpoint_save_dir, save_freq, verbose: bool = False):\n        super().__init__()\n        self.verbose = verbose\n        self.save_dir = checkpoint_save_dir\n        self.save_freq = save_freq\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        if batch_idx % self.save_freq == 0:\n            # save the model at the end of every epoch\n            model_filename = os.path.join(self.save_dir, f\"ckpt_{trainer.global_step}.pt\")\n            # Save only projection layer weights\n            #selected_params = {\n            #    'conv1.weight': model.conv1.weight,\n            #    'fc.bias': model.fc.bias\n            #}\n\n            #torch.save(selected_params, 'selected_params.pth')\n            #            trainer.save_checkpoint(model_filename)\n\n\nclass PrintAccuracyAndLoss(Callback):\n    def __init__(self):\n        super().__init__()\n\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        train_loss = trainer.callback_metrics['train_loss']\n        trainer.model.log(\"train_step_loss\", train_loss)\n        if batch_idx % 100 == 0:\n            print(f\"Step: {trainer.global_step}: train_loss={train_loss:.4f}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:19:12.184413Z","iopub.execute_input":"2024-01-20T20:19:12.184808Z","iopub.status.idle":"2024-01-20T20:19:12.193245Z","shell.execute_reply.started":"2024-01-20T20:19:12.184781Z","shell.execute_reply":"2024-01-20T20:19:12.192316Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":" def train_multimodal_gpt_model(model, train_dataloader, val_dataloader, ckpt_path=None, max_training_steps=2):\n    trainer = Trainer(\n        enable_checkpointing=True,\n        max_steps=max_training_steps,\n        accelerator=\"auto\", #\"auto\" if torch.cuda.is_available() else \"cpu\",\n        devices = 1, \n        logger=TensorBoardLogger(save_dir=log_dir, name=exp_name, default_hp_metric=False),\n        callbacks=[LearningRateMonitor(logging_interval=\"step\"),\n                   TQDMProgressBar(refresh_rate=10),\n                   PeriodicCheckpoint(check_point_save_dir, save_freq, verbose=True),\n                   PrintAccuracyAndLoss()],\n        num_sanity_val_steps=0,\n        val_check_interval = 100,\n        precision=\"16\"\n    )\n    \n    trainer.fit(model, train_dataloader, val_dataloader, ckpt_path=ckpt_path)\n    return trainer","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:19:21.815462Z","iopub.execute_input":"2024-01-20T20:19:21.815832Z","iopub.status.idle":"2024-01-20T20:19:21.822795Z","shell.execute_reply.started":"2024-01-20T20:19:21.815804Z","shell.execute_reply":"2024-01-20T20:19:21.821829Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Data loader**","metadata":{}},{"cell_type":"code","source":"def get_absolute_paths(directory_path, max_files = None):\n    absolute_paths = []\n    image_ids = []\n\n    # Check if the given path is a valid directory\n    if os.path.isdir(directory_path):\n        # Iterate over all files in the directory\n        for root, _, files in os.walk(directory_path):\n            for file in tqdm(files):\n                # extract image ID\n                image_ids.append(Path(file).stem)\n                # Construct the absolute path for each file\n                absolute_path = os.path.abspath(os.path.join(root, file))\n                absolute_paths.append(absolute_path)\n                if max_files is not None and len(absolute_paths) > max_files:\n                    break\n    return absolute_paths, image_ids\n\n\ndef parse_captions_file(captions_path, captions_key):\n    \"\"\"\n    Read a JSON file and return its contents as a dictionary.\n\n    Parameters:\n    - file_path (str): The path to the JSON file.\n\n    Returns:\n    - dict: The contents of the JSON file as a dictionary.\n    \"\"\"\n    try:\n        with open(captions_path, 'r') as file:\n            data = json.load(file)\n        captions = {}\n        annotations = data[captions_key]\n        for annotation in annotations:\n            captions[annotation['image_id']] = annotation['caption']\n        return captions\n    except FileNotFoundError:\n        print(f\"Error: File not found - {captions_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Unable to decode JSON in file - {captions_path}\")\n\n        \ndef load_pickle_file(file_path):\n    with open(file_path, 'rb') as fh:\n        data = pickle.load(fh)\n    keys = list(data.keys()) \n    assert len(keys) == 1\n    return data[keys[0]]\n\n\nclass PickleDataset(Dataset):\n\n    def __init__(self, \n                 raw_images_path,\n                 embeddings_path,\n                 image_ids,\n                 captions_path,\n                 captions_key,\n                 tokenizer, \n                 seq_len=768):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.ds = None\n        self.raw_images_path = raw_images_path\n        self.captions_key = captions_key\n        self.embeddings_path = embeddings_path\n        self.image_ids = image_ids\n        self.bos_token = self.tokenizer.bos_token\n        self.eos_token = self.tokenizer.eos_token\n        self.seq_len = seq_len\n        self.captions = parse_captions_file(captions_path, captions_key)\n        \n\n    def __len__(self):\n        return len(self.image_ids)\n\n\n    def __getitem__(self, idx):\n\n        # get image embeddings\n        img_embds = load_pickle_file(self.embeddings_path[idx])\n        img_embds = torch.tensor(np.squeeze(img_embds))\n        img_embds = img_embds.view(32,24)\n        #print(img_embds.shape)\n        this_img_id = self.image_ids[idx]\n        #print(this_img_id)\n        this_img_f_name = \"{:012d}\".format(int(this_img_id))+'.jpg'\n        \n        # get caption\n        caption = self.captions[int(this_img_id)]\n        tokenized_caption = self.tokenize_caption(caption)\n        \n        return {\n            \"image_embeddings\": img_embds,\n            \"raw_image_path\": os.path.join(self.raw_images_path, this_img_f_name),\n            \"caption\": caption,\n            \"tokenized_caption\": tokenized_caption\n        }\n    \n    def tokenize_caption(self, caption):\n        tokens = self.tokenizer(caption)\n        tokenizer_output = self.tokenizer(caption, return_tensors=\"pt\", return_attention_mask=False)\n        caption_encoded = tokenizer_output['input_ids'].squeeze()\n        if len(caption_encoded) > self.seq_len:\n            caption_encoded = caption_encoded[:self.seq_len-1]\n        num_padding_tokens = self.seq_len - len(caption_encoded) - 1\n        # Add <s> and </s> token\n        tokenized_caption = torch.cat(\n            [\n                torch.tensor([self.tokenizer.bos_token_id],dtype=torch.int64),\n                caption_encoded.squeeze(),\n                torch.tensor([self.tokenizer.eos_token_id]*num_padding_tokens,dtype=torch.int64),\n            ],dim=0)\n        #print(f\"caption length: {len(caption_encoded)} number of padding tokens: {num_padding_tokens} total size: {len(tokenized_caption)}\")\n        return tokenized_caption\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:24:50.109925Z","iopub.execute_input":"2024-01-20T20:24:50.110314Z","iopub.status.idle":"2024-01-20T20:24:50.131668Z","shell.execute_reply.started":"2024-01-20T20:24:50.110285Z","shell.execute_reply":"2024-01-20T20:24:50.130762Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"**Define train dataset and train dataloader**","metadata":{}},{"cell_type":"code","source":"files_list, images_ids_list = get_absolute_paths(train_dataset_path)\nfiles_list = np.array(files_list)\nimages_ids_list = np.array(images_ids_list)\nrand_indices = np.arange(len(files_list))\nnp.random.shuffle(rand_indices)\n\nval_split = int(len(files_list)*val_split_size)\n\nval_filepaths, train_filepaths = files_list[rand_indices[:val_split]], files_list[rand_indices[val_split:]] \nval_image_ids, train_image_ids = images_ids_list[rand_indices[:val_split]], images_ids_list[rand_indices[val_split:]]\n\nprint(f\"Train dataset size: {len(train_filepaths)}\")\nprint(f\"Valid dataset size: {len(val_filepaths)}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:19:54.499352Z","iopub.execute_input":"2024-01-20T20:19:54.500285Z","iopub.status.idle":"2024-01-20T20:21:32.871551Z","shell.execute_reply.started":"2024-01-20T20:19:54.500251Z","shell.execute_reply":"2024-01-20T20:21:32.870580Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"100%|██████████| 118285/118285 [00:01<00:00, 73874.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train dataset size: 106457\nValid dataset size: 11828\n","output_type":"stream"}]},{"cell_type":"code","source":"phi_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\ntrain_ds = PickleDataset(raw_images_path, train_filepaths, train_image_ids, captions_path, captions_key, phi_tokenizer, seq_len = seq_len)\nval_ds = PickleDataset(raw_images_path, val_filepaths, val_image_ids, captions_path, captions_key, phi_tokenizer, seq_len = seq_len)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:24:54.809638Z","iopub.execute_input":"2024-01-20T20:24:54.810529Z","iopub.status.idle":"2024-01-20T20:24:58.439505Z","shell.execute_reply.started":"2024-01-20T20:24:54.810492Z","shell.execute_reply":"2024-01-20T20:24:58.438611Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataloader = DataLoader(dataset = train_ds,\n                              batch_size = batch_size,\n                              num_workers = 1,\n                              collate_fn = None,\n                              shuffle = True)\nval_dataloader = DataLoader(dataset = val_ds,\n                            batch_size = 1,\n                            num_workers = 1,\n                            collate_fn = None,\n                            shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:25:07.793721Z","iopub.execute_input":"2024-01-20T20:25:07.794080Z","iopub.status.idle":"2024-01-20T20:25:07.800003Z","shell.execute_reply.started":"2024-01-20T20:25:07.794052Z","shell.execute_reply":"2024-01-20T20:25:07.799065Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"cc = next(iter(train_dataloader))\ninput_embeds = cc['image_embeddings']\nprint(input_embeds.shape)\n#pred_logits = multimodal_gpt_model.llm_model(inputs_embeds=input_embeds, return_dict=False)\nbatch_size, seq_length, _ = input_embeds.shape\n## Get the predicted token IDs\n#output_tokens = torch.argmax(pred_logits, axis=-1)  \n#text = phi_tokenizer.batch_decode(output_tokens)[0]\n        \n## Convert token IDs to words using the vocabulary and the StringLookup\n\n##inputs = phi_tokenizer(\"hello\", return_tensors=\"pt\", return_attention_mask=False)\n##output_tokens = phi_model.generate(**inputs, max_length=20)\n##text = phi_tokenizer.batch_decode(output_tokens)[0]\n#print(text)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T19:22:09.410158Z","iopub.execute_input":"2024-01-20T19:22:09.411066Z","iopub.status.idle":"2024-01-20T19:22:09.783937Z","shell.execute_reply.started":"2024-01-20T19:22:09.411026Z","shell.execute_reply":"2024-01-20T19:22:09.782737Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"torch.Size([32, 24])\ncaption length: 11 number of padding tokens: 20 total size: 32\ntorch.Size([32, 24])\ncaption length: 9 number of padding tokens: 22 total size: 32\ntorch.Size([1, 32, 24])\n","output_type":"stream"}]},{"cell_type":"code","source":"multimodal_gpt_model = LitMultiModalGPT(projection_layer_in_channels,\n                                        projection_layer_out_channels,\n                                        hidden_size = projection_hidden_size)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:21:54.196404Z","iopub.execute_input":"2024-01-20T20:21:54.196889Z","iopub.status.idle":"2024-01-20T20:22:33.829081Z","shell.execute_reply.started":"2024-01-20T20:21:54.196847Z","shell.execute_reply":"2024-01-20T20:22:33.828108Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbe9f30af0fb4bd5b750d2982b1bbc60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927cb87c62a94879a7eae07f3fb4e236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21a84d0273dc4739a953bab45f61ea3b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03650bad5cc54039b37d2ec451035876"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f49cac76947149f5813d647f0460f42c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81c5cd822a4d4c61a6db7ee2cac2f3fa"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.23.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight']\n- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.16.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45bad50bd8e143899c8341608602a91f"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(multimodal_gpt_model.parameters(), lr=1.0e-4, eps=1e-9)\nmultimodal_gpt_model.set_optimizer(optimizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:24:05.443746Z","iopub.execute_input":"2024-01-20T20:24:05.444094Z","iopub.status.idle":"2024-01-20T20:24:05.451042Z","shell.execute_reply.started":"2024-01-20T20:24:05.444068Z","shell.execute_reply":"2024-01-20T20:24:05.450030Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"**Train the model**","metadata":{}},{"cell_type":"code","source":"count_parameters(multimodal_gpt_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:22:43.631697Z","iopub.execute_input":"2024-01-20T20:22:43.632036Z","iopub.status.idle":"2024-01-20T20:22:43.641591Z","shell.execute_reply.started":"2024-01-20T20:22:43.632012Z","shell.execute_reply":"2024-01-20T20:22:43.640494Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"168048"},"metadata":{}}]},{"cell_type":"code","source":"trainer = train_multimodal_gpt_model(multimodal_gpt_model, train_dataloader, val_dataloader, max_training_steps=max_training_steps)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T20:25:15.317109Z","iopub.execute_input":"2024-01-20T20:25:15.317485Z","iopub.status.idle":"2024-01-20T20:31:58.985425Z","shell.execute_reply.started":"2024-01-20T20:25:15.317453Z","shell.execute_reply":"2024-01-20T20:31:58.983601Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee928cd8cf3d4329a7797769a8e9afb2"}},"metadata":{}},{"name":"stdout","text":"000000034616\n000000325647\n000000420960\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000027495\nStep: 1: train_loss=11.3426\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000481598\nStep: 2: train_loss=10.2766\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000402112\nStep: 3: train_loss=14.1010\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000337802\nStep: 4: train_loss=14.4849\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000417849\nStep: 5: train_loss=10.7172\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000478215\nStep: 6: train_loss=10.9142\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000152819\nStep: 7: train_loss=12.9317\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000330067\nStep: 8: train_loss=12.6978\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000350640\nStep: 9: train_loss=10.2453\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000063503\nStep: 10: train_loss=11.6639\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000412194\nStep: 11: train_loss=9.6370\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000098656\nStep: 12: train_loss=12.3533\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000438047\nStep: 13: train_loss=10.8006\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000378561\nStep: 14: train_loss=10.0956\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000071024\nStep: 15: train_loss=11.7218\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000173146\nStep: 16: train_loss=9.6227\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000208024\nStep: 17: train_loss=10.6024\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000275700\nStep: 18: train_loss=9.9600\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000425898\nStep: 19: train_loss=10.5018\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000413278\nStep: 20: train_loss=8.0027\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000528800\nStep: 21: train_loss=10.4260\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000221083\nStep: 22: train_loss=8.5342\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000106104\nStep: 23: train_loss=9.6525\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000297736\nStep: 24: train_loss=7.6156\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000126925\nStep: 25: train_loss=8.6747\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000573572\nStep: 26: train_loss=9.1887\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000202645\nStep: 27: train_loss=7.9772\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000345142\nStep: 28: train_loss=6.8552\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000348019\nStep: 29: train_loss=11.2267\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000279485\nStep: 30: train_loss=8.8970\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000329480\nStep: 31: train_loss=8.5156\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000507017\nStep: 32: train_loss=9.1568\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000553796\nStep: 33: train_loss=7.6610\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000335660\nStep: 34: train_loss=9.3131\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000035985\nStep: 35: train_loss=10.6493\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000029813\nStep: 36: train_loss=6.0140\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000543985\nStep: 37: train_loss=8.7834\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000567205\nStep: 38: train_loss=8.8170\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000525732\nStep: 39: train_loss=8.5596\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000282879\nStep: 40: train_loss=7.4791\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000435975\nStep: 41: train_loss=6.5192\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000130654\nStep: 42: train_loss=8.6709\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000262016\nStep: 43: train_loss=7.0967\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000347829\nStep: 44: train_loss=9.4576\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000236226\nStep: 45: train_loss=8.6089\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000323155\nStep: 46: train_loss=7.6366\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000121232\nStep: 47: train_loss=7.8715\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000408363\nStep: 48: train_loss=8.1014\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000330499\nStep: 49: train_loss=6.8994\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000183519\nStep: 50: train_loss=6.5407\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000478312\nStep: 51: train_loss=7.5462\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000445933\nStep: 52: train_loss=6.2341\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000145666\nStep: 53: train_loss=6.4729\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000277998\nStep: 54: train_loss=6.9883\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000356496\nStep: 55: train_loss=8.6238\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000477655\nStep: 56: train_loss=6.6576\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000537608\nStep: 57: train_loss=8.2060\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000009945\nStep: 58: train_loss=5.5416\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000469877\nStep: 59: train_loss=8.0373\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000337111\nStep: 60: train_loss=4.5183\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000406068\nStep: 61: train_loss=7.6995\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000251027\nStep: 62: train_loss=7.5087\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000581338\nStep: 63: train_loss=6.3389\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000224055\nStep: 64: train_loss=5.5520\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000335913\nStep: 65: train_loss=7.4824\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000445306\nStep: 66: train_loss=7.2295\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000042091\nStep: 67: train_loss=6.3860\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000435951\nStep: 68: train_loss=6.9577\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000150646\nStep: 69: train_loss=5.9403\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000154002\nStep: 70: train_loss=6.9338\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000507312\nStep: 71: train_loss=7.5858\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000350170\nStep: 72: train_loss=8.0715\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000332407\nStep: 73: train_loss=6.3314\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000500946\nStep: 74: train_loss=4.9513\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000080159\nStep: 75: train_loss=5.4481\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000231963\nStep: 76: train_loss=4.6477\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000272679\nStep: 77: train_loss=7.2895\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000422599\nStep: 78: train_loss=4.9348\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000518200\nStep: 79: train_loss=4.6312\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000363326\nStep: 80: train_loss=5.6533\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000546966\nStep: 81: train_loss=5.2800\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000259420\nStep: 82: train_loss=6.5975\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000412806\nStep: 83: train_loss=5.0772\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000542786\nStep: 84: train_loss=4.4631\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000491835\nStep: 85: train_loss=6.1424\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000552472\nStep: 86: train_loss=6.4937\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000455565\nStep: 87: train_loss=5.9903\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000357526\nStep: 88: train_loss=5.2814\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000143560\nStep: 89: train_loss=4.6938\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000084005\nStep: 90: train_loss=5.3035\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000154972\nStep: 91: train_loss=6.2683\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000051750\nStep: 92: train_loss=4.0685\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000135875\nStep: 93: train_loss=4.3056\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000521817\nStep: 94: train_loss=4.7033\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000322979\nStep: 95: train_loss=4.9112\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000038178\nStep: 96: train_loss=12.0281\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000343852\nStep: 97: train_loss=5.2055\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000016653\nStep: 98: train_loss=4.0563\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\n000000174103\nStep: 99: train_loss=3.8622\nbeginning of projection: torch.Size([1, 32, 24])\nend of projection: torch.Size([1, 32, 2560])\nend of llm: logits: torch.Size([1, 32, 51200])\nlogits: torch.Size([1, 32, 51200]), targets: torch.Size([1, 32])\nStep: 100: train_loss=4.4482\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"068f2c7ac5854037a40999ff0153732a"}},"metadata":{}},{"name":"stdout","text":"000000051278\n000000101017\n000000012696\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_multimodal_gpt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmultimodal_gpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_training_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_training_steps\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[12], line 17\u001b[0m, in \u001b[0;36mtrain_multimodal_gpt_model\u001b[0;34m(model, train_dataloader, val_dataloader, ckpt_path, max_training_steps)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_multimodal_gpt_model\u001b[39m(model, train_dataloader, val_dataloader, ckpt_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      2\u001b[0m    trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      3\u001b[0m        enable_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m        max_steps\u001b[38;5;241m=\u001b[39mmax_training_steps,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m        precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m    )\n\u001b[0;32m---> 17\u001b[0m    \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m    \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvance(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_advance_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:250\u001b[0m, in \u001b[0;36mTrainingEpochLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_check_val:\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mvalidating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# update plateau LR scheduler after metrics are logged\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:308\u001b[0m, in \u001b[0;36mTrainingEpochLoop._run_validation\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loop\u001b[38;5;241m.\u001b[39m_reload_evaluation_dataloaders()\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 308\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:152\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_dataloaders \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataloader_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[0;32m--> 152\u001b[0m dl_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs\u001b[38;5;241m.\u001b[39mappend(dl_outputs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:137\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluation_step_end(output)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:234\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._evaluation_step\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    the outputs of the step\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 234\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1494\u001b[0m, in \u001b[0;36mTrainer._call_strategy_hook\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1494\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mval_step_context():\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, ValidationStep)\n\u001b[0;32m--> 390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[10], line 94\u001b[0m, in \u001b[0;36mLitMultiModalGPT.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     96\u001b[0m         raw_img_path \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraw_image_path\u001b[39m\u001b[38;5;124m'\u001b[39m]\n","Cell \u001b[0;32mIn[10], line 68\u001b[0m, in \u001b[0;36mLitMultiModalGPT.evaluate\u001b[0;34m(self, x, stage)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m,x, stage):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stage:\n\u001b[0;32m---> 68\u001b[0m         predicted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\u001b[43mbatch\u001b[49m)\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicted_list\u001b[38;5;241m.\u001b[39mappend(predicted)\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_list\u001b[38;5;241m.\u001b[39mappend(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcaption\u001b[39m\u001b[38;5;124m'\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"],"ename":"NameError","evalue":"name 'batch' is not defined","output_type":"error"}]},{"cell_type":"code","source":"#del phi_model\nprint(torch.cuda.memory_allocated())\ntest_llm_model.to(\"cpu\")\ngc.collect()\n#del input_embeds\n#del phi_model\ntorch.cuda.empty_cache()\nprint(torch.cuda.memory_allocated())\n#print(torch.cuda.memory_reserved())","metadata":{"execution":{"iopub.status.busy":"2024-01-20T18:22:59.957232Z","iopub.execute_input":"2024-01-20T18:22:59.958244Z","iopub.status.idle":"2024-01-20T18:23:05.202532Z","shell.execute_reply.started":"2024-01-20T18:22:59.958198Z","shell.execute_reply":"2024-01-20T18:23:05.201557Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"5606875648\n22325760\n","output_type":"stream"}]},{"cell_type":"code","source":"input_embeds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#input_embeds = cc['image_embeddings'].to(device)\n#output = multimodal_gpt_model.generate(input_embeds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_llm_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\ntest_llm_model = test_llm_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-20T18:23:12.682817Z","iopub.execute_input":"2024-01-20T18:23:12.683706Z","iopub.status.idle":"2024-01-20T18:23:26.903786Z","shell.execute_reply.started":"2024-01-20T18:23:12.683668Z","shell.execute_reply":"2024-01-20T18:23:26.902905Z"},"trusted":true},"execution_count":31,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2017d5b499e4e4cb0602c0526195b82"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.24.self_attn.k_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.0.self_attn.k_proj.weight']\n- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.3.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-01-20T18:22:38.357392Z","iopub.execute_input":"2024-01-20T18:22:38.358275Z","iopub.status.idle":"2024-01-20T18:22:39.514626Z","shell.execute_reply.started":"2024-01-20T18:22:38.358235Z","shell.execute_reply":"2024-01-20T18:22:39.513390Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Sat Jan 20 18:22:39 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0              32W / 250W |   5706MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}}]}