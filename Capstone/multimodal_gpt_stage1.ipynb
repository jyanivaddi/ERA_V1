{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":7399829,"sourceType":"datasetVersion","datasetId":4302683}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install -qq -U datasets transformers pyarrow torchinfo\n%pip install -qq --upgrade transformers ftfy accelerate regex tqdm\n%pip install git+https://github.com/openai/CLIP.git\n%pip install GPUtil\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:26:49.275407Z","iopub.execute_input":"2024-01-19T10:26:49.275672Z","iopub.status.idle":"2024-01-19T10:28:03.056399Z","shell.execute_reply.started":"2024-01-19T10:26:49.275648Z","shell.execute_reply":"2024-01-19T10:28:03.055193Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyter-server~=1.16, but you have jupyter-server 2.12.3 which is incompatible.\nbeatrix-jupyterlab 2023.814.150030 requires jupyterlab~=3.4, but you have jupyterlab 4.0.10 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 14.0.2 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\nCollecting git+https://github.com/openai/CLIP.git\n  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-0gfqhde5\n  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-0gfqhde5\n  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ftfy in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (6.1.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2023.12.25)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (2.0.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from clip==1.0) (0.15.1)\nRequirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /opt/conda/lib/python3.10/site-packages (from ftfy->clip==1.0) (0.2.13)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->clip==1.0) (3.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (1.24.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (2.31.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->clip==1.0) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->clip==1.0) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision->clip==1.0) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->clip==1.0) (1.3.0)\nBuilding wheels for collected packages: clip\n  Building wheel for clip (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=3ab7454c9b6501ac63ad949d40f942cac168159f78df544142e1392e55ccda39\n  Stored in directory: /tmp/pip-ephem-wheel-cache-mthnxjah/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\nSuccessfully built clip\nInstalling collected packages: clip\nSuccessfully installed clip-1.0\nNote: you may need to restart the kernel to use updated packages.\nCollecting GPUtil\n  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: GPUtil\n  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=bfcc296dadf80f84e85f822aa5212d64f7ac1325e71192420ad5fa5573281c0b\n  Stored in directory: /root/.cache/pip/wheels/a9/8a/bd/81082387151853ab8b6b3ef33426e98f5cbfebc3c397a9d4d0\nSuccessfully built GPUtil\nInstalling collected packages: GPUtil\nSuccessfully installed GPUtil-1.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**All the imports**","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport torch\nimport pickle\nimport json\nimport torchinfo\nimport torch.nn as nn\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:57:34.186340Z","iopub.execute_input":"2024-01-19T10:57:34.186734Z","iopub.status.idle":"2024-01-19T10:57:34.192678Z","shell.execute_reply.started":"2024-01-19T10:57:34.186702Z","shell.execute_reply":"2024-01-19T10:57:34.191725Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"**Helper utilities**","metadata":{}},{"cell_type":"code","source":"\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    #cuda.select_device(0)\n    #cuda.close()\n    #cuda.select_device(0)\n    \n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T09:07:30.953763Z","iopub.execute_input":"2024-01-19T09:07:30.954669Z","iopub.status.idle":"2024-01-19T09:07:30.959473Z","shell.execute_reply.started":"2024-01-19T09:07:30.954634Z","shell.execute_reply":"2024-01-19T09:07:30.958597Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"First, the projection layer...","metadata":{}},{"cell_type":"code","source":"class IdentityMap(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n    @property\n    def config(self):\n        return {\"mm_projector_type\": 'identity'}\n\n\nclass SimpleResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.pre_norm = nn.LayerNorm(in_channels)\n\n        self.proj = nn.Sequential(\n            nn.Linear(in_channels, out_channels),\n            nn.GELU(),\n            nn.Linear(out_channels, out_channels)\n        )\n    def forward(self, x):\n        x = self.pre_norm(x)\n        return x + self.proj(x)\n\n\nclass SimpleLinearBlock(nn.Module):\n    def __init__(self, in_size, out_size, hidden_size = 50, add_residual_connection=True):\n        super().__init__()\n        self.pre_norm = nn.LayerNorm(in_size)\n        self.proj = nn.Sequential(nn.Linear(in_size, hidden_size),\n                                  nn.GELU(),\n                                  nn.Linear(hidden_size, out_size))\n        self.add_residual_connection = add_residual_connection\n        \n    def forward(self,x):\n        return self.proj(x)\n\n\ndef build_resnet_projection_layer(in_channels, out_channels, hidden_size = 50, mlp_depth=2):\n    res_block = SimpleResBlock(in_channels, out_channels, hidden_size = hidden_size)\n    for _ in range(1, mlp_depth):\n        modules.append(res_block)\n    return nn.Sequential(*modules)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:44:02.919481Z","iopub.execute_input":"2024-01-19T10:44:02.920304Z","iopub.status.idle":"2024-01-19T10:44:02.932141Z","shell.execute_reply.started":"2024-01-19T10:44:02.920269Z","shell.execute_reply":"2024-01-19T10:44:02.931060Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"and then the model....","metadata":{}},{"cell_type":"code","source":"\ndef model_summary(model, input_size):\n    torchinfo.summary(model,\n                      input_size = input_size,\n                      batch_dim=0,\n                      col_names=(\"kernel_size\",\n                                 \"input_size\",\n                                 \"output_size\",\n                                 \"num_params\",\n                                 \"mult_adds\"),\n                       verbose=1,)\n\n\n\nclass MultiModalGPT(nn.Module):\n    \"\"\"\n    Pytorch Lightning module for Transformer\n\n    \"\"\"\n    def __init__(self,\n                 llm_model,\n                 tokenizer,\n                 projection_layer_in_channels,\n                 projection_layer_out_channels,\n                 device,\n                 hidden_size = 32,\n                 ):\n        super(MultiModalGPT, self).__init__()\n        self.llm_model = None\n        self.tokenizer = None\n        self.tokenizer = tokenizer\n        self.projection_layer = SimpleLinearBlock(projection_layer_in_channels,projection_layer_out_channels, hidden_size=hidden_size)\n        self.llm_model = llm_model\n        self.device = device\n        \n        # freeze the llm\n        for param in self.llm_model.parameters():\n            param.requires_grad = False\n    \n    \n    def forward(self, x, max_length=1):\n        print(f\"beginning of projection: {x.shape}\")\n        x = self.projection_layer(x)\n        print(f\"end of projection: {x.shape}\")\n        with torch.no_grad():  \n            x = self.llm_model(inputs_embeds = x, return_dict=False)\n        print(f\"end of llm: logits: {x[0].shape}\")\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:06:37.876274Z","iopub.execute_input":"2024-01-19T11:06:37.876674Z","iopub.status.idle":"2024-01-19T11:06:37.886888Z","shell.execute_reply.started":"2024-01-19T11:06:37.876646Z","shell.execute_reply":"2024-01-19T11:06:37.885788Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"**Data loader**","metadata":{}},{"cell_type":"code","source":"def get_absolute_paths(directory_path, max_files = None):\n    absolute_paths = []\n    image_ids = []\n\n    # Check if the given path is a valid directory\n    if os.path.isdir(directory_path):\n        # Iterate over all files in the directory\n        for root, _, files in os.walk(directory_path):\n            for file in tqdm(files):\n                # extract image ID\n                image_ids.append(Path(file).stem)\n                # Construct the absolute path for each file\n                absolute_path = os.path.abspath(os.path.join(root, file))\n                absolute_paths.append(absolute_path)\n                if max_files is not None and len(absolute_paths) > max_files:\n                    break\n    return absolute_paths, image_ids\n\n\ndef parse_captions_file(captions_path, captions_key):\n    \"\"\"\n    Read a JSON file and return its contents as a dictionary.\n\n    Parameters:\n    - file_path (str): The path to the JSON file.\n\n    Returns:\n    - dict: The contents of the JSON file as a dictionary.\n    \"\"\"\n    try:\n        with open(captions_path, 'r') as file:\n            data = json.load(file)\n        captions = {}\n        annotations = data[captions_key]\n        for annotation in annotations:\n            captions[annotation['image_id']] = annotation['caption']\n        return captions\n    except FileNotFoundError:\n        print(f\"Error: File not found - {captions_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Unable to decode JSON in file - {captions_path}\")\n\n        \ndef load_pickle_file(file_path):\n    with open(file_path, 'rb') as fh:\n        data = pickle.load(fh)\n    keys = list(data.keys()) \n    assert len(keys) == 1\n    return data[keys[0]]\n\n\nclass PickleDataset(Dataset):\n\n    def __init__(self, \n                 all_images,\n                 image_ids,\n                 captions_path,\n                 captions_key,\n                 tokenizer, \n                 max_len_of_sentence=2048):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.ds = None\n        self.image_file_names = None\n        self.captions_key = captions_key\n        self.images_path = images_path\n        self.bos_token = self.tokenizer.bos_token\n        self.eos_token = self.tokenizer.eos_token\n        self.pad_token = self.tokenizer.pad_token\n        self.max_len_of_sentence = max_len_of_sentence\n        self.all_images = all_images\n        self.image_ids = image_ids\n        self.captions = parse_captions_file(captions_path, captions_key)\n        \n\n    def __len__(self):\n        return len(self.image_ids)\n\n\n    def __getitem__(self, idx):\n\n        # get image embeddings\n        img_embds = load_pickle_file(self.all_images[idx])\n        img_embds = torch.tensor(np.expand_dims(img_embds,1))\n        this_img_id = self.image_ids[idx]\n        \n        # get caption\n        caption = self.captions[int(this_img_id)]\n        tokenized_caption = self.tokenize_caption(caption)\n        \n        return {\n            \"image_embeddings\": img_embds,\n            \"image_id\": this_img_id,\n            \"caption\": caption,\n            \"tokenized_caption\": tokenized_caption\n        }\n    \n    def tokenize_caption(self, caption):\n        tokens = self.tokenizer(caption)\n        caption_encoded = self.tokenizer(caption, return_tensors=\"pt\", return_attention_mask=False)\n        num_padding_tokens = self.max_len_of_sentence - 2\n        # Add <s> and </s> token\n        tokenized_caption = torch.cat(\n            [\n                self.bos_token,\n                caption_encoded,\n                self.eos_token,\n                torch.tensor([self.pad_token] * num_padding_tokens, dtype=torch.int64),\n            ],\n            dim=0,)\n\n        ## Add only the <s>\n        #y = torch.cat(\n        #    [\n        #        caption_encoded,\n        #        self.eos_token,\n        #        torch.tensor([self.pad_token] * num_padding_tokens, dtype=torch.int64),\n        #    ],\n        #    dim=0,\n        #)\n\n        return tokenized_caption\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T13:03:54.876812Z","iopub.execute_input":"2024-01-19T13:03:54.877538Z","iopub.status.idle":"2024-01-19T13:03:54.897613Z","shell.execute_reply.started":"2024-01-19T13:03:54.877501Z","shell.execute_reply":"2024-01-19T13:03:54.896468Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"def train(\n    config,\n    model,\n    state: dict,\n    train_dataloader: DataLoader,\n    val_dataloader: DataLoader,\n) -> None:\n\n    if val_dataloader is not None:\n        validate(model, val_dataloader)  # sanity check\n\n    for state[\"iter_num\"], train_data in enumerate(train_dataloader, state[\"iter_num\"]):\n        if state[\"iter_num\"] >= max_iters or state[\"iter_num\"] % state[\"save_interval\"] == 0:\n            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n            print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n            save(checkpoint_path, state)\n            break\n\n        # determine and set the learning rate for this iteration\n        lr = get_lr(state[\"iter_num\"]) if decay_lr else learning_rate\n        for param_group in optimizer.param_groups:\n            param_group[\"lr\"] = lr\n\n        iter_t0 = time.perf_counter()\n\n        image_embeddings = train_data['image_embeddings'][:, 0 : config[\"max_seq_length\"].contiguous()\n        targets = train_data['tokenized_caption'][:, 1 : config[\"max_seq_length\"] + 1].contiguous()\n\n        is_accumulating = (state[\"iter_num\"] + 1) % gradient_accumulation_steps != 0\n        with fabric.no_backward_sync(model, enabled=is_accumulating):\n            logits = model(input_ids)\n            loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n            fabric.backward(loss / gradient_accumulation_steps)\n\n        # return\n\n        if not is_accumulating:\n            fabric.clip_gradients(model, optimizer, max_norm=grad_clip)\n            optimizer.step()\n            optimizer.zero_grad()\n            state[\"step_count\"] += 1\n\n        t1 = time.perf_counter()\n        total_lengths += input_ids.size(1)\n        speed_monitor.on_train_batch_end(\n            (state[\"iter_num\"] + 1) * micro_batch_size,\n            t1 - total_t0,\n            # this assumes that device FLOPs are the same and that all devices have the same batch size\n            fabric.world_size,\n            flops_per_batch=measured_flops,\n            lengths=total_lengths,\n        )\n        if state[\"iter_num\"] % log_interval == 0:\n            fabric.print(\n                f\"iter {state['iter_num']} step {state['step_count']}: loss {loss.item():.4f}, LR: {lr:.6f}, iter time:\"\n                f\" {(t1 - iter_t0) * 1000:.2f}ms{' (optimizer.step)' if not is_accumulating else ''}\"\n            )\n\n        if val_dataloader is not None and not is_accumulating and state[\"step_count\"] % eval_interval == 0:\n            t0 = time.perf_counter()\n            val_loss = validate(fabric, model, val_dataloader)\n            t1 = time.perf_counter() - t0\n            speed_monitor.eval_end(t1)\n            fabric.print(f\"step {state['iter_num']}: val loss {val_loss.item():.4f}, val time: {t1 * 1000:.2f}ms\")\n            fabric.barrier()\n        if not is_accumulating and state[\"step_count\"] % save_interval == 0:\n            checkpoint_path = out_dir / f\"iter-{state['iter_num']:06d}-ckpt.pth\"\n            fabric.print(f\"Saving checkpoint to {str(checkpoint_path)!r}\")\n            fabric.save(checkpoint_path, state)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def chunked_cross_entropy(\n    logits: Union[torch.Tensor, List[torch.Tensor]], targets: torch.Tensor, chunk_size: int = 128\n) -> torch.Tensor:\n    # with large max_sequence_lengths, the beginning of `backward` allocates a large memory chunk which can dominate\n    # the memory usage in fine-tuning settings with low number of parameters.\n    # as a workaround hack, the cross entropy computation is chunked to force it to deallocate on the go, reducing\n    # the memory spike's magnitude\n\n    # lm_head was chunked (we are fine-tuning)\n    if isinstance(logits, list):\n        # don't want to chunk cross entropy\n        if chunk_size == 0:\n            logits = torch.cat(logits, dim=1)\n            logits = logits.reshape(-1, logits.size(-1))\n            targets = targets.reshape(-1)\n            return torch.nn.functional.cross_entropy(logits, targets, ignore_index=-1)\n\n        # chunk cross entropy\n        logit_chunks = [logit_chunk.reshape(-1, logit_chunk.size(-1)) for logit_chunk in logits]\n        target_chunks = [target_chunk.reshape(-1) for target_chunk in targets.split(logits[0].size(1), dim=1)]\n        loss_chunks = [\n            torch.nn.functional.cross_entropy(logit_chunk, target_chunk, ignore_index=-1, reduction=\"none\")\n            for logit_chunk, target_chunk in zip(logit_chunks, target_chunks)\n        ]\n        return torch.cat(loss_chunks).mean()\n\n    # no chunking at all\n    logits = logits.reshape(-1, logits.size(-1))\n    targets = targets.reshape(-1)\n    if chunk_size == 0:\n        return torch.nn.functional.cross_entropy(logits, targets, ignore_index=-1)\n\n    # lm_head wasn't chunked, chunk cross entropy\n    logit_chunks = logits.split(chunk_size)\n    target_chunks = targets.split(chunk_size)\n    loss_chunks = [\n        torch.nn.functional.cross_entropy(logit_chunk, target_chunk, ignore_index=-1, reduction=\"none\")\n        for logit_chunk, target_chunk in zip(logit_chunks, target_chunks)\n    ]\n    return torch.cat(loss_chunks).mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom pytorch_lightning import LightningModule\n\nclass LitMultiModalGPT(LightningModule):\n    \"\"\"\n    Pytorch Lightning module for Transformer\n\n    \"\"\"\n    def __init__(self,\n                 multimodal_gpt,\n                 loss_criterion,\n                 tokenizer,\n                 num_validation_examples=10,\n                 num_training_steps=100000):\n        super().__init__()\n        self.loss_criterion = loss_criterion\n        self.tokenizer = tokenizer\n        self.num_validation_examples = num_validation_examples\n        self.num_training_steps = num_training_steps\n        self._vocab_len = tokenizer.get_vocab_size()\n        self.model = multimodal_gpt\n        self.scheduler = None\n        self.scheduler_dict = {}\n        self.optimizer = None\n        self.this_step_train_loss = None\n        self.predicted_list = []\n        self.expected_list = []\n        self.save_hyperparameters(ignore=['loss_criterion', 'epoch'])\n\n\n    def set_optimizer(self, optimizer):\n        self.optimizer = optimizer\n\n    def set_scheduler_dict(self, scheduler, freq='step'):\n        self.scheduler = scheduler\n        self.scheduler_dict = {\n            \"scheduler\": self.scheduler,\n            \"interval\": freq,\n        }\n\n    def configure_optimizers(self):\n        if self.scheduler_dict:\n            return {\"optimizer\": self.optimizer, \"lr_scheduler\": self.scheduler_dict}\n        return {\"optimizer\": self.optimizer}\n\n    def forward(self, x):\n        outputs = self.model(x, return_dict=False)\n        return outputs\n\n        \n    def evaluate(self, batch, stage=None):\n        \"\"\"\n        Evaluate the model on validation dataset.\n        \"\"\"\n        #model_out = self.greedy_decode(encoder_input, encoder_mask)\n\n        #model_out_text = self.tokenizer_tgt.decode(model_out.detach().cpu().numpy())\n\n\n        #if stage:\n        #    # print the source, target, and the model output\n        #    print(\"*****************************************\")\n        #    print(f\"{f'SOURCE: ' :>12}{source_text}\")\n        #    print(f\"{f'TARGET: ' :>12}{target_text}\")\n        #    print(f\"{f'PREDICTED: ' :>12}{model_out_text}\")\n        #    print(\"*****************************************\\n\")\n        return None\n\n    def training_step(self, batch):\n        tokenized_caption = batch['tokenized_caption']  # (B, seq_len)\n        logits, outputs = self(batch['image_embeddings'])\n        loss = chunked_cross_entropy(logits, targets, chunk_size=0)\n        self.log(\"train_loss\", loss.item(), prog_bar=True)\n        self.this_step_train_loss = loss.item()\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        if batch_idx < self.num_validation_examples:\n            predicted, expected = self.evaluate(batch, \"val\")\n            self.predicted_list.append(predicted)\n            self.expected_list.append(expected)\n\n\n    def test_step(self, batch, batch_idx):\n        if batch_idx < self.num_validation_examples:\n            self.evaluate(batch, \"test\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T12:30:25.821926Z","iopub.execute_input":"2024-01-19T12:30:25.822313Z","iopub.status.idle":"2024-01-19T12:30:26.863324Z","shell.execute_reply.started":"2024-01-19T12:30:25.822281Z","shell.execute_reply":"2024-01-19T12:30:26.862379Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"**Download the LLM and tokenizer**","metadata":{}},{"cell_type":"code","source":"phi_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\nphi_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:45:41.817614Z","iopub.execute_input":"2024-01-19T10:45:41.818258Z","iopub.status.idle":"2024-01-19T10:46:10.898730Z","shell.execute_reply.started":"2024-01-19T10:45:41.818223Z","shell.execute_reply":"2024-01-19T10:46:10.896193Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81c215d8215e4e15b22b12b3134a199d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"255b262150f649009ae75459946b8fa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99379b5440a440c7af3c1fc7d7136582"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43bbb00366a94231bd6c5cbe53a012ec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a4b9114139d4a4590a265b331d90bd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e92f77c835614710933d81115bc06a49"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/866 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e263e04c1ff433194b5433ddf155310"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87476b7de66542029c8c40028dd039eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"474c231f19794df58a6737a7ce3b69c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c619d70f3d4154b6d385059568df41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22be9fed42334fdaa2499db87b8c73bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af282c1debdb4086af8d706b0d8f6db2"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/phi-2 were not used when initializing PhiForCausalLM: ['model.layers.0.self_attn.v_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.19.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.bias', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.1.self_attn.k_proj.bias', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.bias', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.28.self_attn.q_proj.bias', 'model.layers.13.self_attn.v_proj.bias', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.18.self_attn.q_proj.bias', 'model.layers.8.self_attn.q_proj.bias', 'model.layers.9.self_attn.k_proj.bias', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.bias', 'model.layers.24.self_attn.v_proj.bias', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.bias', 'model.layers.22.self_attn.q_proj.bias', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.bias', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.17.self_attn.q_proj.bias', 'model.layers.26.self_attn.q_proj.bias', 'model.layers.24.self_attn.k_proj.bias', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.bias', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.13.self_attn.q_proj.bias', 'model.layers.6.self_attn.q_proj.bias', 'model.layers.0.self_attn.q_proj.bias', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.4.self_attn.k_proj.bias', 'model.layers.3.self_attn.v_proj.bias', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.bias', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.bias', 'model.layers.27.self_attn.k_proj.bias', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.bias', 'model.layers.12.self_attn.q_proj.bias', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.bias', 'model.layers.6.self_attn.k_proj.bias', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.25.self_attn.q_proj.bias', 'model.layers.9.self_attn.q_proj.bias', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.bias', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.bias', 'model.layers.29.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.bias', 'model.layers.8.self_attn.v_proj.bias', 'model.layers.28.self_attn.k_proj.bias', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.30.self_attn.k_proj.bias', 'model.layers.20.self_attn.k_proj.bias', 'model.layers.23.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.bias', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.2.self_attn.k_proj.bias', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.6.self_attn.v_proj.bias', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.bias', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.30.self_attn.q_proj.bias', 'model.layers.20.self_attn.v_proj.bias', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.31.self_attn.k_proj.bias', 'model.layers.14.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.8.self_attn.k_proj.bias', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.bias', 'model.layers.11.self_attn.k_proj.bias', 'model.layers.13.self_attn.k_proj.bias', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.5.self_attn.q_proj.bias', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.bias', 'model.layers.9.self_attn.v_proj.bias', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.bias', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.16.self_attn.k_proj.bias', 'model.layers.15.self_attn.q_proj.bias', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.1.self_attn.q_proj.bias', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.bias', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.16.self_attn.v_proj.bias', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.bias', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.bias', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.bias', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.23.self_attn.q_proj.bias', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.2.self_attn.q_proj.bias', 'model.layers.21.self_attn.k_proj.bias', 'model.layers.7.self_attn.v_proj.bias', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.7.self_attn.q_proj.bias', 'model.layers.16.self_attn.q_proj.bias', 'model.layers.30.self_attn.v_proj.bias', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.3.self_attn.q_proj.bias', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.5.self_attn.v_proj.bias', 'model.layers.15.self_attn.k_proj.bias', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.bias', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.29.self_attn.q_proj.bias', 'model.layers.17.self_attn.k_proj.bias', 'model.layers.21.self_attn.v_proj.bias', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.1.self_attn.v_proj.bias', 'model.layers.0.self_attn.v_proj.bias', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.bias', 'model.layers.14.self_attn.q_proj.bias', 'model.layers.24.self_attn.q_proj.bias', 'model.layers.5.self_attn.k_proj.bias', 'model.layers.7.self_attn.k_proj.bias', 'model.layers.27.self_attn.q_proj.bias', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.bias', 'model.layers.11.self_attn.v_proj.bias', 'model.layers.18.self_attn.k_proj.bias', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.bias', 'model.layers.3.self_attn.k_proj.bias', 'model.layers.12.self_attn.k_proj.bias', 'model.layers.11.self_attn.q_proj.bias', 'model.layers.4.self_attn.q_proj.bias', 'model.layers.15.self_attn.q_proj.weight']\n- This IS expected if you are initializing PhiForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing PhiForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of PhiForCausalLM were not initialized from the model checkpoint at microsoft/phi-2 and are newly initialized: ['model.layers.8.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/74.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4de8372453ae40cd814b0585f01fce2b"}},"metadata":{}}]},{"cell_type":"markdown","source":"**Define Hyperparameters**","metadata":{}},{"cell_type":"code","source":"train_dataset_path = '/kaggle/input/coco2017-clip-image-embeddings/coco_embeddings_clip_vision_1x768'\ncaptions_path = '/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\ncaptions_key = 'annotations'\nbatch_size = 1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nval_split_size = 0.1\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T13:18:48.882889Z","iopub.execute_input":"2024-01-19T13:18:48.883573Z","iopub.status.idle":"2024-01-19T13:18:48.890234Z","shell.execute_reply.started":"2024-01-19T13:18:48.883519Z","shell.execute_reply":"2024-01-19T13:18:48.889273Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"**Define train dataset and train dataloader**","metadata":{}},{"cell_type":"code","source":"files_list, images_ids_list = get_absolute_paths(train_dataset_path)\nrand_indices = np.arange(len(all_filepaths))\nnp.random.shuffle(rand_indices)\n\nval_split = int(len(all_filepaths)*val_split_size)\n\nval_filepaths, train_filepaths = files_list[rand_indices[:split]], files_list[rand_indices[split:]] \nval_image_ids, train_image_ids = images_ids_list[rand_indices[:split]], images_ids_list[rand_indices[split:]]\n\nprint(f\"Train dataset size: {len(train_filepaths)}\")\nprint(f\"Valid dataset size: {len(valid_filepaths)}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T13:18:51.697407Z","iopub.execute_input":"2024-01-19T13:18:51.698379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = PickleDataset(train_dataset_path, captions_path, captions_key, phi_tokenizer)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:48:42.532029Z","iopub.execute_input":"2024-01-19T10:48:42.532751Z","iopub.status.idle":"2024-01-19T10:50:22.246724Z","shell.execute_reply.started":"2024-01-19T10:48:42.532714Z","shell.execute_reply":"2024-01-19T10:50:22.245490Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 118285/118285 [00:01<00:00, 68370.61it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"#val_ds = HindiAestheticsDataset(val_dataset_path, tokenizer, block_size=block_size)\ntrain_dataloader = DataLoader(dataset = train_ds,\n                              batch_size = batch_size,\n                              num_workers = 1,\n                              collate_fn = None,\n                              shuffle = True)\n#val_dataloader = DataLoader(dataset = val_ds,\n#                            batch_size = 1,\n#                            num_workers = 1,\n#                            collate_fn = None,\n#                            shuffle = False)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:51:04.022453Z","iopub.execute_input":"2024-01-19T10:51:04.023595Z","iopub.status.idle":"2024-01-19T10:51:04.028802Z","shell.execute_reply.started":"2024-01-19T10:51:04.023558Z","shell.execute_reply":"2024-01-19T10:51:04.027667Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"cc = next(iter(train_dataloader))\ninput_embeds = cc['image_embeddings'].to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:51:07.337437Z","iopub.execute_input":"2024-01-19T10:51:07.338186Z","iopub.status.idle":"2024-01-19T10:51:07.821614Z","shell.execute_reply.started":"2024-01-19T10:51:07.338154Z","shell.execute_reply":"2024-01-19T10:51:07.820302Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"input_embeds.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-19T08:53:28.891176Z","iopub.execute_input":"2024-01-19T08:53:28.891595Z","iopub.status.idle":"2024-01-19T08:53:28.899678Z","shell.execute_reply.started":"2024-01-19T08:53:28.891560Z","shell.execute_reply":"2024-01-19T08:53:28.898777Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 768, 1])"},"metadata":{}}]},{"cell_type":"code","source":"multimodal_gpt_model = MultiModalGPT(phi_model, phi_tokenizer, 1, 2560, device, hidden_size = 32)\nmultimodal_gpt_model = multimodal_gpt_model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:42.984346Z","iopub.execute_input":"2024-01-19T11:08:42.984755Z","iopub.status.idle":"2024-01-19T11:08:46.030830Z","shell.execute_reply.started":"2024-01-19T11:08:42.984725Z","shell.execute_reply":"2024-01-19T11:08:46.030025Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"outputs = multimodal_gpt_model(input_embeds)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:08:49.048766Z","iopub.execute_input":"2024-01-19T11:08:49.049157Z","iopub.status.idle":"2024-01-19T11:08:49.259268Z","shell.execute_reply.started":"2024-01-19T11:08:49.049105Z","shell.execute_reply":"2024-01-19T11:08:49.258323Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"beginning of projection: torch.Size([1, 768, 1])\nend of projection: torch.Size([1, 768, 2560])\nend of llm: logits: torch.Size([1, 768, 51200])\n","output_type":"stream"}]},{"cell_type":"code","source":"multimodal_gpt_model.llm_model.max_seq_length","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:47:44.144076Z","iopub.execute_input":"2024-01-19T11:47:44.145083Z","iopub.status.idle":"2024-01-19T11:47:44.192742Z","shell.execute_reply.started":"2024-01-19T11:47:44.145042Z","shell.execute_reply":"2024-01-19T11:47:44.191621Z"},"trusted":true},"execution_count":42,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmultimodal_gpt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1614\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1613\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1614\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n","\u001b[0;31mAttributeError\u001b[0m: 'PhiForCausalLM' object has no attribute 'max_seq_length'"],"ename":"AttributeError","evalue":"'PhiForCausalLM' object has no attribute 'max_seq_length'","output_type":"error"}]},{"cell_type":"code","source":"logits, preds = outputs","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:52:14.684614Z","iopub.execute_input":"2024-01-19T10:52:14.685013Z","iopub.status.idle":"2024-01-19T10:52:14.689842Z","shell.execute_reply.started":"2024-01-19T10:52:14.684981Z","shell.execute_reply":"2024-01-19T10:52:14.688729Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"print(logits.shape) ","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:52:23.523922Z","iopub.execute_input":"2024-01-19T10:52:23.524899Z","iopub.status.idle":"2024-01-19T10:52:23.529823Z","shell.execute_reply.started":"2024-01-19T10:52:23.524860Z","shell.execute_reply":"2024-01-19T10:52:23.528706Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"torch.Size([1, 768, 51200])\n","output_type":"stream"}]},{"cell_type":"code","source":"preds[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:53:26.699179Z","iopub.execute_input":"2024-01-19T10:53:26.699851Z","iopub.status.idle":"2024-01-19T10:53:26.705772Z","shell.execute_reply.started":"2024-01-19T10:53:26.699817Z","shell.execute_reply":"2024-01-19T10:53:26.704858Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 32, 768, 80])"},"metadata":{}}]},{"cell_type":"code","source":"multimodal_gpt_model","metadata":{"execution":{"iopub.status.busy":"2024-01-19T10:54:01.634801Z","iopub.execute_input":"2024-01-19T10:54:01.635773Z","iopub.status.idle":"2024-01-19T10:54:01.643974Z","shell.execute_reply.started":"2024-01-19T10:54:01.635734Z","shell.execute_reply":"2024-01-19T10:54:01.643145Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"MultiModalGPT(\n  (llm_model): PhiForCausalLM(\n    (model): PhiModel(\n      (embed_tokens): Embedding(51200, 2560)\n      (embed_dropout): Dropout(p=0.0, inplace=False)\n      (layers): ModuleList(\n        (0-31): 32 x PhiDecoderLayer(\n          (self_attn): PhiAttention(\n            (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n            (dense): Linear(in_features=2560, out_features=2560, bias=True)\n            (attention_dropout): Dropout(p=0.0, inplace=False)\n            (rotary_emb): PhiRotaryEmbedding()\n          )\n          (mlp): PhiMLP(\n            (activation_fn): NewGELUActivation()\n            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n          )\n          (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n    )\n    (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n  )\n  (projection_layer): SimpleLinearBlock(\n    (pre_norm): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n    (proj): Sequential(\n      (0): Linear(in_features=1, out_features=32, bias=True)\n      (1): GELU(approximate='none')\n      (2): Linear(in_features=32, out_features=2560, bias=True)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#del phi_model\nprint(torch.cuda.memory_allocated())\nmultimodal_gpt_model.to(\"cpu\")\ngc.collect()\n#del input_embeds\n#del phi_model\ntorch.cuda.empty_cache()\nprint(torch.cuda.memory_allocated())\n#print(torch.cuda.memory_reserved())","metadata":{"execution":{"iopub.status.busy":"2024-01-19T11:07:27.369610Z","iopub.execute_input":"2024-01-19T11:07:27.369966Z","iopub.status.idle":"2024-01-19T11:07:36.275653Z","shell.execute_reply.started":"2024-01-19T11:07:27.369931Z","shell.execute_reply":"2024-01-19T11:07:36.274760Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"13539339264\n1172442112\n","output_type":"stream"}]},{"cell_type":"code","source":"input_embeds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = multimodal_gpt_model(input_embeds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}