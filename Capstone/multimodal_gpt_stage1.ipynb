{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":7399829,"sourceType":"datasetVersion","datasetId":4302683}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install -qq -U datasets transformers pyarrow\n%pip install -qq --upgrade transformers ftfy accelerate regex tqdm\n%pip install git+https://github.com/openai/CLIP.git\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All the imports**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pickle\nimport json\nimport torch.nn as nn\nimport numpy as np\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom pathlib import Path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model definition**","metadata":{}},{"cell_type":"markdown","source":"First, the projection layer...","metadata":{}},{"cell_type":"code","source":"class IdentityMap(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x, *args, **kwargs):\n        return x\n\n    @property\n    def config(self):\n        return {\"mm_projector_type\": 'identity'}\n\n\nclass SimpleResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.pre_norm = nn.LayerNorm(in_channels)\n\n        self.proj = nn.Sequential(\n            nn.Linear(in_channels, out_channels),\n            nn.GELU(),\n            nn.Linear(out_channels, out_channels)\n        )\n    def forward(self, x):\n        x = self.pre_norm(x)\n        return x + self.proj(x)\n\n\nclass SimpleLinearBlock(nn.Module):\n    def __init__(self, in_size, out_size, hidden_size = 50, add_residual_connection=True):\n        super().__init__()\n        self.pre_norm = nn.LayerNorm(in_size)\n        self.proj = nn.Sequential(nn.Linear(in_size, hidden_size),\n                                  nn.GELU(),\n                                  nn.Linear(hidden_size, out_size))\n        self.add_residual_connection = add_residual_connection\n        \n    def forward(self,x):\n        return self.proj(x)\n\n\ndef build_resnet_projection_layer(in_channels, out_channels, hidden_size = 50, mlp_depth=2):\n    res_block = SimpleResBlock(in_channels, out_channels, hidden_size = hidden_size)\n    for _ in range(1, mlp_depth):\n        modules.append(res_block)\n    return nn.Sequential(*modules)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"and then the model....","metadata":{}},{"cell_type":"code","source":"class MultiModalGPT(nn.Module):\n    \"\"\"\n    Pytorch Lightning module for Transformer\n\n    \"\"\"\n    def __init__(self,\n                 llm_model,\n                 tokenizer,\n                 projection_layer_in_channels,\n                 projection_layer_out_channels,\n                 device,\n                 hidden_size = 32,\n                 ):\n        super(MultiModalGPT, self).__init__()\n        self.llm_model = None\n        self.tokenizer = None\n        self.llm_model = llm_model\n        # freeze the llm\n        for param in self.llm_model.parameters():\n            param.requires_grad = False\n        self.tokenizer = tokenizer\n        self.projection_layer = SimpleLinearBlock(projection_layer_in_channels,projection_layer_out_channels, hidden_size=hidden_size)\n        self.device = device\n    \n    \n    def forward(self, x, max_length=1):\n        x = self.projection_layer(x)\n        with torch.no_grad():  \n            x = self.llm_model.generate(inputs_embeds = x, max_length=max_length)\n        return x\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Data loader**","metadata":{}},{"cell_type":"code","source":"def get_absolute_paths(directory_path, max_files = None):\n    absolute_paths = []\n    image_ids = []\n\n    # Check if the given path is a valid directory\n    if os.path.isdir(directory_path):\n        # Iterate over all files in the directory\n        for root, _, files in os.walk(directory_path):\n            for file in tqdm(files):\n                # extract image ID\n                image_ids.append(Path(file).stem)\n                # Construct the absolute path for each file\n                absolute_path = os.path.abspath(os.path.join(root, file))\n                absolute_paths.append(absolute_path)\n                if max_files is not None and len(absolute_paths) > max_files:\n                    break\n    return absolute_paths, image_ids\n\n\ndef parse_captions_file(captions_path, captions_key):\n    \"\"\"\n    Read a JSON file and return its contents as a dictionary.\n\n    Parameters:\n    - file_path (str): The path to the JSON file.\n\n    Returns:\n    - dict: The contents of the JSON file as a dictionary.\n    \"\"\"\n    try:\n        with open(captions_path, 'r') as file:\n            data = json.load(file)\n        captions = {}\n        annotations = data[captions_key]\n        for annotation in annotations:\n            captions[annotation['image_id']] = annotation['caption']\n        return captions\n    except FileNotFoundError:\n        print(f\"Error: File not found - {captions_path}\")\n    except json.JSONDecodeError:\n        print(f\"Error: Unable to decode JSON in file - {captions_path}\")\n\n        \ndef load_pickle_file(file_path):\n    with open(file_path, 'rb') as fh:\n        data = pickle.load(fh)\n    keys = list(data.keys()) \n    assert len(keys) == 1\n    return data[keys[0]]\n\n\nclass PickleDataset(Dataset):\n\n    def __init__(self, \n                 images_path,\n                 captions_path,\n                 captions_key,\n                 tokenizer, \n                 max_embd_len=2048):\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.ds = None\n        self.image_file_names = None\n        self.captions_key = captions_key\n        self.images_path = images_path\n        self.all_images, self.image_ids = get_absolute_paths(images_path)\n        self.captions = parse_captions_file(captions_path, captions_key)\n        \n\n    def __len__(self):\n        return len(self.image_ids)\n\n\n    def __getitem__(self, idx):\n\n        # get image embeddings\n        img_embds = load_pickle_file(self.all_images[idx])\n        img_embds = torch.tensor(np.expand_dims(img_embds,1))\n        this_img_id = self.image_ids[idx]\n        \n        # get caption\n        caption = self.captions[int(this_img_id)]\n        return {\n            \"image_embeddings\": img_embds,\n            \"image_id\": this_img_id,\n            \"caption\": caption,\n        }\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Download the LLM and tokenizer**","metadata":{}},{"cell_type":"code","source":"phi_tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\nphi_model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define Hyperparameters**","metadata":{}},{"cell_type":"code","source":"train_dataset_path = '/kaggle/input/coco2017-clip-image-embeddings/coco_embeddings_clip_vision_1x768'\ncaptions_path = '/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\ncaptions_key = 'annotations'\nbatch_size = 1\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Define train dataset and train dataloader**","metadata":{}},{"cell_type":"code","source":"train_ds = PickleDataset(train_dataset_path, captions_path, captions_key, phi_tokenizer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#val_ds = HindiAestheticsDataset(val_dataset_path, tokenizer, block_size=block_size)\ntrain_dataloader = DataLoader(dataset = train_ds,\n                              batch_size = batch_size,\n                              num_workers = 1,\n                              collate_fn = None,\n                              shuffle = True)\n#val_dataloader = DataLoader(dataset = val_ds,\n#                            batch_size = 1,\n#                            num_workers = 1,\n#                            collate_fn = None,\n#                            shuffle = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cc = next(iter(train_dataloader))\ninput_embeds = cc['image_embeddings'].to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_embeds.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"multimodal_gpt_model = MultiModalGPT(phi_model, phi_tokenizer, 1, 2560, device, hidden_size = 32)\nmultimodal_gpt_model = multimodal_gpt_model.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#del multimodal_gpt_model\n#del input_embeds\n#del phi_model\ntorch.cuda.empty_cache()\nprint(torch.cuda.memory_allocated())\nprint(torch.cuda.memory_reserved())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_embeds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = multimodal_gpt_model(input_embeds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}}]}