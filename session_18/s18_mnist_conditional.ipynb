{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/session_18/s18_mnist_conditional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gfLZh0k_ov5u",
        "outputId": "ec1eb8d3-7586-4aec-ea06-33da84c8dcf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.9.post0-py3-none-any.whl (727 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/727.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.4/727.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2023.6.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.5)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (17.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2.0.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch-lightning) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->pytorch-lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.9.0 pytorch-lightning-2.0.9.post0 torchmetrics-1.2.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-lightning-bolts==0.2.5rc1 (from versions: 0.1.0, 0.1.1, 0.2.0, 0.2.1, 0.2.2, 0.2.3, 0.2.4, 0.2.5, 0.3.0, 0.3.1, 0.3.2, 0.3.2.post0, 0.3.2.post1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-lightning-bolts==0.2.5rc1\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.8/300.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.5/829.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install pytorch-lightning\n",
        "! pip install pytorch-lightning-bolts==0.2.5rc1\n",
        "! pip install --quiet \"torchinfo\" \"lightning-bolts\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KXliwSCnov5v"
      },
      "outputs": [],
      "source": [
        "# prerequisites\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as pl\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBtVRKZ-ov5w"
      },
      "source": [
        "**Define VAE class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IF8deywWov5x"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, num_labels=10, label_enc_dim=28):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        # label embedding\n",
        "        self.label_enc_dim = label_enc_dim\n",
        "        self.num_labels = num_labels\n",
        "        self.class_embeddings = nn.Embedding(self.num_labels, self.label_enc_dim)\n",
        "\n",
        "        # encoder part\n",
        "        self.enc_inp_dim = x_dim+(self.num_labels*self.label_enc_dim)\n",
        "        self.fc1 = nn.Linear(self.enc_inp_dim, h_dim1)\n",
        "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
        "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
        "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
        "        # decoder part\n",
        "        self.dec_inp_dim = z_dim + (self.num_labels*self.label_enc_dim)\n",
        "        self.fc4 = nn.Linear(self.dec_inp_dim, h_dim2)\n",
        "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
        "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
        "\n",
        "\n",
        "    def encoder(self, x):\n",
        "        h = F.relu(self.fc1(x))\n",
        "        h = F.relu(self.fc2(h))\n",
        "        return self.fc31(h), self.fc32(h)  # mu, log_var\n",
        "\n",
        "    def sampling(self, mu, log_var):\n",
        "        std = torch.exp(0.5 * log_var)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std).add_(mu)  # return z sample\n",
        "\n",
        "    def decoder(self, z, label):\n",
        "        label_enc = self.get_label_embedding(label)\n",
        "        z = torch.cat([z, label_enc], dim=1)\n",
        "        h = F.relu(self.fc4(z))\n",
        "        h = F.relu(self.fc5(h))\n",
        "        return F.sigmoid(self.fc6(h))\n",
        "\n",
        "    def get_label_embedding(self, label):\n",
        "        label_one_hot = torch.nn.functional.one_hot(label, num_classes=10)\n",
        "        label_enc = self.class_embeddings(label_one_hot)\n",
        "        label_enc = torch.flatten(label_enc, start_dim=1)\n",
        "        return label_enc\n",
        "\n",
        "    def forward(self, x, label):\n",
        "        x = x.view(-1, 784)\n",
        "        label_enc = self.get_label_embedding(label)\n",
        "        x = torch.cat([x, label_enc], dim=1)\n",
        "        mu, log_var = self.encoder(x)\n",
        "        z = self.sampling(mu, log_var)\n",
        "        return self.decoder(z, label), mu, log_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3iintFMov5y"
      },
      "source": [
        "**Define the Loss Function (Reconstruction Loss + KL Divergence Loss)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4_akCVeYov5z"
      },
      "outputs": [],
      "source": [
        "# return reconstruction error + KL divergence losses\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
        "    return BCE + KLD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8appfv-Iov50"
      },
      "source": [
        "**Define Train and Test methods**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aXkm0aZWov51"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        data = data.cuda()\n",
        "        labels = labels.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        recon_batch, mu, log_var = vae(data, labels)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "def test(epoch):\n",
        "    vae.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data = data.cuda()\n",
        "            labels = labels.cuda()\n",
        "            recon, mu, log_var = vae(data, labels)\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
        "            if epoch % 10 == 0:\n",
        "                z = torch.randn(64, 2).cuda()\n",
        "                this_label = 5 * torch.ones(64, dtype=torch.int64).cuda()\n",
        "                sample = vae.decoder(z, this_label).cuda()\n",
        "                save_image(sample.view(64, 1, 28, 28), f\"./samples/sample_{epoch}.png\")\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqT4-yNHov54"
      },
      "source": [
        "**Show Image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "NGm1cWWqov56"
      },
      "outputs": [],
      "source": [
        "def show_image(img_tensor):\n",
        "    #np_op_img = np.asarray(img_tensor.cpu()).squeeze()\n",
        "    pl.figure()\n",
        "    pl.imshow(img_tensor, cmap='gray')\n",
        "    pl.colorbar()\n",
        "    pl.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzakV1_oov57"
      },
      "source": [
        "**Add the logic to generate conditional images. Here, we generate two images, one based on the input image and label and the other based on Input label. Finally, interpolate between the two samples.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "RqWgwzLpov58"
      },
      "outputs": [],
      "source": [
        "def conditional_generation(vae, test_loader, num_images=32, interp_weight=0.1):\n",
        "    data, label = next(iter(test_loader))\n",
        "    data = data.cuda()\n",
        "    label = label.cuda()\n",
        "    gen_images = []\n",
        "    for cnt in range(num_images):\n",
        "        x = data[cnt].view(-1, 784)\n",
        "        l = label[cnt].unsqueeze(0)\n",
        "        l_e = vae.get_label_embedding(l)\n",
        "        x_l = torch.cat([x, l_e], dim=1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # get mean and std of given image and label\n",
        "            mu0, log_var0 = vae.encoder(x_l)\n",
        "            z0 = vae.sampling(mu0, log_var0)\n",
        "\n",
        "            # draw a random value of mean and std from standard gaussian\n",
        "            shuffled_label_val = np.random.choice(np.arange(10, dtype=int))\n",
        "            shuffled_label = torch.tensor(shuffled_label_val, dtype=torch.int64).unsqueeze(0).cuda()\n",
        "            z1 = torch.randn(1, 2).cuda()\n",
        "\n",
        "            z_interp = torch.lerp(z0, z1, weight=interp_weight)\n",
        "            img_tensor = vae.decoder(z_interp, shuffled_label).view(1, 28, 28)\n",
        "            gen_images.append(img_tensor)\n",
        "    final_output = torch.vstack(gen_images)#.unsqueeze(1)\n",
        "    print(final_output.shape)\n",
        "    show_image(final_output.cpu().permute(1,2,0))\n",
        "    #save_image(final_output, f'./samples/final_output_{weight*100}p.png')\n",
        "    return final_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W291JjmSov59"
      },
      "source": [
        "**Run the training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UjESNjL0ov59",
        "outputId": "65150565-8cbd-40b6-e366-d9b95e6b5926",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]\tLoss: 544.157959\n",
            "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 183.009048\n",
            "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 165.256317\n",
            "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 161.788971\n",
            "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 166.323334\n",
            "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 166.779587\n",
            "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 147.214432\n",
            "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 143.506744\n",
            "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 158.078873\n",
            "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 145.276337\n",
            "====> Epoch: 0 Average loss: 162.7415\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 129.568542\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 139.881393\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 147.922821\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 142.019257\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 136.718048\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 131.878372\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 134.786469\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 136.731110\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 133.567795\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 131.920441\n",
            "====> Epoch: 1 Average loss: 140.0174\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 139.675140\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 139.448593\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 139.862564\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 134.359558\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 134.240005\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 130.671555\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 137.333466\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 130.540344\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 138.528824\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 133.040436\n",
            "====> Epoch: 2 Average loss: 137.0248\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 140.593033\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 142.797134\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 129.751526\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 141.280731\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 127.349304\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 145.548492\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 129.549377\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 129.163620\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 139.474350\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 137.865280\n",
            "====> Epoch: 3 Average loss: 135.5131\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 138.555695\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 118.046173\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 133.586960\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 129.193436\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 129.984177\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 132.325943\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 135.428894\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 126.426506\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 132.625015\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 131.170563\n",
            "====> Epoch: 4 Average loss: 134.5787\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 133.911972\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 126.356781\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 140.512482\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 133.718613\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 131.347397\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 132.523712\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 141.086975\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 138.385361\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 128.287186\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 142.298187\n",
            "====> Epoch: 5 Average loss: 133.8430\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 131.501266\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 130.238541\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 132.249008\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 133.549500\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 135.298950\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 134.606720\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 131.200745\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 143.447174\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 131.306656\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 120.764069\n",
            "====> Epoch: 6 Average loss: 133.2301\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 140.935379\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 132.213104\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 132.198257\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 132.314056\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 129.443848\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 138.676147\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 137.190903\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 132.322479\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 142.500534\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 139.639053\n",
            "====> Epoch: 7 Average loss: 132.7485\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 134.870316\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 126.677826\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 134.585403\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 134.440033\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 130.322037\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 143.824005\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 121.595505\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 142.270676\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 137.071045\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 134.143967\n",
            "====> Epoch: 8 Average loss: 132.2902\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 132.885437\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 131.784592\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 125.239883\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 131.546906\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 122.412903\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 130.432541\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 130.031525\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 135.826157\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 129.305344\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 129.591812\n",
            "====> Epoch: 9 Average loss: 131.9533\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 130.998230\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 140.032837\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 138.808060\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 129.475128\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 127.231659\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 133.988098\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 131.518906\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 138.547089\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 124.579361\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 128.921860\n",
            "====> Epoch: 10 Average loss: 131.6295\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 134.900330\n",
            "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 126.501289\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 135.502197\n",
            "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 124.871819\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 127.176407\n",
            "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 129.324265\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 135.214859\n",
            "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 126.782211\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 132.102463\n",
            "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 133.371506\n",
            "====> Epoch: 11 Average loss: 131.3896\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 131.019058\n",
            "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 139.526779\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 133.816177\n",
            "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 126.522758\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 132.533203\n",
            "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 134.516617\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 122.014923\n",
            "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 136.644974\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 120.563225\n",
            "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 127.914116\n",
            "====> Epoch: 12 Average loss: 131.1582\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 132.687805\n",
            "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 135.674652\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 127.908646\n",
            "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 131.838654\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 132.247391\n",
            "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 130.293167\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 133.664993\n",
            "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 127.626221\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 140.074158\n",
            "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 129.674850\n",
            "====> Epoch: 13 Average loss: 130.9392\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 130.630737\n",
            "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 124.519135\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 120.252304\n",
            "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 125.948349\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 125.833176\n",
            "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 124.838150\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 137.475021\n",
            "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 123.518250\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 122.164742\n",
            "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 134.692184\n",
            "====> Epoch: 14 Average loss: 130.7867\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 121.502831\n",
            "Train Epoch: 15 [6400/60000 (11%)]\tLoss: 121.833244\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 126.623886\n",
            "Train Epoch: 15 [19200/60000 (32%)]\tLoss: 132.556030\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 131.216415\n",
            "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 124.512024\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 124.876579\n",
            "Train Epoch: 15 [44800/60000 (75%)]\tLoss: 127.821930\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 139.174850\n",
            "Train Epoch: 15 [57600/60000 (96%)]\tLoss: 127.038391\n",
            "====> Epoch: 15 Average loss: 130.6038\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 135.335129\n",
            "Train Epoch: 16 [6400/60000 (11%)]\tLoss: 124.697350\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 134.287964\n",
            "Train Epoch: 16 [19200/60000 (32%)]\tLoss: 126.065453\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 127.390137\n",
            "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 130.950684\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 121.548988\n",
            "Train Epoch: 16 [44800/60000 (75%)]\tLoss: 128.684998\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 126.365776\n",
            "Train Epoch: 16 [57600/60000 (96%)]\tLoss: 127.718788\n",
            "====> Epoch: 16 Average loss: 130.4722\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 124.242813\n",
            "Train Epoch: 17 [6400/60000 (11%)]\tLoss: 122.917328\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 131.338287\n",
            "Train Epoch: 17 [19200/60000 (32%)]\tLoss: 126.005760\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 128.952942\n",
            "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 129.554489\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 131.344818\n",
            "Train Epoch: 17 [44800/60000 (75%)]\tLoss: 129.893936\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 130.843964\n",
            "Train Epoch: 17 [57600/60000 (96%)]\tLoss: 127.603607\n",
            "====> Epoch: 17 Average loss: 130.3451\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 132.764938\n",
            "Train Epoch: 18 [6400/60000 (11%)]\tLoss: 136.127884\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 124.145195\n",
            "Train Epoch: 18 [19200/60000 (32%)]\tLoss: 128.745956\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 134.143433\n",
            "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 120.166664\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 132.159546\n",
            "Train Epoch: 18 [44800/60000 (75%)]\tLoss: 126.387840\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 135.772171\n",
            "Train Epoch: 18 [57600/60000 (96%)]\tLoss: 128.662720\n",
            "====> Epoch: 18 Average loss: 130.2018\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 126.049500\n",
            "Train Epoch: 19 [6400/60000 (11%)]\tLoss: 130.147812\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 129.286667\n",
            "Train Epoch: 19 [19200/60000 (32%)]\tLoss: 124.329521\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 127.992676\n",
            "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 131.658905\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 126.443184\n",
            "Train Epoch: 19 [44800/60000 (75%)]\tLoss: 129.913361\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 126.493919\n",
            "Train Epoch: 19 [57600/60000 (96%)]\tLoss: 138.853928\n",
            "====> Epoch: 19 Average loss: 130.0861\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 119.614731\n",
            "Train Epoch: 20 [6400/60000 (11%)]\tLoss: 134.002121\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 118.899025\n",
            "Train Epoch: 20 [19200/60000 (32%)]\tLoss: 121.999207\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 122.782013\n",
            "Train Epoch: 20 [32000/60000 (53%)]\tLoss: 132.498108\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 134.229034\n",
            "Train Epoch: 20 [44800/60000 (75%)]\tLoss: 142.066528\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 129.857910\n",
            "Train Epoch: 20 [57600/60000 (96%)]\tLoss: 133.012558\n",
            "====> Epoch: 20 Average loss: 129.9152\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 122.799355\n",
            "Train Epoch: 21 [6400/60000 (11%)]\tLoss: 125.239693\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 128.628830\n",
            "Train Epoch: 21 [19200/60000 (32%)]\tLoss: 122.590347\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 125.482132\n",
            "Train Epoch: 21 [32000/60000 (53%)]\tLoss: 132.328369\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 131.585327\n",
            "Train Epoch: 21 [44800/60000 (75%)]\tLoss: 132.677551\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 130.848541\n",
            "Train Epoch: 21 [57600/60000 (96%)]\tLoss: 133.736633\n",
            "====> Epoch: 21 Average loss: 129.8075\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 128.114105\n",
            "Train Epoch: 22 [6400/60000 (11%)]\tLoss: 127.478279\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 131.433487\n",
            "Train Epoch: 22 [19200/60000 (32%)]\tLoss: 127.893578\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 139.867249\n",
            "Train Epoch: 22 [32000/60000 (53%)]\tLoss: 121.941658\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 135.414307\n",
            "Train Epoch: 22 [44800/60000 (75%)]\tLoss: 138.248184\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 128.227859\n",
            "Train Epoch: 22 [57600/60000 (96%)]\tLoss: 129.261917\n",
            "====> Epoch: 22 Average loss: 129.7281\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 117.978691\n",
            "Train Epoch: 23 [6400/60000 (11%)]\tLoss: 130.794144\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 132.424240\n",
            "Train Epoch: 23 [19200/60000 (32%)]\tLoss: 134.880997\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 120.882729\n",
            "Train Epoch: 23 [32000/60000 (53%)]\tLoss: 128.703583\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 123.487236\n",
            "Train Epoch: 23 [44800/60000 (75%)]\tLoss: 126.161476\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 120.546661\n",
            "Train Epoch: 23 [57600/60000 (96%)]\tLoss: 126.717628\n",
            "====> Epoch: 23 Average loss: 129.6749\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 141.062073\n",
            "Train Epoch: 24 [6400/60000 (11%)]\tLoss: 129.964569\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 136.453384\n",
            "Train Epoch: 24 [19200/60000 (32%)]\tLoss: 132.054901\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 131.456223\n",
            "Train Epoch: 24 [32000/60000 (53%)]\tLoss: 122.090561\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 121.722778\n",
            "Train Epoch: 24 [44800/60000 (75%)]\tLoss: 122.246284\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 144.624542\n",
            "Train Epoch: 24 [57600/60000 (96%)]\tLoss: 127.913612\n",
            "====> Epoch: 24 Average loss: 129.5547\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 130.077408\n",
            "Train Epoch: 25 [6400/60000 (11%)]\tLoss: 136.084961\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 128.696564\n",
            "Train Epoch: 25 [19200/60000 (32%)]\tLoss: 123.687126\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 124.458588\n",
            "Train Epoch: 25 [32000/60000 (53%)]\tLoss: 120.947693\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 140.285416\n",
            "Train Epoch: 25 [44800/60000 (75%)]\tLoss: 126.769234\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 123.987846\n",
            "Train Epoch: 25 [57600/60000 (96%)]\tLoss: 125.453438\n",
            "====> Epoch: 25 Average loss: 129.4484\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 123.854347\n",
            "Train Epoch: 26 [6400/60000 (11%)]\tLoss: 135.367767\n",
            "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 122.441055\n",
            "Train Epoch: 26 [19200/60000 (32%)]\tLoss: 122.033493\n",
            "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 119.759880\n",
            "Train Epoch: 26 [32000/60000 (53%)]\tLoss: 138.813538\n",
            "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 129.101273\n",
            "Train Epoch: 26 [44800/60000 (75%)]\tLoss: 138.422562\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 139.238754\n",
            "Train Epoch: 26 [57600/60000 (96%)]\tLoss: 125.244751\n",
            "====> Epoch: 26 Average loss: 129.3444\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 126.358910\n",
            "Train Epoch: 27 [6400/60000 (11%)]\tLoss: 129.358200\n",
            "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 128.172653\n",
            "Train Epoch: 27 [19200/60000 (32%)]\tLoss: 134.937531\n",
            "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 126.727791\n",
            "Train Epoch: 27 [32000/60000 (53%)]\tLoss: 141.249130\n",
            "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 122.363045\n",
            "Train Epoch: 27 [44800/60000 (75%)]\tLoss: 130.918625\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 125.684868\n",
            "Train Epoch: 27 [57600/60000 (96%)]\tLoss: 125.573090\n",
            "====> Epoch: 27 Average loss: 129.2389\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 127.963623\n",
            "Train Epoch: 28 [6400/60000 (11%)]\tLoss: 126.497772\n",
            "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 141.925705\n",
            "Train Epoch: 28 [19200/60000 (32%)]\tLoss: 129.082596\n",
            "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 134.051102\n",
            "Train Epoch: 28 [32000/60000 (53%)]\tLoss: 125.639206\n",
            "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 135.514709\n",
            "Train Epoch: 28 [44800/60000 (75%)]\tLoss: 120.704849\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 127.145035\n",
            "Train Epoch: 28 [57600/60000 (96%)]\tLoss: 121.150345\n",
            "====> Epoch: 28 Average loss: 129.2018\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 126.308182\n",
            "Train Epoch: 29 [6400/60000 (11%)]\tLoss: 131.123825\n",
            "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 133.609573\n",
            "Train Epoch: 29 [19200/60000 (32%)]\tLoss: 127.529282\n",
            "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 127.753937\n",
            "Train Epoch: 29 [32000/60000 (53%)]\tLoss: 129.581741\n",
            "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 124.710838\n",
            "Train Epoch: 29 [44800/60000 (75%)]\tLoss: 127.219666\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 126.877327\n",
            "Train Epoch: 29 [57600/60000 (96%)]\tLoss: 137.599182\n",
            "====> Epoch: 29 Average loss: 129.0882\n"
          ]
        }
      ],
      "source": [
        "bs = 64\n",
        "num_epochs=30\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=True)\n",
        "\n",
        "# build model\n",
        "vae = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
        "if torch.cuda.is_available():\n",
        "    vae.cuda()\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = optim.Adam(vae.parameters())\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train(epoch)\n",
        "    #test(epoch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GgDhIZTov59"
      },
      "source": [
        "**Generate Images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "wzQcBNAnov59",
        "outputId": "e77c9ce1-8e70-42d8-bcf4-c7334ada8c9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 28, 28])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-72c8d5380b4a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconditional_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-42-7862a44eafba>\u001b[0m in \u001b[0;36mconditional_generation\u001b[0;34m(vae, test_loader, num_images, interp_weight)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mfinal_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mshow_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0;31m#save_image(final_output, f'./samples/final_output_{weight*100}p.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-f22dc2a5fd0a>\u001b[0m in \u001b[0;36mshow_image\u001b[0;34m(img_tensor)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#np_op_img = np.asarray(img_tensor.cpu()).squeeze()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0minterpolation_stage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         resample=None, url=None, data=None, **kwargs):\n\u001b[0;32m-> 2695\u001b[0;31m     __ret = gca().imshow(\n\u001b[0m\u001b[1;32m   2696\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maspect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5663\u001b[0m                               **kwargs)\n\u001b[1;32m   5664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5665\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5666\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    708\u001b[0m         if not (self._A.ndim == 2\n\u001b[1;32m    709\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[0;32m--> 710\u001b[0;31m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[1;32m    711\u001b[0m                             .format(self._A.shape))\n\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (28, 28, 32) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "op = conditional_generation(vae, test_loader, num_images=32, interp_weight=0.5)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}