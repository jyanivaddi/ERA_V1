{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pytorch-lightning\n",
    "! pip install pytorch-lightning-bolts==0.2.5rc1\n",
    "! pip install --quiet \"torchinfo\" \"lightning-bolts\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as pl\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Classifier Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, eps:float=1E-6) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # alpha, learnable mean\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # bias, learnable bias\n",
    "\n",
    "    def forward(self,x):\n",
    "        # x: (batch_size, seq_len, hidden_size)\n",
    "        # Keep the dimension for broadcasting\n",
    "        mean = x.mean(dim=-1, keepdim=True) # (batch, seq_len, 1)\n",
    "        # Keep the dimension for broadcasting\n",
    "        std = x.std(dim=-1, keepdim=True) # (batch, seq_len, 1)\n",
    "        # eps is to prevent dividing by zero or when std is very small\n",
    "        return self.alpha*(x-mean)/(std+self.eps) + self.bias\n",
    "\n",
    "\n",
    "class MNIST_Classifier(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=784, hidden_dim=512, output_dim=10):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Sequential(nn.Linear(input_dim, hidden_dim, bias=False),\n",
    "                                    nn.Dropout(0.1),\n",
    "                                    nn.ReLU())\n",
    "        self.fc_2 = nn.Sequential(nn.Linear(hidden_dim, hidden_dim, bias=False),\n",
    "                                    nn.Dropout(0.1),\n",
    "                                    nn.ReLU())\n",
    "        self.fc_3 = nn.Sequential(nn.Linear(hidden_dim, output_dim, bias=False), nn.ReLU())\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(self.fc_1(x))\n",
    "        x = self.norm(self.fc_2(x))\n",
    "        x = self.norm(self.fc_3(x))\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and save the classifier model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_mnist_classifier(num_epochs = 10):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    train_data = datasets.MNIST('../data', train=True, download=True, transform=train_transforms)\n",
    "    torch.manual_seed(1)\n",
    "    batch_size = 128\n",
    "    kwargs = {'num_workers': 2, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)\n",
    "    model = MNIST_Classifier().to(device)\n",
    "    mnist_optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    #mnist_scheduler = optim.lr_scheduler.StepLR(mnist_optimizer, step_size=8, gamma=0.1, verbose=True)\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader)\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        processed = 0\n",
    "        for batch_idx, (data, target) in enumerate(pbar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = torch.flatten(data, start_dim=1)\n",
    "            mnist_optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            mnist_optimizer.step()\n",
    "            correct += output.argmax(dim=1).eq(target).sum().item()\n",
    "            processed += len(data)\n",
    "            pbar.set_description(\n",
    "                desc=f'loss={loss.item()} batch_id={batch_idx} Accuracy = {100 * correct / processed:0.2f}')\n",
    "        #mnist_scheduler.step()\n",
    "    torch.save(model.state_dict(), 'classifier_model.pth')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define VAE model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, num_labels=10, label_enc_dim=28):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # label embedding\n",
    "        self.label_enc_dim = label_enc_dim\n",
    "        self.num_labels = num_labels\n",
    "        self.class_embeddings = nn.Embedding(self.num_labels, self.label_enc_dim)\n",
    "\n",
    "        # encoder part\n",
    "        self.enc_inp_dim = x_dim+(self.num_labels*self.label_enc_dim)\n",
    "        self.fc1 = nn.Linear(self.enc_inp_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.dec_inp_dim = z_dim + (self.num_labels*self.label_enc_dim)\n",
    "        self.fc4 = nn.Linear(self.dec_inp_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)  # mu, log_var\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)  # return z sample\n",
    "\n",
    "    def decoder(self, z, label):\n",
    "        label_enc = self.get_label_embedding(label)\n",
    "        z = torch.cat([z, label_enc], dim=1)\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h))\n",
    "\n",
    "    def get_label_embedding(self, label):\n",
    "        label_one_hot = torch.nn.functional.one_hot(label, num_classes=10)\n",
    "        label_enc = self.class_embeddings(label_one_hot)\n",
    "        label_enc = torch.flatten(label_enc, start_dim=1)\n",
    "        return label_enc\n",
    "\n",
    "\n",
    "    def forward(self, x, label):\n",
    "        x = x.view(-1, 784)\n",
    "        label_enc = self.get_label_embedding(label)\n",
    "        x = torch.cat([x, label_enc], dim=1)\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, label), mu, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Loss Function for both reconstruction and adversarial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# return classifier loss\n",
    "def get_classifier_loss(epoch, x, labels, adversarial_prob=0.5, classifier_model=None):\n",
    "    # randomly shuffle labels\n",
    "    if np.random.random() > 1. - adversarial_prob:\n",
    "        # With view\n",
    "        idx = torch.randperm(labels.nelement())\n",
    "        labels = labels.view(-1)[idx].view(labels.size())\n",
    "    #if epoch == 10:\n",
    "    #    print(\"epoch 10\")\n",
    "    classifier_preds = classifier_model(x)\n",
    "    classifier_loss = nn.functional.cross_entropy(torch.exp(classifier_preds), labels)\n",
    "    return classifier_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define train and test functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def build_and_train_vae_model(train_loader, test_loader, classifier_model=None, num_epochs=100, use_adversarial=True, adversarial_prob=0.5):\n",
    "    # build model\n",
    "    vae = VAE(x_dim=784, h_dim1=512, h_dim2=256, z_dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        vae.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=1e-4)\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.01,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        epochs=num_epochs,\n",
    "        pct_start=5/num_epochs,\n",
    "        div_factor=100,\n",
    "        three_phase=True,\n",
    "        final_div_factor=1,\n",
    "        anneal_strategy='linear'\n",
    "    )\n",
    "\n",
    "    for epoch in range(0, num_epochs):\n",
    "        vae.train()\n",
    "        train_loss = 0\n",
    "        recon_loss = 0\n",
    "        classifier_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            global_step = batch_idx+epoch*len(train_loader)\n",
    "            writer.add_scalar(\"LR\", scheduler.get_last_lr()[0], global_step)\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_batch, mu, log_var = vae(data, labels)\n",
    "            this_recon_loss = loss_function(recon_batch, data, mu, log_var)\n",
    "\n",
    "            if use_adversarial:\n",
    "                this_classifier_loss = get_classifier_loss(epoch, recon_batch.to(device), labels, adversarial_prob=adversarial_prob, classifier_model=classifier_model)\n",
    "                loss = 0.01*this_recon_loss + 100*this_classifier_loss\n",
    "            else:\n",
    "                loss = this_recon_loss\n",
    "                this_classifier_loss=None\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            if this_classifier_loss is not None:\n",
    "                classifier_loss += this_classifier_loss.item()\n",
    "\n",
    "            recon_loss += this_recon_loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            writer.add_scalar(\"Total_Loss\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"Classifier_Loss\", this_classifier_loss.item(), global_step)\n",
    "            writer.add_scalar(\"Recon_Loss\", this_recon_loss.item(), global_step)\n",
    "\n",
    "        avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "        avg_recon_loss = recon_loss / len(train_loader.dataset)\n",
    "        avg_classifier_loss = classifier_loss / len(train_loader.dataset)\n",
    "        print('====> Epoch: {} Total loss: {:.4f} Recon loss: {:.4f} Classifier loss: {:.4f}'.format(epoch, avg_train_loss, avg_recon_loss, avg_classifier_loss))\n",
    "        test(test_loader, vae, epoch)\n",
    "    return vae\n",
    "\n",
    "\n",
    "def test(test_loader, vae, epoch):\n",
    "    vae.eval()\n",
    "    test_loss = 0\n",
    "    tgt_labels = torch.arange(10, dtype=torch.int64)\n",
    "    outputs = []\n",
    "    data, labels = next(iter(test_loader))\n",
    "    x = data[0].unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        for lbl in tgt_labels:\n",
    "            for _ in tgt_labels:\n",
    "                op, *_ = vae(x, lbl.unsqueeze(0).to(device))\n",
    "                outputs.append(op.view(1,28,28))\n",
    "    final_op = torch.vstack(outputs).unsqueeze(1)\n",
    "    save_image(final_op, f'./samples/sample_{epoch}.png')\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define supporting functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def get_loaders(batch_size):\n",
    "    # MNIST Dataset\n",
    "    train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "    test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "    # Data Loader (Input Pipeline)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def get_classifier_model(train_classifier, model_path):\n",
    "    if train_classifier:\n",
    "        trained_classifier_model = train_mnist_classifier(num_epochs=10)\n",
    "    else:\n",
    "        trained_classifier_model = MNIST_Classifier()\n",
    "        trained_classifier_model.load_state_dict(torch.load(model_path))\n",
    "    return trained_classifier_model\n",
    "\n",
    "def show_image(img_tensor):\n",
    "    np_op_img = np.asarray(img_tensor.detach().cpu()).squeeze()\n",
    "    pl.figure()\n",
    "    pl.imshow(np_op_img, cmap='gray')\n",
    "    pl.colorbar()\n",
    "    pl.show()\n",
    "\n",
    "\n",
    "def generate_image(x, label):\n",
    "    gen_img_tensor, *_ = vae(x, torch.tensor(label, dtype=torch.int64).unsqueeze(0).cuda())\n",
    "    gen_img_tensor = gen_img_tensor.view((1,28,28))\n",
    "    show_image(gen_img_tensor)\n",
    "    return gen_img_tensor\n",
    "\n",
    "\n",
    "def conditional_generation(vae, test_loader, num_images=32, interp_weight=0.1):\n",
    "    data, label = next(iter(test_loader))\n",
    "    gen_images = []\n",
    "    for cnt in range(num_images):\n",
    "        x = data[cnt].view(-1, 784)\n",
    "        l_e = vae.get_label_embedding(label[cnt])\n",
    "        x_l = torch.cat([x, l_e], dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # get mean and std of given image and label\n",
    "            mu0, log_var0 = vae.encoder(x_l)\n",
    "            z0 = vae.sampling(mu0, log_var0)\n",
    "\n",
    "            # draw a random value of mean and std from standard gaussian\n",
    "            shuffled_label_val = np.random.choice(np.arange(10, dtype=int))\n",
    "            shuffled_label = torch.tensor(shuffled_label_val, dtype=torch.int64).unsqueeze(0).to(device)\n",
    "            z1 = torch.randn(1, 2).cuda()\n",
    "\n",
    "            z_interp = torch.lerp(z0, z1, weight=interp_weight)\n",
    "            img_tensor = vae.decoder(z_interp, shuffled_label_val).view(1, 28, 28)\n",
    "            gen_images.append(img_tensor)\n",
    "        final_output = torch.vstack(gen_images).unsqueeze(1)\n",
    "        show_image(final_output)\n",
    "        #save_image(final_output, f'./samples/final_output_{weight*100}p.png')\n",
    "        return final_output\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the whole flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#use_cuda = True if torch.cuda.is_available() else False\n",
    "use_cuda=True\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "writer = SummaryWriter()\n",
    "\n",
    "train_classifier = False\n",
    "model_path = 'classifier_model.pth'\n",
    "trained_classifier_model = get_classifier_model(train_classifier, model_path)\n",
    "trained_classifier_model.to(device)\n",
    "trained_classifier_model.eval()\n",
    "\n",
    "bs = 64\n",
    "train_loader, test_loader = get_loaders(bs)\n",
    "vae = build_and_train_vae_model(train_loader, test_loader, num_epochs=40, classifier_model=trained_classifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "op = conditional_generation(vae, test_loader, num_images=32, interp_weight=0.5):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
