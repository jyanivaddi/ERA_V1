{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP0bmTk0fk0RQdFArffvMFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jyanivaddi/ERA_V1/blob/master/gpt2_hindi_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARkRHQ2v7KSD",
        "outputId": "d762d04b-296b-4ab1-f6e9-a885db99da07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fte3KS2c68Pr",
        "outputId": "ba6867f7-4392-438e-e00b-4018cae4f6a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.7/493.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --quiet \"torchtext\" \"datasets\" \"tokenizers\" \"transformers\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the train and val datasets"
      ],
      "metadata": {
        "id": "JW0kCFY8ALdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_path = '/content/gdrive/MyDrive/Datasets/Hindi_Aesthetics/hindi_train.txt'\n",
        "val_dataset_path = '/content/gdrive/MyDrive/Datasets/Hindi_Aesthetics/hindi_val.txt'"
      ],
      "metadata": {
        "id": "VIeI2nM4AKIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(train_dataset_path, 'r', encoding='utf-8') as f:\n",
        "    train_text = f.read()\n"
      ],
      "metadata": {
        "id": "k7Wgmo5gdnvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_words = train_text.split()\n",
        "print(len(train_text_words))\n",
        "print(len(np.unique(train_text_words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX498CQufAsb",
        "outputId": "852cbe24-e816-4888-997e-dfb2199cbf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7158513\n",
            "142052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from: https://towardsdatascience.com/train-gpt-2-in-your-own-language-fc6ad4d60171"
      ],
      "metadata": {
        "id": "zYtPUv0Sx3gC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.decoders import ByteLevel as ByteLevelDecoder\n",
        "from tokenizers.normalizers import NFKC, Sequence\n",
        "from tokenizers.pre_tokenizers import ByteLevel\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "class BPE_token(object):\n",
        "    def __init__(self):\n",
        "        self.tokenizer = Tokenizer(BPE())\n",
        "        self.tokenizer.normalizer = Sequence([\n",
        "            NFKC()\n",
        "        ])\n",
        "        self.tokenizer.pre_tokenizer = ByteLevel()\n",
        "        self.tokenizer.decoder = ByteLevelDecoder()\n",
        "\n",
        "    def bpe_train(self, paths):\n",
        "        trainer = BpeTrainer(vocab_size=50000, show_progress=True, inital_alphabet=ByteLevel.alphabet(), special_tokens=[\n",
        "            \"<s>\",\n",
        "            \"<pad>\",\n",
        "            \"</s>\",\n",
        "            \"<unk>\",\n",
        "            \"<mask>\"\n",
        "        ])\n",
        "        self.tokenizer.train(paths, trainer)\n",
        "\n",
        "    def save_tokenizer(self, location, prefix=None):\n",
        "        if not os.path.exists(location):\n",
        "            os.makedirs(location)\n",
        "        self.tokenizer.model.save(location, prefix)"
      ],
      "metadata": {
        "id": "Qmoa2NJ8sMhq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "# the folder 'text' contains all the files\n",
        "paths = [train_dataset_path]\n",
        "tokenizer = BPE_token()\n",
        "# train the tokenizer model\n",
        "tokenizer.bpe_train(paths)\n",
        "# saving the tokenized data in our specified folder\n",
        "save_path = 'tokenized_data'\n",
        "tokenizer.save_tokenizer(save_path)"
      ],
      "metadata": {
        "id": "O5XKvReqsZ5M"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from: https://huggingface.co/blog/how-to-train"
      ],
      "metadata": {
        "id": "_WFDLtmYx8ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#! pip install tokenizers\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "paths = [train_dataset_path]\n",
        "\n",
        "# Initialize a tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Customize training\n",
        "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",\n",
        "])\n",
        "\n",
        "# Save files to disk\n",
        "tokenizer.save_model(\".\", \"hindi_gpt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8oYB2pWxN2i",
        "outputId": "c6985c6b-a964-4382-e8f6-dc9a800a52a4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./hindi_gpt-vocab.json', './hindi_gpt-merges.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"/content/tokenized_data/vocab.json\",\n",
        "    \"/content/tokenized_data/merges.txt\",\n",
        ")\n",
        "tokenizer._tokenizer.post_processor = BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_truncation(max_length=512)\n",
        "\n",
        "print(\n",
        "    tokenizer.encode(\"नाम के लोग कैसे होते हैं?\")\n",
        ")\n",
        "# Encoding(num_tokens=7, ...)\n",
        "# tokens: ['<s>', 'Mi', 'Ġestas', 'ĠJuli', 'en', '.', '</s>']\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8uAOy4nufyW",
        "outputId": "d0a043af-0c0e-4f31-e944-660285fdf5ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding(num_tokens=20, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
          ]
        }
      ]
    }
  ]
}